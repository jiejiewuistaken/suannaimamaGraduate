{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f2f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 设置中文字体（Mac 系统自带）\n",
    "plt.rcParams['font.sans-serif'] = ['PingFang SC', 'Arial Unicode MS', 'STHeiti', 'STFangsong']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185cafac",
   "metadata": {},
   "source": [
    "### 1. 数据预处理\n",
    "\n",
    "生成merged_df（为特征工程作准备）和sales data（为预测作准备）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e444c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "电解镍 数据处理完成并保存至: /Users/bytedance/Desktop/特征工程补充/结果/处理后的电解镍_销售数据.xlsx\n",
      "电解镍 大客户数量: 603\n",
      "高碳铬铁 数据处理完成并保存至: /Users/bytedance/Desktop/特征工程补充/结果/处理后的高碳铬铁_销售数据.xlsx\n",
      "高碳铬铁 大客户数量: 622\n",
      "铝锭 数据处理完成并保存至: /Users/bytedance/Desktop/特征工程补充/结果/处理后的铝锭_销售数据.xlsx\n",
      "铝锭 大客户数量: 240\n",
      "\n",
      "开始处理电解镍的时间序列数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:214: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['demand'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:215: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['inquiry_quantity'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:216: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['inquiry_count'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:217: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['big_customer_ratio'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['big_customer_quantity_ratio'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:226: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[col].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:237: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df[price_cols] = merged_df[price_cols].fillna(method='ffill').fillna(method='bfill')\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:243: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df[col] = merged_df[col].fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "电解镍合并数据保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍_merged_df.xlsx\n",
      "电解镍2024年销售数据保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍_2024_daily_sales.xlsx\n",
      "\n",
      "开始处理高碳铬铁的时间序列数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:214: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['demand'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:215: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['inquiry_quantity'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:216: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['inquiry_count'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:217: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['big_customer_ratio'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['big_customer_quantity_ratio'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:226: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[col].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:237: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df[price_cols] = merged_df[price_cols].fillna(method='ffill').fillna(method='bfill')\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:243: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df[col] = merged_df[col].fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高碳铬铁合并数据保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁_merged_df.xlsx\n",
      "高碳铬铁2024年销售数据保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁_2024_daily_sales.xlsx\n",
      "\n",
      "开始处理铝锭的时间序列数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:214: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['demand'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:215: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['inquiry_quantity'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:216: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['inquiry_count'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:217: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['big_customer_ratio'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['big_customer_quantity_ratio'].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:226: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[col].fillna(0, inplace=True)\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:237: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df[price_cols] = merged_df[price_cols].fillna(method='ffill').fillna(method='bfill')\n",
      "/var/folders/x4/ln8wxptd18sbrbwgzv2_7wjm0000gn/T/ipykernel_85815/2605631414.py:243: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df[col] = merged_df[col].fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "铝锭合并数据保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭_merged_df.xlsx\n",
      "铝锭2024年销售数据保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭_2024_daily_sales.xlsx\n",
      "\n",
      "所有处理完成!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 预处理三个品种的销售数据（增强聚合）\n",
    "# =============================================================================\n",
    "def preprocess_sales_data(file_path, sheet_name):\n",
    "    \"\"\"预处理单个品种的销售数据，确保每天只有一条记录\"\"\"\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # 处理客户列\n",
    "    if '客户' in df.columns:\n",
    "        df['客户'] = df['客户'].str.extract(r'(\\d+)$').astype(float)\n",
    "    df = df[df['单据类型'] == '出库']\n",
    "    \n",
    "    # 确保日期是日期类型\n",
    "    df['日期'] = pd.to_datetime(df['日期'])\n",
    "    \n",
    "    # 按天聚合销售数据\n",
    "    agg_dict = {\n",
    "        '数量': 'sum',  # 每日总数量\n",
    "        '价格': 'mean',  # 每日平均价格\n",
    "    }\n",
    "    \n",
    "    # 添加其他需要聚合的列\n",
    "    if '期货市场参考价' in df.columns:\n",
    "        agg_dict['期货市场参考价'] = 'mean'\n",
    "    \n",
    "    # 按日期和客户分组聚合\n",
    "    daily_sales = df.groupby(['日期', '客户']).agg(agg_dict).reset_index()\n",
    "    \n",
    "    # 计算销售额\n",
    "    daily_sales['销售额'] = daily_sales['数量'] * daily_sales['价格']\n",
    "    \n",
    "    # 重命名列\n",
    "    rename_dict = {\n",
    "        '日期': '销售日期',\n",
    "        '数量': '采购数量',\n",
    "        '价格': '采购价格'\n",
    "    }\n",
    "    daily_sales.rename(columns=rename_dict, inplace=True)\n",
    "    \n",
    "    # 保留需要的列\n",
    "    keep_cols = ['销售日期', '采购数量', '采购价格', '客户', '销售额']\n",
    "    if '期货市场参考价' in daily_sales.columns:\n",
    "        keep_cols.append('期货市场参考价')\n",
    "    \n",
    "    return daily_sales[keep_cols]\n",
    "\n",
    "# 输入输出路径\n",
    "input_file = r\"/Users/bytedance/Desktop/特征工程补充/采销明细.xls\"\n",
    "output_dir = r\"/Users/bytedance/Desktop/特征工程补充/结果\"\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 处理三个品种\n",
    "products = ['电解镍', '高碳铬铁', '铝锭']\n",
    "processed_dfs = {}\n",
    "big_customers_dict = {}  # 存储每个品种的大客户列表\n",
    "\n",
    "for product in products:\n",
    "    df = preprocess_sales_data(input_file, product)\n",
    "    output_path = os.path.join(output_dir, f\"处理后的{product}_销售数据.xlsx\")\n",
    "    df.to_excel(output_path, index=False)\n",
    "    processed_dfs[product] = df\n",
    "    \n",
    "    # 计算每个客户的累计销售额\n",
    "    customer_sales = df.groupby('客户')['销售额'].sum().reset_index()\n",
    "    \n",
    "    # 计算累计销售额的阈值（前20%）\n",
    "    threshold = customer_sales['销售额'].quantile(0.8)\n",
    "    \n",
    "    # 识别大客户\n",
    "    big_customers = customer_sales[customer_sales['销售额'] >= threshold]['客户'].tolist()\n",
    "    big_customers_dict[product] = big_customers\n",
    "    \n",
    "    print(f\"{product} 数据处理完成并保存至: {output_path}\")\n",
    "    print(f\"{product} 大客户数量: {len(big_customers)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. 为所有品种创建时间序列数据（添加大客户询价比例，确保每天一条记录）\n",
    "# =============================================================================\n",
    "def create_time_series_data(sales_df, inquiry_file, product, big_customers):\n",
    "    \"\"\"为指定品种创建时间序列数据，确保每天只有一条记录\"\"\"\n",
    "    # 加载销售数据\n",
    "    sales_df = sales_df.copy()\n",
    "    sales_df.rename(columns={'销售日期': 'date'}, inplace=True)\n",
    "    sales_df['date'] = pd.to_datetime(sales_df['date']).dt.normalize()\n",
    "    \n",
    "    if '期货市场参考价' in sales_df.columns:\n",
    "        sales_df['期货市场参考价'] = sales_df['期货市场参考价'] / 1000\n",
    "\n",
    "    # 按天聚合销售数据（确保每天只有一条记录）\n",
    "    agg_dict = {\n",
    "        '采购数量': 'sum',\n",
    "        '采购价格': 'mean',\n",
    "        '销售额': 'sum'\n",
    "    }\n",
    "    if '期货市场参考价' in sales_df.columns:\n",
    "        agg_dict['期货市场参考价'] = 'mean'\n",
    "    \n",
    "    # 按日期聚合，确保每天只有一条记录\n",
    "    daily_sales = sales_df.groupby('date').agg(agg_dict)\n",
    "    if '期货市场参考价' in sales_df.columns:\n",
    "        daily_sales.columns = ['demand', 'avg_price', 'future_ref_price', 'sales']\n",
    "    else:\n",
    "        daily_sales.columns = ['demand', 'avg_price', 'sales']\n",
    "    \n",
    "    # 加载询价数据\n",
    "    try:\n",
    "        inquiry_df = pd.read_excel(inquiry_file, sheet_name=f\"{product}报价\")\n",
    "        inquiry_df['date'] = pd.to_datetime(inquiry_df['报价时间']).dt.normalize()\n",
    "        \n",
    "        # 添加大客户标记\n",
    "        inquiry_df['is_big_customer'] = inquiry_df['客户编码'].apply(\n",
    "            lambda x: 1 if x in big_customers else 0\n",
    "        )\n",
    "        \n",
    "        # 按天聚合询价数据（确保每天只有一条记录）\n",
    "        daily_inquiry = inquiry_df.groupby('date').agg(\n",
    "            inquiry_count=('客户编码', 'count'),\n",
    "            our_quote_avg=('报价', 'mean'),\n",
    "            inquiry_quantity=('报价数量', 'sum'),\n",
    "            price_std=('报价', 'std'),\n",
    "            big_customer_inquiries=('is_big_customer', 'sum'),\n",
    "            big_customer_quantity=('报价数量', lambda x: x[inquiry_df.loc[x.index, 'is_big_customer'] == 1].sum())\n",
    "        ).reset_index()\n",
    "        \n",
    "        # 计算大客户询价比例\n",
    "        daily_inquiry['big_customer_ratio'] = daily_inquiry['big_customer_inquiries'] / daily_inquiry['inquiry_count']\n",
    "        daily_inquiry['big_customer_quantity_ratio'] = daily_inquiry['big_customer_quantity'] / daily_inquiry['inquiry_quantity']\n",
    "        \n",
    "        # 添加时间特征用于计算上周和上个月\n",
    "        daily_inquiry['year'] = daily_inquiry['date'].dt.year\n",
    "        daily_inquiry['month'] = daily_inquiry['date'].dt.month\n",
    "        daily_inquiry['week'] = daily_inquiry['date'].dt.isocalendar().week\n",
    "        \n",
    "        # 计算上周的数据\n",
    "        daily_inquiry['last_week'] = daily_inquiry['date'] - timedelta(days=7)\n",
    "        \n",
    "        # 计算上个月的数据\n",
    "        daily_inquiry['last_month'] = daily_inquiry['date'] - pd.DateOffset(months=1)\n",
    "        \n",
    "        # 创建上周和上个月的数据副本\n",
    "        last_week_data = daily_inquiry[['date', 'big_customer_ratio', 'big_customer_quantity_ratio']].copy()\n",
    "        last_week_data.rename(columns={\n",
    "            'date': 'last_week_date',\n",
    "            'big_customer_ratio': 'last_week_big_customer_ratio',\n",
    "            'big_customer_quantity_ratio': 'last_week_big_customer_quantity_ratio'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        last_month_data = daily_inquiry[['date', 'big_customer_ratio', 'big_customer_quantity_ratio']].copy()\n",
    "        last_month_data.rename(columns={\n",
    "            'date': 'last_month_date',\n",
    "            'big_customer_ratio': 'last_month_big_customer_ratio',\n",
    "            'big_customer_quantity_ratio': 'last_month_big_customer_quantity_ratio'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        # 合并上周数据\n",
    "        daily_inquiry = daily_inquiry.merge(\n",
    "            last_week_data,\n",
    "            left_on='last_week',\n",
    "            right_on='last_week_date',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # 合并上个月数据\n",
    "        daily_inquiry = daily_inquiry.merge(\n",
    "            last_month_data,\n",
    "            left_on='last_month',\n",
    "            right_on='last_month_date',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # 清理多余的列\n",
    "        drop_cols = ['last_week', 'last_month', 'last_week_date', 'last_month_date']\n",
    "        daily_inquiry.drop(columns=[col for col in drop_cols if col in daily_inquiry.columns], inplace=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"警告: 处理{product}询价数据时出错: {e}\")\n",
    "        print(f\"将使用空数据\")\n",
    "        daily_inquiry = pd.DataFrame(columns=['date', 'inquiry_count', 'our_quote_avg', \n",
    "                                              'inquiry_quantity', 'price_std', \n",
    "                                              'big_customer_ratio', 'big_customer_quantity_ratio',\n",
    "                                              'last_week_big_customer_ratio', 'last_week_big_customer_quantity_ratio',\n",
    "                                              'last_month_big_customer_ratio', 'last_month_big_customer_quantity_ratio'])\n",
    "    \n",
    "    # 创建完整时间序列面板 (2020-2024)\n",
    "    all_dates = pd.date_range(start='2020-01-01', end='2024-02-29', freq='D')\n",
    "    full_panel = pd.DataFrame({'date': all_dates})\n",
    "    \n",
    "    # 合并数据\n",
    "    merged_df = full_panel.merge(daily_sales.reset_index(), on='date', how='left')\n",
    "    \n",
    "    if not daily_inquiry.empty:\n",
    "        merged_df = merged_df.merge(daily_inquiry, on='date', how='left')\n",
    "    else:\n",
    "        # 如果没有询价数据，添加空列\n",
    "        merged_df['inquiry_count'] = None\n",
    "        merged_df['our_quote_avg'] = None\n",
    "        merged_df['inquiry_quantity'] = None\n",
    "        merged_df['price_std'] = None\n",
    "        merged_df['big_customer_ratio'] = None\n",
    "        merged_df['big_customer_quantity_ratio'] = None\n",
    "        merged_df['last_week_big_customer_ratio'] = None\n",
    "        merged_df['last_week_big_customer_quantity_ratio'] = None\n",
    "        merged_df['last_month_big_customer_ratio'] = None\n",
    "        merged_df['last_month_big_customer_quantity_ratio'] = None\n",
    "    \n",
    "    # 填充缺失值\n",
    "    merged_df['demand'].fillna(0, inplace=True)\n",
    "    merged_df['inquiry_quantity'].fillna(0, inplace=True)\n",
    "    merged_df['inquiry_count'].fillna(0, inplace=True)\n",
    "    merged_df['big_customer_ratio'].fillna(0, inplace=True)\n",
    "    merged_df['big_customer_quantity_ratio'].fillna(0, inplace=True)\n",
    "    \n",
    "    # 填充上周和上个月的占比\n",
    "    ratio_cols = ['last_week_big_customer_ratio', 'last_week_big_customer_quantity_ratio',\n",
    "                  'last_month_big_customer_ratio', 'last_month_big_customer_quantity_ratio']\n",
    "    \n",
    "    for col in ratio_cols:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col].fillna(0, inplace=True)\n",
    "    \n",
    "    # 价格列向前向后填充\n",
    "    price_cols = ['avg_price']\n",
    "    if 'min_price' in merged_df.columns:\n",
    "        price_cols.append('min_price')\n",
    "    if 'max_price' in merged_df.columns:\n",
    "        price_cols.append('max_price')\n",
    "    if 'future_ref_price' in merged_df.columns:\n",
    "        price_cols.append('future_ref_price')\n",
    "    \n",
    "    merged_df[price_cols] = merged_df[price_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # 询价列向前向后填充\n",
    "    quote_cols = ['our_quote_avg', 'price_std']\n",
    "    for col in quote_cols:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "  \n",
    "    return merged_df, daily_sales\n",
    "\n",
    "# 处理所有品种的时间序列\n",
    "inquiry_file = r\"/Users/bytedance/Desktop/特征工程补充/4个品种6年报价数据.xls\"\n",
    "\n",
    "for product in products:\n",
    "    print(f\"\\n开始处理{product}的时间序列数据...\")\n",
    "    df = processed_dfs[product]\n",
    "    big_customers = big_customers_dict[product]\n",
    "    merged_df, daily_sales = create_time_series_data(df, inquiry_file, product, big_customers)\n",
    "    \n",
    "    # 检查是否每天只有一条记录\n",
    "    date_counts = merged_df['date'].value_counts()\n",
    "    if any(date_counts > 1):\n",
    "        print(f\"警告: {product}数据中存在一天多条记录的情况\")\n",
    "        print(f\"重复日期数量: {len(date_counts[date_counts > 1])}\")\n",
    "        # 合并重复记录\n",
    "        merged_df = merged_df.groupby('date').first().reset_index()\n",
    "        print(f\"已合并重复记录，现在每天只有一条记录\")\n",
    "    \n",
    "    # 保存合并数据\n",
    "    merged_output = os.path.join(output_dir, f\"{product}_merged_df.xlsx\")\n",
    "    merged_df.to_excel(merged_output, index=False)\n",
    "    print(f\"{product}合并数据保存至: {merged_output}\")\n",
    "    \n",
    "    # 提取2024年后的daily_sales\n",
    "    if not daily_sales.empty:\n",
    "        daily_sales_2024 = daily_sales[daily_sales.index >= datetime(2024, 3, 1)]\n",
    "        if not daily_sales_2024.empty:\n",
    "            daily_output = os.path.join(output_dir, f\"{product}_2024_daily_sales.xlsx\")\n",
    "            daily_sales_2024.to_excel(daily_output)\n",
    "            print(f\"{product}2024年销售数据保存至: {daily_output}\")\n",
    "        else:\n",
    "            print(f\"提示: {product}没有2024年后的销售数据\")\n",
    "    else:\n",
    "        print(f\"警告: {product}没有销售数据\")\n",
    "\n",
    "print(\"\\n所有处理完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5744b7",
   "metadata": {},
   "source": [
    "### 2. 安全的增强特征工程\n",
    "\n",
    "为每个产品创建大量预测特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "734fcbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "开始处理 电解镍 的特征工程和相关性分析\n",
      "================================================================================\n",
      "输出目录: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果\n",
      "\n",
      "开始为电解镍执行特征工程...\n",
      "应用增强的特征工程...\n",
      "添加大客户相关特征: 6个\n",
      "执行无泄漏缺失值填充...\n",
      "电解镍特征工程后数据集形状: (1521, 249)\n",
      "=== 修复数据类型问题 ===\n",
      "发现包含年-周格式的列: year_week\n",
      "删除字符串列: ['year_week']\n",
      "\n",
      "=== 最终数据类型检查 ===\n",
      "✓ 所有特征列都是数值类型\n",
      "最终数据形状: (1521, 248)\n",
      "电解镍特征工程数据保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_特征工程数据.xlsx\n",
      "\n",
      "开始全面分析电解镍的特征相关性...\n",
      "特征相关性分析结果将保存在: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_特征相关性\n",
      "数值特征列: ['demand', 'avg_price', 'sales', 'inquiry_count', 'our_quote_avg', 'inquiry_quantity', 'price_std', 'big_customer_inquiries', 'big_customer_quantity', 'big_customer_ratio', 'big_customer_quantity_ratio', 'week', 'last_week_big_customer_ratio', 'last_week_big_customer_quantity_ratio', 'last_month_big_customer_ratio', 'last_month_big_customer_quantity_ratio', 'week_of_year', 'is_Monday', 'is_Tuesday', 'is_Wednesday', 'is_Thursday', 'is_Friday', 'is_Saturday', 'is_Sunday', 'is_holiday', 'is_pre_holiday', 'is_post_holiday', 'is_weekend', 'is_month_start', 'is_month_end', 'demand_lag1', 'demand_lag2', 'demand_lag3', 'demand_lag5', 'demand_lag6', 'demand_lag7', 'demand_lag14', 'demand_lag30', 'demand_lag60', 'demand_lag90', 'demand_lag180', 'demand_lag365', 'demand_ma2', 'demand_ma3', 'demand_ma5', 'demand_ma6', 'demand_ma7', 'demand_ma14', 'demand_ma30', 'demand_ma60', 'demand_ma90', 'demand_ma180', 'demand_ma365', 'seasonal', 'trend', 'residual', '7d_avg_quote', '7d_avg_price', '14d_avg_quote', '14d_avg_price', '30d_avg_quote', '30d_avg_price', '60d_avg_quote', '60d_avg_price', '90d_avg_quote', '90d_avg_price', '180d_avg_quote', '180d_avg_price', 'price_diff', 'price_ratio', 'price_volatility_7d', 'price_volatility_14d', 'price_volatility_30d', 'price_volatility_60d', 'price_volatility_90d', 'inquiry_quantity_lag1', 'inquiry_quantity_lag2', 'inquiry_quantity_lag3', 'inquiry_quantity_lag5', 'inquiry_quantity_lag6', 'inquiry_quantity_lag7', 'inquiry_quantity_lag14', 'inquiry_quantity_lag30', 'inquiry_quantity_lag60', 'inquiry_quantity_lag90', 'inquiry_count_lag1', 'inquiry_count_lag2', 'inquiry_count_lag3', 'inquiry_count_lag5', 'inquiry_count_lag6', 'inquiry_count_lag7', 'inquiry_count_lag14', 'inquiry_count_lag30', 'inquiry_count_lag60', 'inquiry_count_lag90', 'our_quote_avg_lag1', 'our_quote_avg_lag2', 'our_quote_avg_lag3', 'our_quote_avg_lag5', 'our_quote_avg_lag6', 'our_quote_avg_lag7', 'our_quote_avg_lag14', 'our_quote_avg_lag30', 'our_quote_avg_lag60', 'our_quote_avg_lag90', 'avg_price_lag1', 'avg_price_lag2', 'avg_price_lag3', 'avg_price_lag5', 'avg_price_lag6', 'avg_price_lag7', 'avg_price_lag14', 'avg_price_lag30', 'avg_price_lag60', 'avg_price_lag90', 'inquiry_count_ma2', 'inquiry_count_ma3', 'inquiry_count_ma5', 'inquiry_count_ma6', 'inquiry_count_ma7', 'inquiry_count_ma14', 'inquiry_count_ma30', 'inquiry_count_ma60', 'inquiry_count_ma90', 'our_quote_avg_ma2', 'our_quote_avg_ma3', 'our_quote_avg_ma5', 'our_quote_avg_ma6', 'our_quote_avg_ma7', 'our_quote_avg_ma14', 'our_quote_avg_ma30', 'our_quote_avg_ma60', 'our_quote_avg_ma90', 'avg_price_ma2', 'avg_price_ma3', 'avg_price_ma5', 'avg_price_ma6', 'avg_price_ma7', 'avg_price_ma14', 'avg_price_ma30', 'avg_price_ma60', 'avg_price_ma90', 'demand_inquiry_ratio', 'demand_inquiry_diff', 'price_demand_ratio', 'seasonal_factor', 'weekday_factor', 'is_future_volatile', 'price_demand_interaction', 'price_inquiry_interaction', 'demand_change_1d', 'demand_change_7d', 'price_change_1d', 'price_change_7d', 'demand_volatility_7d', 'demand_volatility_30d', 'seasonal_trend', 'big_customer_ratio_lag1', 'big_customer_ratio_lag7', 'big_customer_ratio_lag14', 'big_customer_ratio_lag30', 'big_customer_quantity_ratio_lag1', 'big_customer_quantity_ratio_lag7', 'big_customer_quantity_ratio_lag14', 'big_customer_quantity_ratio_lag30', 'last_week_big_customer_ratio_lag1', 'last_week_big_customer_ratio_lag7', 'last_week_big_customer_ratio_lag14', 'last_week_big_customer_ratio_lag30', 'last_week_big_customer_quantity_ratio_lag1', 'last_week_big_customer_quantity_ratio_lag7', 'last_week_big_customer_quantity_ratio_lag14', 'last_week_big_customer_quantity_ratio_lag30', 'last_month_big_customer_ratio_lag1', 'last_month_big_customer_ratio_lag7', 'last_month_big_customer_ratio_lag14', 'last_month_big_customer_ratio_lag30', 'last_month_big_customer_quantity_ratio_lag1', 'last_month_big_customer_quantity_ratio_lag7', 'last_month_big_customer_quantity_ratio_lag14', 'last_month_big_customer_quantity_ratio_lag30', 'big_customer_ratio_ma7', 'big_customer_ratio_ma14', 'big_customer_ratio_ma30', 'big_customer_quantity_ratio_ma7', 'big_customer_quantity_ratio_ma14', 'big_customer_quantity_ratio_ma30', 'last_week_big_customer_ratio_ma7', 'last_week_big_customer_ratio_ma14', 'last_week_big_customer_ratio_ma30', 'last_week_big_customer_quantity_ratio_ma7', 'last_week_big_customer_quantity_ratio_ma14', 'last_week_big_customer_quantity_ratio_ma30', 'last_month_big_customer_ratio_ma7', 'last_month_big_customer_ratio_ma14', 'last_month_big_customer_ratio_ma30', 'last_month_big_customer_quantity_ratio_ma7', 'last_month_big_customer_quantity_ratio_ma14', 'last_month_big_customer_quantity_ratio_ma30', 'big_customer_ratio_change_1d', 'big_customer_ratio_change_7d', 'big_customer_quantity_ratio_change_1d', 'big_customer_quantity_ratio_change_7d', 'last_week_big_customer_ratio_change_1d', 'last_week_big_customer_ratio_change_7d', 'last_week_big_customer_quantity_ratio_change_1d', 'last_week_big_customer_quantity_ratio_change_7d', 'last_month_big_customer_ratio_change_1d', 'last_month_big_customer_ratio_change_7d', 'last_month_big_customer_quantity_ratio_change_1d', 'last_month_big_customer_quantity_ratio_change_7d', 'big_customer_ratio_demand_interaction', 'big_customer_ratio_price_interaction', 'big_customer_quantity_ratio_demand_interaction', 'big_customer_quantity_ratio_price_interaction', 'last_week_big_customer_ratio_demand_interaction', 'last_week_big_customer_ratio_price_interaction', 'last_week_big_customer_quantity_ratio_demand_interaction', 'last_week_big_customer_quantity_ratio_price_interaction', 'last_month_big_customer_ratio_demand_interaction', 'last_month_big_customer_ratio_price_interaction', 'last_month_big_customer_quantity_ratio_demand_interaction', 'last_month_big_customer_quantity_ratio_price_interaction', 'big_customer_ratio_inquiry_interaction', 'big_customer_quantity_ratio_inquiry_interaction', 'last_week_big_customer_ratio_inquiry_interaction', 'last_week_big_customer_quantity_ratio_inquiry_interaction', 'last_month_big_customer_ratio_inquiry_interaction', 'last_month_big_customer_quantity_ratio_inquiry_interaction', 'big_customer_ratio_seasonal_interaction', 'big_customer_quantity_ratio_seasonal_interaction', 'last_week_big_customer_ratio_seasonal_interaction', 'last_week_big_customer_quantity_ratio_seasonal_interaction', 'last_month_big_customer_ratio_seasonal_interaction', 'last_month_big_customer_quantity_ratio_seasonal_interaction', 'product_seasonal_index', 'demand_ema12', 'demand_ema26', 'demand_macd', 'demand_rsi']\n",
      "特征相关性结果已保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_特征相关性/电解镍_特征相关性.xlsx\n",
      "与demand相关性结果已保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_特征相关性/电解镍_与demand相关性.xlsx\n",
      "与demand相关性图已保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_特征相关性/电解镍_与demand相关性.png\n",
      "所有特征相关性矩阵已保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_特征相关性/电解镍_所有特征相关性矩阵.xlsx\n",
      "特征相关性矩阵图已保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_特征相关性/电解镍_特征相关性矩阵.png\n",
      "星期几平均需求图已保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_特征相关性/电解镍_星期几平均需求.png\n",
      "期货波动需求分布图已保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_特征相关性/电解镍_期货波动需求分布.png\n",
      "\n",
      "电解镍 的特征工程和相关性分析完成!\n",
      "\n",
      "================================================================================\n",
      "开始处理 高碳铬铁 的特征工程和相关性分析\n",
      "================================================================================\n",
      "输出目录: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果\n",
      "\n",
      "开始为高碳铬铁执行特征工程...\n",
      "应用增强的特征工程...\n",
      "添加大客户相关特征: 6个\n",
      "执行无泄漏缺失值填充...\n",
      "高碳铬铁特征工程后数据集形状: (1521, 249)\n",
      "=== 修复数据类型问题 ===\n",
      "发现包含年-周格式的列: year_week\n",
      "删除字符串列: ['year_week']\n",
      "\n",
      "=== 最终数据类型检查 ===\n",
      "✓ 所有特征列都是数值类型\n",
      "最终数据形状: (1521, 248)\n",
      "高碳铬铁特征工程数据保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_特征工程数据.xlsx\n",
      "\n",
      "开始全面分析高碳铬铁的特征相关性...\n",
      "特征相关性分析结果将保存在: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_特征相关性\n",
      "数值特征列: ['demand', 'avg_price', 'sales', 'inquiry_count', 'our_quote_avg', 'inquiry_quantity', 'price_std', 'big_customer_inquiries', 'big_customer_quantity', 'big_customer_ratio', 'big_customer_quantity_ratio', 'week', 'last_week_big_customer_ratio', 'last_week_big_customer_quantity_ratio', 'last_month_big_customer_ratio', 'last_month_big_customer_quantity_ratio', 'week_of_year', 'is_Monday', 'is_Tuesday', 'is_Wednesday', 'is_Thursday', 'is_Friday', 'is_Saturday', 'is_Sunday', 'is_holiday', 'is_pre_holiday', 'is_post_holiday', 'is_weekend', 'is_month_start', 'is_month_end', 'demand_lag1', 'demand_lag2', 'demand_lag3', 'demand_lag5', 'demand_lag6', 'demand_lag7', 'demand_lag14', 'demand_lag30', 'demand_lag60', 'demand_lag90', 'demand_lag180', 'demand_lag365', 'demand_ma2', 'demand_ma3', 'demand_ma5', 'demand_ma6', 'demand_ma7', 'demand_ma14', 'demand_ma30', 'demand_ma60', 'demand_ma90', 'demand_ma180', 'demand_ma365', 'seasonal', 'trend', 'residual', '7d_avg_quote', '7d_avg_price', '14d_avg_quote', '14d_avg_price', '30d_avg_quote', '30d_avg_price', '60d_avg_quote', '60d_avg_price', '90d_avg_quote', '90d_avg_price', '180d_avg_quote', '180d_avg_price', 'price_diff', 'price_ratio', 'price_volatility_7d', 'price_volatility_14d', 'price_volatility_30d', 'price_volatility_60d', 'price_volatility_90d', 'inquiry_quantity_lag1', 'inquiry_quantity_lag2', 'inquiry_quantity_lag3', 'inquiry_quantity_lag5', 'inquiry_quantity_lag6', 'inquiry_quantity_lag7', 'inquiry_quantity_lag14', 'inquiry_quantity_lag30', 'inquiry_quantity_lag60', 'inquiry_quantity_lag90', 'inquiry_count_lag1', 'inquiry_count_lag2', 'inquiry_count_lag3', 'inquiry_count_lag5', 'inquiry_count_lag6', 'inquiry_count_lag7', 'inquiry_count_lag14', 'inquiry_count_lag30', 'inquiry_count_lag60', 'inquiry_count_lag90', 'our_quote_avg_lag1', 'our_quote_avg_lag2', 'our_quote_avg_lag3', 'our_quote_avg_lag5', 'our_quote_avg_lag6', 'our_quote_avg_lag7', 'our_quote_avg_lag14', 'our_quote_avg_lag30', 'our_quote_avg_lag60', 'our_quote_avg_lag90', 'avg_price_lag1', 'avg_price_lag2', 'avg_price_lag3', 'avg_price_lag5', 'avg_price_lag6', 'avg_price_lag7', 'avg_price_lag14', 'avg_price_lag30', 'avg_price_lag60', 'avg_price_lag90', 'inquiry_count_ma2', 'inquiry_count_ma3', 'inquiry_count_ma5', 'inquiry_count_ma6', 'inquiry_count_ma7', 'inquiry_count_ma14', 'inquiry_count_ma30', 'inquiry_count_ma60', 'inquiry_count_ma90', 'our_quote_avg_ma2', 'our_quote_avg_ma3', 'our_quote_avg_ma5', 'our_quote_avg_ma6', 'our_quote_avg_ma7', 'our_quote_avg_ma14', 'our_quote_avg_ma30', 'our_quote_avg_ma60', 'our_quote_avg_ma90', 'avg_price_ma2', 'avg_price_ma3', 'avg_price_ma5', 'avg_price_ma6', 'avg_price_ma7', 'avg_price_ma14', 'avg_price_ma30', 'avg_price_ma60', 'avg_price_ma90', 'demand_inquiry_ratio', 'demand_inquiry_diff', 'price_demand_ratio', 'seasonal_factor', 'weekday_factor', 'is_future_volatile', 'price_demand_interaction', 'price_inquiry_interaction', 'demand_change_1d', 'demand_change_7d', 'price_change_1d', 'price_change_7d', 'demand_volatility_7d', 'demand_volatility_30d', 'seasonal_trend', 'big_customer_ratio_lag1', 'big_customer_ratio_lag7', 'big_customer_ratio_lag14', 'big_customer_ratio_lag30', 'big_customer_quantity_ratio_lag1', 'big_customer_quantity_ratio_lag7', 'big_customer_quantity_ratio_lag14', 'big_customer_quantity_ratio_lag30', 'last_week_big_customer_ratio_lag1', 'last_week_big_customer_ratio_lag7', 'last_week_big_customer_ratio_lag14', 'last_week_big_customer_ratio_lag30', 'last_week_big_customer_quantity_ratio_lag1', 'last_week_big_customer_quantity_ratio_lag7', 'last_week_big_customer_quantity_ratio_lag14', 'last_week_big_customer_quantity_ratio_lag30', 'last_month_big_customer_ratio_lag1', 'last_month_big_customer_ratio_lag7', 'last_month_big_customer_ratio_lag14', 'last_month_big_customer_ratio_lag30', 'last_month_big_customer_quantity_ratio_lag1', 'last_month_big_customer_quantity_ratio_lag7', 'last_month_big_customer_quantity_ratio_lag14', 'last_month_big_customer_quantity_ratio_lag30', 'big_customer_ratio_ma7', 'big_customer_ratio_ma14', 'big_customer_ratio_ma30', 'big_customer_quantity_ratio_ma7', 'big_customer_quantity_ratio_ma14', 'big_customer_quantity_ratio_ma30', 'last_week_big_customer_ratio_ma7', 'last_week_big_customer_ratio_ma14', 'last_week_big_customer_ratio_ma30', 'last_week_big_customer_quantity_ratio_ma7', 'last_week_big_customer_quantity_ratio_ma14', 'last_week_big_customer_quantity_ratio_ma30', 'last_month_big_customer_ratio_ma7', 'last_month_big_customer_ratio_ma14', 'last_month_big_customer_ratio_ma30', 'last_month_big_customer_quantity_ratio_ma7', 'last_month_big_customer_quantity_ratio_ma14', 'last_month_big_customer_quantity_ratio_ma30', 'big_customer_ratio_change_1d', 'big_customer_ratio_change_7d', 'big_customer_quantity_ratio_change_1d', 'big_customer_quantity_ratio_change_7d', 'last_week_big_customer_ratio_change_1d', 'last_week_big_customer_ratio_change_7d', 'last_week_big_customer_quantity_ratio_change_1d', 'last_week_big_customer_quantity_ratio_change_7d', 'last_month_big_customer_ratio_change_1d', 'last_month_big_customer_ratio_change_7d', 'last_month_big_customer_quantity_ratio_change_1d', 'last_month_big_customer_quantity_ratio_change_7d', 'big_customer_ratio_demand_interaction', 'big_customer_ratio_price_interaction', 'big_customer_quantity_ratio_demand_interaction', 'big_customer_quantity_ratio_price_interaction', 'last_week_big_customer_ratio_demand_interaction', 'last_week_big_customer_ratio_price_interaction', 'last_week_big_customer_quantity_ratio_demand_interaction', 'last_week_big_customer_quantity_ratio_price_interaction', 'last_month_big_customer_ratio_demand_interaction', 'last_month_big_customer_ratio_price_interaction', 'last_month_big_customer_quantity_ratio_demand_interaction', 'last_month_big_customer_quantity_ratio_price_interaction', 'big_customer_ratio_inquiry_interaction', 'big_customer_quantity_ratio_inquiry_interaction', 'last_week_big_customer_ratio_inquiry_interaction', 'last_week_big_customer_quantity_ratio_inquiry_interaction', 'last_month_big_customer_ratio_inquiry_interaction', 'last_month_big_customer_quantity_ratio_inquiry_interaction', 'big_customer_ratio_seasonal_interaction', 'big_customer_quantity_ratio_seasonal_interaction', 'last_week_big_customer_ratio_seasonal_interaction', 'last_week_big_customer_quantity_ratio_seasonal_interaction', 'last_month_big_customer_ratio_seasonal_interaction', 'last_month_big_customer_quantity_ratio_seasonal_interaction', 'product_seasonal_index', 'demand_ema12', 'demand_ema26', 'demand_macd', 'demand_rsi']\n",
      "特征相关性结果已保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_特征相关性/高碳铬铁_特征相关性.xlsx\n",
      "与demand相关性结果已保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_特征相关性/高碳铬铁_与demand相关性.xlsx\n",
      "与demand相关性图已保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_特征相关性/高碳铬铁_与demand相关性.png\n",
      "所有特征相关性矩阵已保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_特征相关性/高碳铬铁_所有特征相关性矩阵.xlsx\n",
      "特征相关性矩阵图已保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_特征相关性/高碳铬铁_特征相关性矩阵.png\n",
      "星期几平均需求图已保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_特征相关性/高碳铬铁_星期几平均需求.png\n",
      "期货波动需求分布图已保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_特征相关性/高碳铬铁_期货波动需求分布.png\n",
      "\n",
      "高碳铬铁 的特征工程和相关性分析完成!\n",
      "\n",
      "================================================================================\n",
      "开始处理 铝锭 的特征工程和相关性分析\n",
      "================================================================================\n",
      "输出目录: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果\n",
      "\n",
      "开始为铝锭执行特征工程...\n",
      "应用增强的特征工程...\n",
      "添加大客户相关特征: 6个\n",
      "执行无泄漏缺失值填充...\n",
      "铝锭特征工程后数据集形状: (1521, 249)\n",
      "=== 修复数据类型问题 ===\n",
      "发现包含年-周格式的列: year_week\n",
      "删除字符串列: ['year_week']\n",
      "\n",
      "=== 最终数据类型检查 ===\n",
      "✓ 所有特征列都是数值类型\n",
      "最终数据形状: (1521, 248)\n",
      "铝锭特征工程数据保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_特征工程数据.xlsx\n",
      "\n",
      "开始全面分析铝锭的特征相关性...\n",
      "特征相关性分析结果将保存在: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_特征相关性\n",
      "数值特征列: ['demand', 'avg_price', 'sales', 'inquiry_count', 'our_quote_avg', 'inquiry_quantity', 'price_std', 'big_customer_inquiries', 'big_customer_quantity', 'big_customer_ratio', 'big_customer_quantity_ratio', 'week', 'last_week_big_customer_ratio', 'last_week_big_customer_quantity_ratio', 'last_month_big_customer_ratio', 'last_month_big_customer_quantity_ratio', 'week_of_year', 'is_Monday', 'is_Tuesday', 'is_Wednesday', 'is_Thursday', 'is_Friday', 'is_Saturday', 'is_Sunday', 'is_holiday', 'is_pre_holiday', 'is_post_holiday', 'is_weekend', 'is_month_start', 'is_month_end', 'demand_lag1', 'demand_lag2', 'demand_lag3', 'demand_lag5', 'demand_lag6', 'demand_lag7', 'demand_lag14', 'demand_lag30', 'demand_lag60', 'demand_lag90', 'demand_lag180', 'demand_lag365', 'demand_ma2', 'demand_ma3', 'demand_ma5', 'demand_ma6', 'demand_ma7', 'demand_ma14', 'demand_ma30', 'demand_ma60', 'demand_ma90', 'demand_ma180', 'demand_ma365', 'seasonal', 'trend', 'residual', '7d_avg_quote', '7d_avg_price', '14d_avg_quote', '14d_avg_price', '30d_avg_quote', '30d_avg_price', '60d_avg_quote', '60d_avg_price', '90d_avg_quote', '90d_avg_price', '180d_avg_quote', '180d_avg_price', 'price_diff', 'price_ratio', 'price_volatility_7d', 'price_volatility_14d', 'price_volatility_30d', 'price_volatility_60d', 'price_volatility_90d', 'inquiry_quantity_lag1', 'inquiry_quantity_lag2', 'inquiry_quantity_lag3', 'inquiry_quantity_lag5', 'inquiry_quantity_lag6', 'inquiry_quantity_lag7', 'inquiry_quantity_lag14', 'inquiry_quantity_lag30', 'inquiry_quantity_lag60', 'inquiry_quantity_lag90', 'inquiry_count_lag1', 'inquiry_count_lag2', 'inquiry_count_lag3', 'inquiry_count_lag5', 'inquiry_count_lag6', 'inquiry_count_lag7', 'inquiry_count_lag14', 'inquiry_count_lag30', 'inquiry_count_lag60', 'inquiry_count_lag90', 'our_quote_avg_lag1', 'our_quote_avg_lag2', 'our_quote_avg_lag3', 'our_quote_avg_lag5', 'our_quote_avg_lag6', 'our_quote_avg_lag7', 'our_quote_avg_lag14', 'our_quote_avg_lag30', 'our_quote_avg_lag60', 'our_quote_avg_lag90', 'avg_price_lag1', 'avg_price_lag2', 'avg_price_lag3', 'avg_price_lag5', 'avg_price_lag6', 'avg_price_lag7', 'avg_price_lag14', 'avg_price_lag30', 'avg_price_lag60', 'avg_price_lag90', 'inquiry_count_ma2', 'inquiry_count_ma3', 'inquiry_count_ma5', 'inquiry_count_ma6', 'inquiry_count_ma7', 'inquiry_count_ma14', 'inquiry_count_ma30', 'inquiry_count_ma60', 'inquiry_count_ma90', 'our_quote_avg_ma2', 'our_quote_avg_ma3', 'our_quote_avg_ma5', 'our_quote_avg_ma6', 'our_quote_avg_ma7', 'our_quote_avg_ma14', 'our_quote_avg_ma30', 'our_quote_avg_ma60', 'our_quote_avg_ma90', 'avg_price_ma2', 'avg_price_ma3', 'avg_price_ma5', 'avg_price_ma6', 'avg_price_ma7', 'avg_price_ma14', 'avg_price_ma30', 'avg_price_ma60', 'avg_price_ma90', 'demand_inquiry_ratio', 'demand_inquiry_diff', 'price_demand_ratio', 'seasonal_factor', 'weekday_factor', 'is_future_volatile', 'price_demand_interaction', 'price_inquiry_interaction', 'demand_change_1d', 'demand_change_7d', 'price_change_1d', 'price_change_7d', 'demand_volatility_7d', 'demand_volatility_30d', 'seasonal_trend', 'big_customer_ratio_lag1', 'big_customer_ratio_lag7', 'big_customer_ratio_lag14', 'big_customer_ratio_lag30', 'big_customer_quantity_ratio_lag1', 'big_customer_quantity_ratio_lag7', 'big_customer_quantity_ratio_lag14', 'big_customer_quantity_ratio_lag30', 'last_week_big_customer_ratio_lag1', 'last_week_big_customer_ratio_lag7', 'last_week_big_customer_ratio_lag14', 'last_week_big_customer_ratio_lag30', 'last_week_big_customer_quantity_ratio_lag1', 'last_week_big_customer_quantity_ratio_lag7', 'last_week_big_customer_quantity_ratio_lag14', 'last_week_big_customer_quantity_ratio_lag30', 'last_month_big_customer_ratio_lag1', 'last_month_big_customer_ratio_lag7', 'last_month_big_customer_ratio_lag14', 'last_month_big_customer_ratio_lag30', 'last_month_big_customer_quantity_ratio_lag1', 'last_month_big_customer_quantity_ratio_lag7', 'last_month_big_customer_quantity_ratio_lag14', 'last_month_big_customer_quantity_ratio_lag30', 'big_customer_ratio_ma7', 'big_customer_ratio_ma14', 'big_customer_ratio_ma30', 'big_customer_quantity_ratio_ma7', 'big_customer_quantity_ratio_ma14', 'big_customer_quantity_ratio_ma30', 'last_week_big_customer_ratio_ma7', 'last_week_big_customer_ratio_ma14', 'last_week_big_customer_ratio_ma30', 'last_week_big_customer_quantity_ratio_ma7', 'last_week_big_customer_quantity_ratio_ma14', 'last_week_big_customer_quantity_ratio_ma30', 'last_month_big_customer_ratio_ma7', 'last_month_big_customer_ratio_ma14', 'last_month_big_customer_ratio_ma30', 'last_month_big_customer_quantity_ratio_ma7', 'last_month_big_customer_quantity_ratio_ma14', 'last_month_big_customer_quantity_ratio_ma30', 'big_customer_ratio_change_1d', 'big_customer_ratio_change_7d', 'big_customer_quantity_ratio_change_1d', 'big_customer_quantity_ratio_change_7d', 'last_week_big_customer_ratio_change_1d', 'last_week_big_customer_ratio_change_7d', 'last_week_big_customer_quantity_ratio_change_1d', 'last_week_big_customer_quantity_ratio_change_7d', 'last_month_big_customer_ratio_change_1d', 'last_month_big_customer_ratio_change_7d', 'last_month_big_customer_quantity_ratio_change_1d', 'last_month_big_customer_quantity_ratio_change_7d', 'big_customer_ratio_demand_interaction', 'big_customer_ratio_price_interaction', 'big_customer_quantity_ratio_demand_interaction', 'big_customer_quantity_ratio_price_interaction', 'last_week_big_customer_ratio_demand_interaction', 'last_week_big_customer_ratio_price_interaction', 'last_week_big_customer_quantity_ratio_demand_interaction', 'last_week_big_customer_quantity_ratio_price_interaction', 'last_month_big_customer_ratio_demand_interaction', 'last_month_big_customer_ratio_price_interaction', 'last_month_big_customer_quantity_ratio_demand_interaction', 'last_month_big_customer_quantity_ratio_price_interaction', 'big_customer_ratio_inquiry_interaction', 'big_customer_quantity_ratio_inquiry_interaction', 'last_week_big_customer_ratio_inquiry_interaction', 'last_week_big_customer_quantity_ratio_inquiry_interaction', 'last_month_big_customer_ratio_inquiry_interaction', 'last_month_big_customer_quantity_ratio_inquiry_interaction', 'big_customer_ratio_seasonal_interaction', 'big_customer_quantity_ratio_seasonal_interaction', 'last_week_big_customer_ratio_seasonal_interaction', 'last_week_big_customer_quantity_ratio_seasonal_interaction', 'last_month_big_customer_ratio_seasonal_interaction', 'last_month_big_customer_quantity_ratio_seasonal_interaction', 'product_seasonal_index', 'demand_ema12', 'demand_ema26', 'demand_macd', 'demand_rsi']\n",
      "特征相关性结果已保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_特征相关性/铝锭_特征相关性.xlsx\n",
      "与demand相关性结果已保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_特征相关性/铝锭_与demand相关性.xlsx\n",
      "与demand相关性图已保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_特征相关性/铝锭_与demand相关性.png\n",
      "所有特征相关性矩阵已保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_特征相关性/铝锭_所有特征相关性矩阵.xlsx\n",
      "特征相关性矩阵图已保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_特征相关性/铝锭_特征相关性矩阵.png\n",
      "星期几平均需求图已保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_特征相关性/铝锭_星期几平均需求.png\n",
      "期货波动需求分布图已保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_特征相关性/铝锭_期货波动需求分布.png\n",
      "\n",
      "铝锭 的特征工程和相关性分析完成!\n",
      "\n",
      "所有处理完成!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import holidays\n",
    "import warnings\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置中文字体\n",
    "# plt.rcParams['font.sans-serif'] = ['SimHei']  \n",
    "# plt.rcParams['axes.unicode_minus'] = False  \n",
    "\n",
    "# =============================================================================\n",
    "# 特征工程函数（增强版）- 修改了缺失值处理策略\n",
    "# =============================================================================\n",
    "def perform_feature_engineering(merged_df, product_name):\n",
    "    \"\"\"为指定品种执行特征工程，使用智能填充策略处理缺失值\"\"\"\n",
    "    print(f\"\\n开始为{product_name}执行特征工程...\")\n",
    "    \n",
    "    # 1. 添加基本时间特征\n",
    "    merged_df['year'] = merged_df['date'].dt.year\n",
    "    merged_df['month'] = merged_df['date'].dt.month\n",
    "    merged_df['day_of_month'] = merged_df['date'].dt.day\n",
    "    merged_df['quarter'] = merged_df['date'].dt.quarter\n",
    "    merged_df['week_of_year'] = merged_df['date'].dt.isocalendar().week.astype(int)\n",
    "    \n",
    "    # 创建星期几的数值变量（0=周一，6=周日）\n",
    "    merged_df['day_of_week_num'] = merged_df['date'].dt.dayofweek\n",
    "    \n",
    "    # 创建星期几的01变量（使用英文变量名）\n",
    "    weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    for i, day_name in enumerate(weekday_names):\n",
    "        merged_df[f'is_{day_name}'] = (merged_df['day_of_week_num'] == i).astype(int)\n",
    "    \n",
    "    # 确保按日期排序\n",
    "    merged_df = merged_df.sort_values('date')\n",
    "\n",
    "    \n",
    "    # 2. 添加节假日特征\n",
    "    def add_holiday_features(df):\n",
    "        \"\"\"添加节假日特征\"\"\"\n",
    "        cn_holidays = holidays.CountryHoliday('CN')\n",
    "        \n",
    "        df['is_holiday'] = df['date'].apply(lambda x: 1 if x in cn_holidays else 0)\n",
    "        df['holiday_name'] = df['date'].apply(lambda x: cn_holidays.get(x, ''))\n",
    "        df['is_pre_holiday'] = df['date'].apply(lambda x: 1 if (x + timedelta(days=1)) in cn_holidays else 0)\n",
    "        df['is_post_holiday'] = df['date'].apply(lambda x: 1 if (x - timedelta(days=1)) in cn_holidays else 0)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    merged_df = add_holiday_features(merged_df)\n",
    "\n",
    "\n",
    "    \n",
    "    # 添加更多时间特征\n",
    "    def add_time_features(df):\n",
    "        \"\"\"添加更多时间特征\"\"\"\n",
    "        df = df.copy()\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['day_of_year'] = df['date'].dt.dayofyear\n",
    "        df['is_weekend'] = (df['day_of_week_num'] >= 5).astype(int)  # 周六和周日\n",
    "        df['quarter'] = df['date'].dt.quarter\n",
    "        df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "        df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "        return df\n",
    "\n",
    "    # 添加滞后特征（扩展更多滞后天数）\n",
    "    def add_lag_features(df, lags=[1, 2, 3, 5, 6, 7, 14, 30, 60, 90, 180, 365]):\n",
    "        \"\"\"添加滞后特征\"\"\"\n",
    "        df = df.copy()\n",
    "        for lag in lags:\n",
    "            df[f'demand_lag{lag}'] = df['demand'].shift(lag)\n",
    "        return df\n",
    "\n",
    "    # 添加移动平均特征（扩展更多窗口大小）--已修改\n",
    "    def add_moving_average_features(df, windows=[2, 3, 5, 6, 7, 14, 30, 60, 90, 180, 365]):\n",
    "        \"\"\"添加移动平均特征\"\"\"\n",
    "        df = df.copy()\n",
    "        for window in windows:\n",
    "            df[f'demand_ma{window}'] = df['demand'].shift(1).rolling(window, min_periods=1).mean()\n",
    "        return df\n",
    "\n",
    "    def add_seasonal_features(df):\n",
    "        \"\"\"\n",
    "        添加季节性分解特征，每行只使用历史数据（不包含本行）\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df['seasonal'] = np.nan\n",
    "        df['trend'] = np.nan\n",
    "        df['residual'] = np.nan\n",
    "        \n",
    "        # 最小需要的数据量（至少2个周期 + 一些额外数据）\n",
    "        min_data_points = 21  # 3周的数据\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            if i < min_data_points:\n",
    "                # 数据不够，保持为NaN\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # 只使用当前行之前的数据\n",
    "                historical_data = df['demand'].iloc[:i]  # 不包括当前行i\n",
    "                \n",
    "                # 填充缺失值\n",
    "                temp_demand = historical_data.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "                \n",
    "                # 确保有足够的数据\n",
    "                if len(temp_demand) >= min_data_points and temp_demand.std() > 0:\n",
    "                    # 进行季节性分解\n",
    "                    decomposition = seasonal_decompose(\n",
    "                        temp_demand, \n",
    "                        period=7, \n",
    "                        model='additive', \n",
    "                        extrapolate_trend='freq'\n",
    "                    )\n",
    "                    \n",
    "                    # 取最后一个值作为当前预测\n",
    "                    df.loc[df.index[i], 'seasonal'] = decomposition.seasonal.iloc[-1]\n",
    "                    df.loc[df.index[i], 'trend'] = decomposition.trend.iloc[-1]\n",
    "                    df.loc[df.index[i], 'residual'] = decomposition.resid.iloc[-1]\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"第{i}行季节性分解失败: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return df\n",
    "\n",
    "    # 应用增强的特征工程\n",
    "    print(\"应用增强的特征工程...\")\n",
    "    merged_df = add_time_features(merged_df)\n",
    "    merged_df = add_lag_features(merged_df)\n",
    "    merged_df = add_moving_average_features(merged_df)\n",
    "    merged_df = add_seasonal_features(merged_df)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 3. 添加价格相关特征（扩展更多价格特征）\n",
    "    # 计算不同时间窗口的平均报价\n",
    "    price_windows = [7, 14, 30, 60, 90, 180]\n",
    "    for window in price_windows:\n",
    "        merged_df[f'{window}d_avg_quote'] = merged_df['our_quote_avg'].rolling(window, min_periods=1).mean().shift(1)\n",
    "        merged_df[f'{window}d_avg_price'] = merged_df['avg_price'].rolling(window, min_periods=1).mean().shift(1)\n",
    "    \n",
    "    # 计算价格差异和比率\n",
    "    merged_df['price_diff'] = merged_df['30d_avg_quote'] - merged_df['30d_avg_price']\n",
    "    merged_df['price_ratio'] = merged_df['30d_avg_quote'] / (merged_df['30d_avg_price'] + 1e-5)\n",
    "    \n",
    "    # 计算价格波动率（不同时间窗口）\n",
    "    volatility_windows = [7, 14, 30, 60, 90]\n",
    "    for window in volatility_windows:\n",
    "        merged_df[f'price_volatility_{window}d'] = merged_df['avg_price'].rolling(window, min_periods=1).std().shift(1)\n",
    "\n",
    "    \n",
    "    # 4. 添加滞后特征（扩展更多特征和滞后天数）\n",
    "    lag_features = ['demand', 'inquiry_quantity', 'inquiry_count', 'our_quote_avg', 'avg_price']\n",
    "    lag_periods = [1, 2, 3, 5, 6, 7, 14, 30, 60, 90]\n",
    "    \n",
    "    for feature in lag_features:\n",
    "        for lag in lag_periods:\n",
    "            merged_df[f'{feature}_lag{lag}'] = merged_df[feature].shift(lag)\n",
    "\n",
    "    \n",
    "    # 5. 添加移动平均特征（扩展更多特征和窗口大小）\n",
    "    window_sizes = [2, 3, 5, 6, 7, 14, 30, 60, 90]\n",
    "    for feature in ['demand', 'inquiry_count', 'our_quote_avg', 'avg_price']:\n",
    "        for window in window_sizes:\n",
    "            merged_df[f'{feature}_ma{window}'] = merged_df[feature].rolling(window, min_periods=1).mean().shift(1)\n",
    "            \n",
    "    \n",
    "    # 6. 添加需求-询价比例特征（扩展更多组合）\n",
    "    merged_df['demand_inquiry_ratio'] = merged_df['demand_lag30'] / (merged_df['inquiry_quantity_lag30'] + 1)\n",
    "    merged_df['demand_inquiry_diff'] = merged_df['demand_lag30'] - merged_df['inquiry_quantity_lag30']\n",
    "    \n",
    "    # 添加价格-需求比率\n",
    "    merged_df['price_demand_ratio'] = merged_df['avg_price_lag7'] / (merged_df['demand_lag7'] + 1)\n",
    "    \n",
    "    \n",
    "    # 7. 添加季节性因子\n",
    "    def add_seasonal_factors_no_leak(df):\n",
    "        \"\"\"添加季节性因子，避免数据泄漏\"\"\"\n",
    "        df = df.copy()\n",
    "        df['seasonal_factor'] = np.nan\n",
    "        \n",
    "        # 需要足够的历史数据来计算季节性\n",
    "        min_months = 13  # 至少需要13个月的数据\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            if i < min_months * 30:  # 粗略估计，假设每月30天\n",
    "                continue\n",
    "                \n",
    "            # 只使用当前行之前的历史数据\n",
    "            historical_data = df.iloc[:i].copy()\n",
    "            \n",
    "            if len(historical_data) > 0:\n",
    "                # 计算历史数据的月度平均和总体平均\n",
    "                monthly_avg_hist = historical_data.groupby('month')['demand'].mean()\n",
    "                overall_avg_hist = historical_data['demand'].mean()\n",
    "                \n",
    "                # 获取当前行的月份\n",
    "                current_month = df.iloc[i]['month']\n",
    "                \n",
    "                # 如果历史数据中有该月份的记录\n",
    "                if current_month in monthly_avg_hist.index:\n",
    "                    seasonal_factor = monthly_avg_hist[current_month] / overall_avg_hist\n",
    "                    df.iloc[i, df.columns.get_loc('seasonal_factor')] = seasonal_factor\n",
    "        \n",
    "        return df\n",
    "    merged_df = add_seasonal_factors_no_leak(merged_df)\n",
    "\n",
    "    \n",
    "    # 8. 添加星期因子（使用星期几01变量）\n",
    "    # 计算每周总需求\n",
    "    def add_weekday_factors_no_leak(df):\n",
    "        \"\"\"添加星期因子，避免数据泄漏\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # 确保有year_week列\n",
    "        df['year_week'] = df['date'].dt.strftime('%Y-%U')\n",
    "        df['weekday_factor'] = np.nan\n",
    "        \n",
    "        # 需要足够的历史数据（至少8周）\n",
    "        min_days = 56\n",
    "        \n",
    "        for i in range(min_days, len(df)):\n",
    "            # 只使用历史数据\n",
    "            historical_data = df.iloc[:i].copy()\n",
    "            \n",
    "            if len(historical_data) > 0:\n",
    "                # 计算历史数据中每周的需求总和\n",
    "                weekly_totals = historical_data.groupby('year_week')['demand'].sum()\n",
    "                \n",
    "                # 为历史数据计算weekday_demand_ratio\n",
    "                historical_data['weekly_total'] = historical_data['year_week'].map(weekly_totals)\n",
    "                historical_data['hist_weekday_demand_ratio'] = historical_data['demand'] / (historical_data['weekly_total'] + 1e-5)\n",
    "                \n",
    "                # 计算每个星期几的平均占比（基于历史数据）\n",
    "                weekday_avg_ratio = {}\n",
    "                \n",
    "                for dow_num in range(7):\n",
    "                    mask = historical_data['day_of_week_num'] == dow_num\n",
    "                    if mask.any():\n",
    "                        avg_ratio = historical_data.loc[mask, 'hist_weekday_demand_ratio'].mean()\n",
    "                        weekday_avg_ratio[dow_num] = avg_ratio\n",
    "                    else:\n",
    "                        weekday_avg_ratio[dow_num] = 1.0/7  # 默认值\n",
    "                \n",
    "                # 为当前行分配星期因子\n",
    "                current_dow = df.iloc[i]['day_of_week_num']\n",
    "                df.iloc[i, df.columns.get_loc('weekday_factor')] = weekday_avg_ratio.get(current_dow, 1.0/7)\n",
    "        \n",
    "        # 清理临时列\n",
    "        df.drop(columns=['year_week'], inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    # 使用修改后的函数\n",
    "    merged_df = add_weekday_factors_no_leak(merged_df)\n",
    "\n",
    "\n",
    "    # 9. 添加期货市场参考价波动特征\n",
    "    def add_future_volatility_no_leak(df):\n",
    "        \"\"\"添加期货波动特征，避免数据泄漏\"\"\"\n",
    "        df = df.copy()\n",
    "        df['year_week'] = df['date'].dt.strftime('%Y-%U')\n",
    "        df['is_future_volatile'] = 0  # 默认值\n",
    "        \n",
    "        # 需要足够的历史数据\n",
    "        min_days = 14  # 至少2周的数据\n",
    "        \n",
    "        for i in range(min_days, len(df)):\n",
    "            current_date = df.iloc[i]['date']\n",
    "            current_week = df.iloc[i]['year_week']\n",
    "            \n",
    "            # 只使用历史数据（当前行之前的数据）\n",
    "            historical_data = df.iloc[:i]\n",
    "            \n",
    "            if len(historical_data) > 0:\n",
    "                # 方案1：使用上一个完整周的数据\n",
    "                # 找到上一个完整的周\n",
    "                prev_weeks = historical_data['year_week'].unique()\n",
    "                if len(prev_weeks) > 0:\n",
    "                    # 取最近的完整周\n",
    "                    latest_complete_week = prev_weeks[-1]\n",
    "                    week_data = historical_data[historical_data['year_week'] == latest_complete_week]\n",
    "                    \n",
    "                    if len(week_data) >= 5:  # 确保有足够天数的数据\n",
    "                        week_min = week_data['avg_price'].min()\n",
    "                        week_max = week_data['avg_price'].max()\n",
    "                        \n",
    "                        if week_min > 0:\n",
    "                            future_range = week_max - week_min\n",
    "                            future_range_pct = future_range / week_min\n",
    "                            \n",
    "                            # 判断是否波动超过5%\n",
    "                            if future_range_pct > 0.05:\n",
    "                                df.iloc[i, df.columns.get_loc('is_future_volatile')] = 1\n",
    "        \n",
    "        return df\n",
    "    merged_df = add_future_volatility_no_leak(merged_df)\n",
    "    \n",
    "    # 10. 添加更多时间特征\n",
    "    merged_df['day_of_year'] = merged_df['date'].dt.dayofyear\n",
    "    \n",
    "    \n",
    "    # # 13. 添加下个月需求特征（用于相关性分析）\n",
    "    # # 创建月份列\n",
    "    # merged_df['year_month'] = merged_df['date'].dt.to_period('M')\n",
    "    \n",
    "    # # 计算每月平均需求\n",
    "    # monthly_demand = merged_df.groupby('year_month')['demand'].mean().reset_index()\n",
    "    # monthly_demand.rename(columns={'demand': 'next_month_demand'}, inplace=True)\n",
    "    \n",
    "    # # 将下个月需求添加到原始数据\n",
    "    # merged_df['next_month'] = merged_df['year_month'] + 1\n",
    "    # merged_df = merged_df.merge(monthly_demand, left_on='next_month', right_on='year_month', how='left')\n",
    "    # merged_df.rename(columns={'next_month_demand': 'next_month_demand'}, inplace=True)\n",
    "    \n",
    "    # # 删除临时列（如果存在）\n",
    "    # temp_cols = ['year_month', 'next_month', 'year_month_x', 'year_month_y']\n",
    "    # cols_to_drop = [col for col in temp_cols if col in merged_df.columns]\n",
    "    # if cols_to_drop:\n",
    "    #     merged_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    \n",
    "    # 14. 添加组合特征（价格与需求的交互）\n",
    "    merged_df['price_demand_interaction'] = merged_df['avg_price_lag7'] * merged_df['demand_lag7']\n",
    "    merged_df['price_inquiry_interaction'] = merged_df['avg_price_lag7'] * merged_df['inquiry_count_lag7']\n",
    "    \n",
    "    # 15. 添加变化率特征（修正版）\n",
    "    # 计算前一期相对于更前一期的变化率\n",
    "    merged_df['demand_change_1d'] = merged_df['demand'].pct_change(periods=1).shift(1)\n",
    "    merged_df['demand_change_7d'] = merged_df['demand'].pct_change(periods=7).shift(1)\n",
    "    merged_df['price_change_1d'] = merged_df['avg_price'].pct_change(periods=1).shift(1)\n",
    "    merged_df['price_change_7d'] = merged_df['avg_price'].pct_change(periods=7).shift(1)\n",
    "\n",
    "    # 16. 添加波动率特征（修正版）\n",
    "    merged_df['demand_volatility_7d'] = merged_df['demand'].rolling(7).std().shift(1)\n",
    "    merged_df['demand_volatility_30d'] = merged_df['demand'].rolling(30).std().shift(1)\n",
    "    \n",
    "    # 17. 添加季节性趋势特征\n",
    "    merged_df['seasonal_trend'] = merged_df['seasonal'] * merged_df['trend']\n",
    "    \n",
    "    # 18. 添加大客户相关特征（新增）\n",
    "    # 检查是否存在大客户相关特征\n",
    "    big_customer_features = [\n",
    "        'big_customer_ratio', 'big_customer_quantity_ratio',\n",
    "        'last_week_big_customer_ratio', 'last_week_big_customer_quantity_ratio',\n",
    "        'last_month_big_customer_ratio', 'last_month_big_customer_quantity_ratio'\n",
    "    ]\n",
    "    \n",
    "    existing_features = [col for col in big_customer_features if col in merged_df.columns]\n",
    "    \n",
    "    if existing_features:\n",
    "        print(f\"添加大客户相关特征: {len(existing_features)}个\")\n",
    "        \n",
    "        # 添加大客户特征的滞后特征\n",
    "        for feature in existing_features:\n",
    "            for lag in [1, 7, 14, 30]:\n",
    "                merged_df[f'{feature}_lag{lag}'] = merged_df[feature].shift(lag)\n",
    "        \n",
    "        # 添加大客户特征的移动平均\n",
    "        for feature in existing_features:\n",
    "            for window in [7, 14, 30]:\n",
    "                merged_df[f'{feature}_ma{window}'] = merged_df[feature].rolling(window, min_periods=1).mean().shift(1)\n",
    "        \n",
    "        # 添加大客户特征的变化率\n",
    "        # 修正变化率特征\n",
    "        for feature in existing_features:\n",
    "            merged_df[f'{feature}_change_1d'] = merged_df[feature].pct_change(periods=1).shift(1)  # 加shift(1)\n",
    "            merged_df[f'{feature}_change_7d'] = merged_df[feature].pct_change(periods=7).shift(1)  # 加shift(1)\n",
    "        \n",
    "        # 添加大客户特征与需求特征的交互\n",
    "        for feature in existing_features:\n",
    "            merged_df[f'{feature}_demand_interaction'] = merged_df[feature] * merged_df['demand_lag7']\n",
    "            merged_df[f'{feature}_price_interaction'] = merged_df[feature] * merged_df['avg_price_lag7']\n",
    "        \n",
    "        # 添加大客户特征与询价特征的交互\n",
    "        for feature in existing_features:\n",
    "            merged_df[f'{feature}_inquiry_interaction'] = merged_df[feature] * merged_df['inquiry_count_lag7']\n",
    "        \n",
    "        # 添加大客户特征与季节因子的交互\n",
    "        for feature in existing_features:\n",
    "            merged_df[f'{feature}_seasonal_interaction'] = merged_df[feature] * merged_df['seasonal_factor']\n",
    "    else:\n",
    "        print(f\"警告: {product_name}没有大客户相关特征\")\n",
    "    \n",
    "    # 19. 添加行业特定特征\n",
    "    # 添加产品季节性指数（根据产品类型）\n",
    "    if product_name == '电解镍':\n",
    "        # 电解镍在第二季度需求较高\n",
    "        merged_df['product_seasonal_index'] = np.where(\n",
    "            merged_df['quarter'] == 2, 1.2, \n",
    "            np.where(merged_df['quarter'] == 4, 0.9, 1.0)\n",
    "        )\n",
    "    elif product_name == '高碳铬铁':\n",
    "        # 高碳铬铁在第一季度需求较高\n",
    "        merged_df['product_seasonal_index'] = np.where(\n",
    "            merged_df['quarter'] == 1, 1.3, \n",
    "            np.where(merged_df['quarter'] == 3, 0.8, 1.0)\n",
    "        )\n",
    "    else:  # 铝锭\n",
    "        # 铝锭在第三季度需求较高\n",
    "        merged_df['product_seasonal_index'] = np.where(\n",
    "            merged_df['quarter'] == 3, 1.1, \n",
    "            np.where(merged_df['quarter'] == 1, 0.95, 1.0)\n",
    "        )\n",
    "    \n",
    "    # 20. 添加技术指标特征（修正版）\n",
    "\n",
    "    # 添加需求MACD指标（修正版）\n",
    "    merged_df['demand_ema12'] = merged_df['demand'].ewm(span=12, adjust=False).mean().shift(1)\n",
    "    merged_df['demand_ema26'] = merged_df['demand'].ewm(span=26, adjust=False).mean().shift(1) \n",
    "    merged_df['demand_macd'] = merged_df['demand_ema12'] - merged_df['demand_ema26']\n",
    "\n",
    "    # 添加需求RSI指标（修正版）\n",
    "    delta = merged_df['demand'].diff().shift(1)  # shift差值\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=14, min_periods=1).mean()\n",
    "    rs = avg_gain / (avg_loss + 1e-5)\n",
    "    merged_df['demand_rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 智能填充缺失值（而不是删除）\n",
    "    # =============================================================================\n",
    "    # 简单版本：只用前向填充，不会泄漏\n",
    "    print(\"执行无泄漏缺失值填充...\")\n",
    "\n",
    "    numeric_cols = merged_df.select_dtypes(include=[np.number]).columns\n",
    "    demand_related = [col for col in numeric_cols if 'demand' in col or 'inquiry' in col or 'price' in col]\n",
    "\n",
    "    for col in demand_related:\n",
    "        # 只用前向填充，不用后向填充\n",
    "        merged_df[col] = merged_df[col].fillna(method='ffill')\n",
    "        \n",
    "        # 如果还有缺失值（最开始的几行），用0填充\n",
    "        merged_df[col] = merged_df[col].fillna(0)\n",
    "\n",
    "    other_numeric = [col for col in numeric_cols if col not in demand_related]\n",
    "    merged_df[other_numeric] = merged_df[other_numeric].fillna(0)\n",
    "    \n",
    "    # 2. 填充非数值型特征\n",
    "    non_numeric_cols = merged_df.select_dtypes(exclude=[np.number]).columns\n",
    "    for col in non_numeric_cols:\n",
    "        if col == 'date':\n",
    "            continue  # 日期列不应该有缺失值\n",
    "        merged_df[col] = merged_df[col].fillna('unknown')\n",
    "    \n",
    "    # 检查是否还有缺失值\n",
    "    if merged_df.isnull().any().any():\n",
    "        missing_cols = merged_df.columns[merged_df.isnull().any()].tolist()\n",
    "        print(f\"警告: {product_name}数据中仍有缺失值列: {missing_cols}\")\n",
    "        # 用0填充剩余的缺失值\n",
    "        merged_df.fillna(0, inplace=True)\n",
    "    \n",
    "    print(f\"{product_name}特征工程后数据集形状: {merged_df.shape}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 特征相关性分析函数（增强版）\n",
    "# =============================================================================\n",
    "def analyze_feature_correlations(merged_df, product_name, output_dir):\n",
    "    \"\"\"全面分析特征与当前需求及下个月平均需求的相关性\"\"\"\n",
    "    print(f\"\\n开始全面分析{product_name}的特征相关性...\")\n",
    "    \n",
    "    # 创建相关性分析输出目录\n",
    "    corr_dir = os.path.join(output_dir, f\"{product_name}_特征相关性\")\n",
    "    os.makedirs(corr_dir, exist_ok=True)\n",
    "    print(f\"特征相关性分析结果将保存在: {corr_dir}\")\n",
    "    \n",
    "    # 1. 计算特征与当前需求的相关性\n",
    "    numeric_cols = merged_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    exclude_cols = ['date', 'holiday_name']\n",
    "    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    # 确保星期几特征被包含\n",
    "    weekday_features = [f'is_{day}' for day in ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']]\n",
    "    for feature in weekday_features:\n",
    "        if feature not in numeric_cols and feature in merged_df.columns:\n",
    "            numeric_cols.append(feature)\n",
    "            print(f\"添加星期几特征: {feature}\")\n",
    "    \n",
    "    # 打印所有数值列名以验证\n",
    "    print(f\"数值特征列: {list(numeric_cols)}\")\n",
    "    \n",
    "    # 计算相关系数\n",
    "    corr_with_demand = merged_df[numeric_cols].corrwith(merged_df['demand'])\n",
    "    corr_with_demand = corr_with_demand.sort_values(ascending=False)\n",
    "    \n",
    "    # # 计算特征与下个月平均需求的相关性\n",
    "    # corr_with_next_month = merged_df[numeric_cols].corrwith(merged_df['next_month_demand'])\n",
    "    # corr_with_next_month = corr_with_next_month.sort_values(ascending=False)\n",
    "    \n",
    "    # 创建相关性DataFrame\n",
    "    corr_df = pd.DataFrame({\n",
    "        'feature': corr_with_demand.index,\n",
    "        'correlation_with_current_demand': corr_with_demand.values,\n",
    "        'abs_correlation_current': np.abs(corr_with_demand.values)\n",
    "    })\n",
    "    \n",
    "    # 添加特征描述\n",
    "    feature_descriptions = {\n",
    "        'demand': '当前需求',\n",
    "        'year': '年份',\n",
    "        'month': '月份',\n",
    "        'day_of_month': '月中第几天',\n",
    "        'is_month_start': '是否是月初（1=是，0=否）',\n",
    "        'is_month_end': '是否是月末（1=是，0=否）',\n",
    "        'quarter': '季度',\n",
    "        'week_of_year': '年中第几周',\n",
    "        'is_holiday': '是否是节假日（1=是，0=否）',\n",
    "        'is_pre_holiday': '是否是节假日前一天（1=是，0=否）',\n",
    "        'is_post_holiday': '是否是节假日后一天（1=是，0=否）',\n",
    "        'day_of_year': '年中第几天',\n",
    "        'is_weekend': '是否是周末（1=是，0=否）',\n",
    "        'seasonal': '季节性分量（类似于arima的分解）',\n",
    "        'trend': '趋势分量（类似于arima的分解）',\n",
    "        'residual': '残差分量（类似于arima的分解）',\n",
    "        '30d_avg_quote': '前30天平均报价',\n",
    "        '30d_avg_price': '前30天平均价格',\n",
    "        'price_diff': '报价与价格差异',\n",
    "        'price_ratio': '报价与价格比率',\n",
    "        'price_volatility': '价格波动率',\n",
    "        'demand_inquiry_ratio': '需求与询价比例',\n",
    "        'demand_inquiry_diff': '需求与询价差异',\n",
    "        'seasonal_factor': '季节性因子（当季节需求在全年的占比）',\n",
    "        'weekday_factor': '星期因子（当日需求在全周的占比）',\n",
    "        'inquiry_quantity': '询价量',\n",
    "        'inquiry_count': '询价次数',\n",
    "        'avg_price': '期货市场参考价',\n",
    "        'is_future_volatile': '当周期货价格波动是否超过10%（1=是，0=否）',\n",
    "        'price_demand_ratio': '价格与需求比率',\n",
    "        'price_demand_interaction': '价格与需求交互特征',\n",
    "        'price_inquiry_interaction': '价格与询价交互特征',\n",
    "        'demand_change_1d': '需求日变化率',\n",
    "        'demand_change_7d': '需求周变化率',\n",
    "        'price_change_1d': '价格日变化率',\n",
    "        'price_change_7d': '价格周变化率',\n",
    "        'demand_volatility_7d': '需求7天波动率',\n",
    "        'demand_volatility_30d': '需求30天波动率',\n",
    "        'seasonal_trend': '季节性趋势特征',\n",
    "        'product_seasonal_index': '产品季节性指数（基于产品类型）',\n",
    "        'demand_ema12': '需求12天指数移动平均',\n",
    "        'demand_ema26': '需求26天指数移动平均',\n",
    "        'demand_macd': '需求MACD指标（12天和26天EMA的差值）',\n",
    "        'demand_rsi': '需求RSI指标（14天相对强度指数）'\n",
    "    }\n",
    "    \n",
    "    # 为星期几01变量添加描述\n",
    "    weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    for day_name in weekday_names:\n",
    "        feature_descriptions[f'is_{day_name}'] = f'是否为{day_name}（1=是，0=否）'\n",
    "    \n",
    "    # 为滞后特征和移动平均特征添加描述\n",
    "    for lag in [1, 2, 3, 5, 6, 7, 14, 30, 60, 90, 180, 365]:\n",
    "        feature_descriptions[f'demand_lag{lag}'] = f'滞后{lag}天需求'\n",
    "        feature_descriptions[f'demand_ma{lag}'] = f'{lag}天移动平均需求'\n",
    "    \n",
    "    for feature in ['inquiry_quantity', 'inquiry_count', 'our_quote_avg', 'avg_price']:\n",
    "        for lag in [1, 2, 3, 5, 6, 7, 14, 30, 60, 90]:\n",
    "            feature_descriptions[f'{feature}_lag{lag}'] = f'滞后{lag}天{feature}'\n",
    "    \n",
    "    # 为价格特征添加描述\n",
    "    for window in [7, 14, 30, 60, 90, 180]:\n",
    "        feature_descriptions[f'{window}d_avg_quote'] = f'前{window}天平均报价'\n",
    "        feature_descriptions[f'{window}d_avg_price'] = f'前{window}天平均价格'\n",
    "    \n",
    "    for window in [7, 14, 30, 60, 90]:\n",
    "        feature_descriptions[f'price_volatility_{window}d'] = f'{window}天价格波动率'\n",
    "    \n",
    "    # 为大客户特征添加描述\n",
    "    big_customer_features = [\n",
    "        'big_customer_ratio', 'big_customer_quantity_ratio',\n",
    "        'last_week_big_customer_ratio', 'last_week_big_customer_quantity_ratio',\n",
    "        'last_month_big_customer_ratio', 'last_month_big_customer_quantity_ratio'\n",
    "    ]\n",
    "    \n",
    "    big_customer_descriptions = {\n",
    "        'big_customer_ratio': '大客户询价次数占比',\n",
    "        'big_customer_quantity_ratio': '大客户询价数量占比',\n",
    "        'last_week_big_customer_ratio': '上周大客户询价次数占比',\n",
    "        'last_week_big_customer_quantity_ratio': '上周大客户询价数量占比',\n",
    "        'last_month_big_customer_ratio': '上个月大客户询价次数占比',\n",
    "        'last_month_big_customer_quantity_ratio': '上个月大客户询价数量占比'\n",
    "    }\n",
    "    \n",
    "    # 为大客户衍生特征添加描述\n",
    "    for feature in big_customer_features:\n",
    "        # 滞后特征\n",
    "        for lag in [1, 7, 14, 30]:\n",
    "            feature_descriptions[f'{feature}_lag{lag}'] = f'滞后{lag}天{big_customer_descriptions[feature]}'\n",
    "        \n",
    "        # 移动平均特征\n",
    "        for window in [7, 14, 30]:\n",
    "            feature_descriptions[f'{feature}_ma{window}'] = f'{window}天移动平均{big_customer_descriptions[feature]}'\n",
    "        \n",
    "        # 变化率特征\n",
    "        feature_descriptions[f'{feature}_change_1d'] = f'{big_customer_descriptions[feature]}日变化率'\n",
    "        feature_descriptions[f'{feature}_change_7d'] = f'{big_customer_descriptions[feature]}周变化率'\n",
    "        \n",
    "        # 交互特征\n",
    "        feature_descriptions[f'{feature}_demand_interaction'] = f'{big_customer_descriptions[feature]}与需求交互特征'\n",
    "        feature_descriptions[f'{feature}_price_interaction'] = f'{big_customer_descriptions[feature]}与价格交互特征'\n",
    "        feature_descriptions[f'{feature}_inquiry_interaction'] = f'{big_customer_descriptions[feature]}与询价交互特征'\n",
    "        feature_descriptions[f'{feature}_seasonal_interaction'] = f'{big_customer_descriptions[feature]}与季节因子交互特征'\n",
    "    \n",
    "    # 添加特征描述到相关性DataFrame\n",
    "    corr_df['description'] = corr_df['feature'].map(feature_descriptions)\n",
    "    \n",
    "    # # 排序（按下个月绝对相关性降序）\n",
    "    # corr_df = corr_df.sort_values('abs_correlation_next_month', ascending=False)\n",
    "    \n",
    "    # 保存相关性结果\n",
    "    corr_output = os.path.join(corr_dir, f\"{product_name}_特征相关性.xlsx\")\n",
    "    corr_df.to_excel(corr_output, index=False)\n",
    "    print(f\"特征相关性结果已保存至: {corr_output}\")\n",
    "    \n",
    "    # 2. 可视化所有特征的相关性\n",
    "    # 创建目标变量列表\n",
    "    targets = ['demand']\n",
    "    \n",
    "    # 为每个目标变量创建相关性图表\n",
    "    for target in targets:\n",
    "        # 计算与目标变量的相关性\n",
    "        corr_with_target = merged_df[numeric_cols].corrwith(merged_df[target])\n",
    "        corr_with_target = corr_with_target.sort_values(ascending=False)\n",
    "        \n",
    "        # 创建相关性DataFrame\n",
    "        target_corr_df = pd.DataFrame({\n",
    "            'feature': corr_with_target.index,\n",
    "            f'correlation_with_{target}': corr_with_target.values,\n",
    "            f'abs_correlation_{target}': np.abs(corr_with_target.values)\n",
    "        })\n",
    "        \n",
    "        # 添加特征描述\n",
    "        target_corr_df['description'] = target_corr_df['feature'].map(feature_descriptions)\n",
    "        \n",
    "        # 排序（按绝对相关性降序）\n",
    "        target_corr_df = target_corr_df.sort_values(f'abs_correlation_{target}', ascending=False)\n",
    "        \n",
    "        # 保存相关性结果\n",
    "        target_corr_output = os.path.join(corr_dir, f\"{product_name}_与{target}相关性.xlsx\")\n",
    "        target_corr_df.to_excel(target_corr_output, index=False)\n",
    "        print(f\"与{target}相关性结果已保存至: {target_corr_output}\")\n",
    "        \n",
    "        # 可视化前30个特征的相关性\n",
    "        plt.figure(figsize=(16, 20))\n",
    "        top_features = target_corr_df.head(30)['feature'].tolist()\n",
    "        top_corr = target_corr_df.head(30)[f'correlation_with_{target}'].values\n",
    "        \n",
    "        # 创建水平条形图\n",
    "        plt.barh(top_features, top_corr, color='skyblue')\n",
    "        plt.xlabel('相关系数')\n",
    "        plt.ylabel('特征')\n",
    "        plt.title(f'{product_name}特征与{target}相关性 (Top 30)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(corr_dir, f'{product_name}_与{target}相关性.png'), dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"与{target}相关性图已保存至: {os.path.join(corr_dir, f'{product_name}_与{target}相关性.png')}\")\n",
    "    \n",
    "    # 3. 可视化相关性矩阵\n",
    "    # 选择所有特征\n",
    "    all_features = numeric_cols + ['demand']\n",
    "    \n",
    "    # 计算相关系数矩阵\n",
    "    corr_matrix = merged_df[all_features].corr()\n",
    "    \n",
    "    # 保存完整相关性矩阵\n",
    "    corr_matrix_output = os.path.join(corr_dir, f\"{product_name}_所有特征相关性矩阵.xlsx\")\n",
    "    corr_matrix.to_excel(corr_matrix_output)\n",
    "    print(f\"所有特征相关性矩阵已保存至: {corr_matrix_output}\")\n",
    "    \n",
    "    # 可视化（仅显示与目标变量相关性较高的特征）\n",
    "    # 选择与下个月需求相关性最高的30个特征\n",
    "    top_features = corr_df.head(30)['feature'].tolist()\n",
    "    if 'demand' not in top_features:\n",
    "        top_features.append('demand')\n",
    "    # if 'next_month_demand' not in top_features:\n",
    "    #     top_features.append('next_month_demand')\n",
    "    \n",
    "    # 计算相关系数矩阵\n",
    "    corr_matrix = merged_df[top_features].corr()\n",
    "    \n",
    "    # 可视化\n",
    "    plt.figure(figsize=(24, 20))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "    plt.title(f'{product_name}特征相关性矩阵 (Top 30)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(corr_dir, f'{product_name}_特征相关性矩阵.png'), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"特征相关性矩阵图已保存至: {os.path.join(corr_dir, f'{product_name}_特征相关性矩阵.png')}\")\n",
    "    \n",
    "    # # 4. 可视化与下个月需求相关性最高的特征\n",
    "    # top_corr_features = corr_df.head(20)['feature'].tolist()\n",
    "    \n",
    "    # # 创建子图\n",
    "    # fig, axes = plt.subplots(5, 4, figsize=(24, 30))\n",
    "    # axes = axes.flatten()\n",
    "    \n",
    "    # for i, feature in enumerate(top_corr_features):\n",
    "    #     if feature == 'next_month_demand':\n",
    "    #         continue\n",
    "            \n",
    "    #     # 绘制散点图\n",
    "    #     sns.scatterplot(data=merged_df, x=feature, y='next_month_demand', ax=axes[i])\n",
    "    #     axes[i].set_title(f'{feature} vs 下个月需求 (r={corr_with_next_month[feature]:.2f})')\n",
    "    #     axes[i].set_xlabel(feature_descriptions.get(feature, feature))\n",
    "    #     axes[i].set_ylabel('下个月平均需求')\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(os.path.join(corr_dir, f'{product_name}_与下个月需求相关性最高的特征.png'), dpi=300)\n",
    "    # plt.close()\n",
    "    # print(f\"与下个月需求相关性最高的特征图已保存至: {os.path.join(corr_dir, f'{product_name}_与下个月需求相关性最高的特征.png')}\")\n",
    "    \n",
    "    # 5. 添加星期几特征的可视化\n",
    "    weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    weekday_demand = []\n",
    "    weekday_next_month_demand = []\n",
    "    \n",
    "    for day_name in weekday_names:\n",
    "        col_name = f'is_{day_name}'\n",
    "        if col_name in merged_df.columns:\n",
    "            day_demand = merged_df.loc[merged_df[col_name] == 1, 'demand'].mean()\n",
    "            weekday_demand.append(day_demand)\n",
    "    \n",
    "    if weekday_demand:\n",
    "        # 当日需求\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.bar(weekday_names, weekday_demand)\n",
    "        plt.title(f'{product_name}各星期几的平均需求')\n",
    "        plt.xlabel('星期几')\n",
    "        plt.ylabel('平均需求')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(corr_dir, f'{product_name}_星期几平均需求.png'), dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"星期几平均需求图已保存至: {os.path.join(corr_dir, f'{product_name}_星期几平均需求.png')}\")\n",
    "        \n",
    "        # # 下月需求\n",
    "        # plt.figure(figsize=(12, 8))\n",
    "        # plt.bar(weekday_names, weekday_next_month_demand)\n",
    "        # plt.title(f'{product_name}各星期几的下月平均需求')\n",
    "        # plt.xlabel('星期几')\n",
    "        # plt.ylabel('下月平均需求')\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(os.path.join(corr_dir, f'{product_name}_星期几下月平均需求.png'), dpi=300)\n",
    "        # plt.close()\n",
    "        # print(f\"星期几下月平均需求图已保存至: {os.path.join(corr_dir, f'{product_name}_星期几下月平均需求.png')}\")\n",
    "    \n",
    "    # 6. 添加期货波动特征的可视化\n",
    "    if 'is_future_volatile' in merged_df.columns:\n",
    "        # 期货波动状态下的需求分布\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.boxplot(data=merged_df, x='is_future_volatile', y='demand')\n",
    "        plt.title(f'{product_name}期货波动状态下的需求分布')\n",
    "        plt.xlabel('期货波动是否超过10%')\n",
    "        plt.ylabel('需求')\n",
    "        plt.xticks([0, 1], ['否', '是'])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(corr_dir, f'{product_name}_期货波动需求分布.png'), dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"期货波动需求分布图已保存至: {os.path.join(corr_dir, f'{product_name}_期货波动需求分布.png')}\")\n",
    "        \n",
    "        # # 期货波动状态与下个月需求的关系\n",
    "        # plt.figure(figsize=(12, 8))\n",
    "        # sns.boxplot(data=merged_df, x='is_future_volatile', y='next_month_demand')\n",
    "        # plt.title(f'{product_name}期货波动状态与下个月需求')\n",
    "        # plt.xlabel('期货波动是否超过10%')\n",
    "        # plt.ylabel('下个月平均需求')\n",
    "        # plt.xticks([0, 1], ['否', '是'])\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(os.path.join(corr_dir, f'{product_name}_期货波动与下个月需求.png'), dpi=300)\n",
    "        # plt.close()\n",
    "        # print(f\"期货波动与下个月需求图已保存至: {os.path.join(corr_dir, f'{product_name}_期货波动与下个月需求.png')}\")\n",
    "    \n",
    "    # 7. 添加滞后特征的可视化\n",
    "    lag_features = [f'demand_lag{lag}' for lag in [1, 7, 14, 30, 60, 90]]\n",
    "    for lag_feature in lag_features:\n",
    "        if lag_feature in merged_df.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.scatterplot(data=merged_df, x=lag_feature, y='demand')\n",
    "            plt.title(f'{lag_feature} vs 当前需求')\n",
    "            plt.xlabel(lag_feature)\n",
    "            plt.ylabel('当前需求')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(corr_dir, f'{product_name}_{lag_feature}_vs_当前需求.png'), dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            # plt.figure(figsize=(10, 6))\n",
    "            # sns.scatterplot(data=merged_df, x=lag_feature, y='next_month_demand')\n",
    "            # plt.title(f'{lag_feature} vs 下个月需求')\n",
    "            # plt.xlabel(lag_feature)\n",
    "            # plt.ylabel('下个月需求')\n",
    "            # plt.tight_layout()\n",
    "            # plt.savefig(os.path.join(corr_dir, f'{product_name}_{lag_feature}_vs_下个月需求.png'), dpi=300)\n",
    "            # plt.close()\n",
    "    \n",
    "    # 8. 添加价格-需求交互特征的可视化\n",
    "    if 'price_demand_interaction' in merged_df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(data=merged_df, x='price_demand_interaction', y='demand')\n",
    "        plt.title('价格-需求交互特征 vs 需求')\n",
    "        plt.xlabel('价格-需求交互特征')\n",
    "        plt.ylabel('需求')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(corr_dir, f'{product_name}_价格需求交互特征_vs_需求.png'), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 9. 添加变化率特征的可视化\n",
    "    if 'demand_change_7d' in merged_df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(data=merged_df, x='demand_change_7d', y='demand')\n",
    "        plt.title('需求7天变化率 vs 需求')\n",
    "        plt.xlabel('需求7天变化率')\n",
    "        plt.ylabel('需求')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(corr_dir, f'{product_name}_需求变化率_vs_需求.png'), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 10. 添加大客户特征的可视化\n",
    "    big_customer_features = [\n",
    "        'big_customer_ratio', 'last_week_big_customer_ratio', 'last_month_big_customer_ratio'\n",
    "    ]\n",
    "    \n",
    "    for feature in big_customer_features:\n",
    "        if feature in merged_df.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.scatterplot(data=merged_df, x=feature, y='demand')\n",
    "            plt.title(f'{feature} vs 需求')\n",
    "            plt.xlabel(feature_descriptions.get(feature, feature))\n",
    "            plt.ylabel('需求')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(corr_dir, f'{product_name}_{feature}_vs_需求.png'), dpi=300)\n",
    "            plt.close()\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 修复函数\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def fix_data_types_before_model(merged_df):\n",
    "    \"\"\"\n",
    "    在模型训练前修复数据类型问题\n",
    "    \"\"\"\n",
    "    print(\"=== 修复数据类型问题 ===\")\n",
    "    \n",
    "    # 1. 删除所有可能的字符串列（临时列）\n",
    "    string_cols_to_drop = []\n",
    "    for col in merged_df.columns:\n",
    "        if merged_df[col].dtype == 'object' and col not in ['date', 'holiday_name']:\n",
    "            # 检查是否包含 '2023-27' 这样的值\n",
    "            if merged_df[col].astype(str).str.contains(r'\\d{4}-\\d+', na=False).any():\n",
    "                print(f\"发现包含年-周格式的列: {col}\")\n",
    "                string_cols_to_drop.append(col)\n",
    "            elif col.startswith('year_week'):\n",
    "                print(f\"发现年周临时列: {col}\")\n",
    "                string_cols_to_drop.append(col)\n",
    "    \n",
    "    # 删除这些列\n",
    "    if string_cols_to_drop:\n",
    "        print(f\"删除字符串列: {string_cols_to_drop}\")\n",
    "        merged_df = merged_df.drop(columns=string_cols_to_drop)\n",
    "    \n",
    "    # 2. 强制转换所有应该是数值的列\n",
    "    exclude_cols = ['date', 'holiday_name']\n",
    "    numeric_cols = [col for col in merged_df.columns if col not in exclude_cols]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        # 检查这一列是否有非数值数据\n",
    "        original_dtype = merged_df[col].dtype\n",
    "        \n",
    "        if original_dtype == 'object':\n",
    "            print(f\"发现对象类型列: {col}\")\n",
    "            # 尝试转换为数值\n",
    "            numeric_series = pd.to_numeric(merged_df[col], errors='coerce')\n",
    "            \n",
    "            # 检查有多少值无法转换\n",
    "            non_convertible = merged_df[col][numeric_series.isna() & merged_df[col].notna()]\n",
    "            if len(non_convertible) > 0:\n",
    "                print(f\"  无法转换的值: {non_convertible.unique()[:5]}\")\n",
    "                \n",
    "            # 用数值数据替换\n",
    "            merged_df[col] = numeric_series\n",
    "            print(f\"  转换为数值类型: {original_dtype} -> {merged_df[col].dtype}\")\n",
    "        \n",
    "        elif 'int' in str(original_dtype) or 'float' in str(original_dtype):\n",
    "            # 已经是数值类型，检查是否有异常\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"警告: 意外的数据类型 {col}: {original_dtype}\")\n",
    "    \n",
    "    # 3. 填充转换后产生的 NaN 值\n",
    "    numeric_cols = merged_df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if merged_df[col].isnull().sum() > 0:\n",
    "            # 用前向填充，然后用0填充\n",
    "            merged_df[col] = merged_df[col].fillna(method='ffill').fillna(0)\n",
    "            print(f\"填充 {col} 的缺失值\")\n",
    "    \n",
    "    # 4. 最终检查\n",
    "    print(\"\\n=== 最终数据类型检查 ===\")\n",
    "    non_numeric_cols = merged_df.select_dtypes(exclude=[np.number]).columns\n",
    "    non_numeric_cols = [col for col in non_numeric_cols if col not in ['date', 'holiday_name']]\n",
    "    \n",
    "    if len(non_numeric_cols) > 0:\n",
    "        print(f\"仍然存在非数值列: {non_numeric_cols}\")\n",
    "        for col in non_numeric_cols:\n",
    "            print(f\"  {col}: {merged_df[col].dtype}, 样本值: {merged_df[col].head().tolist()}\")\n",
    "    else:\n",
    "        print(\"✓ 所有特征列都是数值类型\")\n",
    "    \n",
    "    print(f\"最终数据形状: {merged_df.shape}\")\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 主程序\n",
    "# =============================================================================\n",
    "\n",
    "# 输入输出路径\n",
    "output_dir = r\"/Users/bytedance/Desktop/特征工程补充/结果\"\n",
    "\n",
    "# 处理三个品种\n",
    "products = ['电解镍', '高碳铬铁', '铝锭']\n",
    "\n",
    "for product in products:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"开始处理 {product} 的特征工程和相关性分析\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 创建产品输出目录\n",
    "    product_output_dir = os.path.join(output_dir, f\"{product}预测结果\")\n",
    "    os.makedirs(product_output_dir, exist_ok=True)\n",
    "    print(f\"输出目录: {product_output_dir}\")\n",
    "    \n",
    "    # 1. 读取之前生成的合并数据\n",
    "    input_path = os.path.join(output_dir, f\"{product}_merged_df.xlsx\")\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"警告: 找不到 {product} 的合并数据文件: {input_path}\")\n",
    "        continue\n",
    "    \n",
    "    merged_df = pd.read_excel(input_path)\n",
    "    \n",
    "    # 检查数据是否为空\n",
    "    if merged_df.empty:\n",
    "        print(f\"警告: {product} 的合并数据为空，跳过处理\")\n",
    "        continue\n",
    "    \n",
    "    # 确保日期列是datetime类型\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date'], errors='coerce')\n",
    "    \n",
    "    # 2. 执行特征工程\n",
    "    feature_df = perform_feature_engineering(merged_df, product)\n",
    "\n",
    "    feature_df = fix_data_types_before_model(feature_df)\n",
    "    \n",
    "    # 保存特征工程后的数据\n",
    "    feature_output = os.path.join(product_output_dir, f\"{product}_特征工程数据.xlsx\")\n",
    "    feature_df.to_excel(feature_output, index=False)\n",
    "    print(f\"{product}特征工程数据保存至: {feature_output}\")\n",
    "    \n",
    "    # 3. 执行特征相关性分析\n",
    "    analyze_feature_correlations(feature_df, product, product_output_dir)\n",
    "    \n",
    "    print(f\"\\n{product} 的特征工程和相关性分析完成!\")\n",
    "\n",
    "print(\"\\n所有处理完成!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45556709",
   "metadata": {},
   "source": [
    "### 3. 特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "283dc987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "开始处理 电解镍 的特征选择\n",
      "================================================================================\n",
      "输出目录: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果\n",
      "初始候选特征数量: 235\n",
      "\n",
      "开始为电解镍执行特征选择...\n",
      "特征选择结果将保存在: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/特征选择结果\n",
      "开始过滤特征，排除实际应用中当天无法获得的特征...\n",
      "原始特征数量: 235\n",
      "可用特征数量: 235\n",
      "被过滤特征数量: 0\n",
      "保留的特征类型包括:\n",
      "  time: 79个特征\n",
      "  lag: 45个特征\n",
      "  ma: 61个特征\n",
      "  seasonal: 8个特征\n",
      "  other: 42个特征\n",
      "特征过滤结果已保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/特征选择结果/电解镍_特征过滤结果.csv\n",
      "清理数据...\n",
      "执行数据清理...\n",
      "删除了 0 行缺失数据。\n",
      "清理后训练集形状: (1216, 235), 测试集形状: (305, 235)\n",
      "使用分位数回归森林(QRF)评估特征重要性...\n",
      "使用递归特征消除(RFE)结合QRF选择特征...\n",
      "Fitting estimator with 235 features.\n",
      "Fitting estimator with 212 features.\n",
      "Fitting estimator with 189 features.\n",
      "Fitting estimator with 166 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 28 features.\n",
      "RFE-QRF选择的特征数量: 20\n",
      "使用XGBoost评估特征重要性...\n",
      "XGBoost选择的特征数量: 118\n",
      "使用LightGBM评估特征重要性...\n",
      "LightGBM选择的特征数量: 126\n",
      "使用RandomForest评估特征重要性...\n",
      "RandomForest选择的特征数量: 118\n",
      "使用GradientBoosting评估特征重要性...\n",
      "GradientBoosting选择的特征数量: 118\n",
      "综合所有方法选择最佳特征...\n",
      "基于模型性能选择最佳特征数量...\n",
      "    5个特征的平均RMSE: 16283.8892\n",
      "    10个特征的平均RMSE: 16408.4772\n",
      "    15个特征的平均RMSE: 16294.1190\n",
      "    20个特征的平均RMSE: 16357.4998\n",
      "    25个特征的平均RMSE: 16228.3819\n",
      "    30个特征的平均RMSE: 15958.9730\n",
      "    35个特征的平均RMSE: 16157.7336\n",
      "    40个特征的平均RMSE: 15673.0423\n",
      "    45个特征的平均RMSE: 15808.5927\n",
      "最佳特征数量: 40, 平均RMSE: 15673.0423\n",
      "最佳特征列表:\n",
      "  1. big_customer_ratio_inquiry_interaction\n",
      "  2. big_customer_quantity_ratio_inquiry_interaction\n",
      "  3. weekday_factor\n",
      "  4. demand_lag60\n",
      "  5. price_change_1d\n",
      "  6. big_customer_ratio_price_interaction\n",
      "  7. big_customer_quantity_ratio_price_interaction\n",
      "  8. inquiry_count_ma5\n",
      "  9. big_customer_ratio_ma7\n",
      "  10. price_change_7d\n",
      "  11. last_month_big_customer_quantity_ratio_lag14\n",
      "  12. inquiry_count_lag1\n",
      "  13. inquiry_count_ma6\n",
      "  14. inquiry_quantity_lag3\n",
      "  15. last_month_big_customer_ratio_lag30\n",
      "  16. inquiry_quantity_lag1\n",
      "  17. big_customer_ratio_change_1d\n",
      "  18. demand_ma7\n",
      "  19. demand_lag6\n",
      "  20. demand_lag14\n",
      "  21. inquiry_quantity_lag60\n",
      "  22. last_week_big_customer_quantity_ratio\n",
      "  23. demand_lag1\n",
      "  24. demand_change_1d\n",
      "  25. inquiry_count_ma3\n",
      "  26. demand_ma3\n",
      "  27. demand_lag3\n",
      "  28. demand_ma2\n",
      "  29. big_customer_ratio_demand_interaction\n",
      "  30. price_demand_ratio\n",
      "  31. demand_ma90\n",
      "  32. inquiry_count_ma2\n",
      "  33. demand_change_7d\n",
      "  34. last_week_big_customer_ratio_lag14\n",
      "  35. price_volatility_7d\n",
      "  36. last_month_big_customer_quantity_ratio\n",
      "  37. is_holiday\n",
      "  38. last_week_big_customer_ratio_ma7\n",
      "  39. big_customer_ratio_lag30\n",
      "  40. big_customer_ratio_lag1\n",
      "最佳特征已保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_最佳特征.csv\n",
      "选定特征数据集已保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_选定特征数据.xlsx\n",
      "\n",
      "电解镍 的特征选择完成!\n",
      "\n",
      "================================================================================\n",
      "开始处理 高碳铬铁 的特征选择\n",
      "================================================================================\n",
      "输出目录: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果\n",
      "初始候选特征数量: 235\n",
      "\n",
      "开始为高碳铬铁执行特征选择...\n",
      "特征选择结果将保存在: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/特征选择结果\n",
      "开始过滤特征，排除实际应用中当天无法获得的特征...\n",
      "原始特征数量: 235\n",
      "可用特征数量: 235\n",
      "被过滤特征数量: 0\n",
      "保留的特征类型包括:\n",
      "  time: 79个特征\n",
      "  lag: 45个特征\n",
      "  ma: 61个特征\n",
      "  seasonal: 8个特征\n",
      "  other: 42个特征\n",
      "特征过滤结果已保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/特征选择结果/高碳铬铁_特征过滤结果.csv\n",
      "清理数据...\n",
      "执行数据清理...\n",
      "  警告: seasonal_trend 包含过大值 (min=-35036520044.94835, max=20302213020.25488)\n",
      "删除了 0 行缺失数据。\n",
      "清理后训练集形状: (1216, 235), 测试集形状: (305, 235)\n",
      "使用分位数回归森林(QRF)评估特征重要性...\n",
      "使用递归特征消除(RFE)结合QRF选择特征...\n",
      "Fitting estimator with 235 features.\n",
      "Fitting estimator with 212 features.\n",
      "Fitting estimator with 189 features.\n",
      "Fitting estimator with 166 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 28 features.\n",
      "RFE-QRF选择的特征数量: 20\n",
      "使用XGBoost评估特征重要性...\n",
      "XGBoost选择的特征数量: 118\n",
      "使用LightGBM评估特征重要性...\n",
      "LightGBM选择的特征数量: 123\n",
      "使用RandomForest评估特征重要性...\n",
      "RandomForest选择的特征数量: 118\n",
      "使用GradientBoosting评估特征重要性...\n",
      "GradientBoosting选择的特征数量: 118\n",
      "综合所有方法选择最佳特征...\n",
      "基于模型性能选择最佳特征数量...\n",
      "    5个特征的平均RMSE: 88283.8029\n",
      "    10个特征的平均RMSE: 86192.1851\n",
      "    15个特征的平均RMSE: 86436.8342\n",
      "    20个特征的平均RMSE: 91590.8379\n",
      "    25个特征的平均RMSE: 94799.7159\n",
      "    30个特征的平均RMSE: 94513.5641\n",
      "    35个特征的平均RMSE: 94456.2535\n",
      "    40个特征的平均RMSE: 95292.4488\n",
      "    45个特征的平均RMSE: 96870.2262\n",
      "最佳特征数量: 10, 平均RMSE: 86192.1851\n",
      "最佳特征列表:\n",
      "  1. weekday_factor\n",
      "  2. is_holiday\n",
      "  3. big_customer_quantity_ratio_inquiry_interaction\n",
      "  4. big_customer_quantity_ratio_price_interaction\n",
      "  5. demand_change_1d\n",
      "  6. demand_rsi\n",
      "  7. inquiry_quantity_lag5\n",
      "  8. demand_lag14\n",
      "  9. big_customer_ratio_inquiry_interaction\n",
      "  10. big_customer_quantity_ratio_change_1d\n",
      "最佳特征已保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_最佳特征.csv\n",
      "选定特征数据集已保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_选定特征数据.xlsx\n",
      "\n",
      "高碳铬铁 的特征选择完成!\n",
      "\n",
      "================================================================================\n",
      "开始处理 铝锭 的特征选择\n",
      "================================================================================\n",
      "输出目录: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果\n",
      "初始候选特征数量: 235\n",
      "\n",
      "开始为铝锭执行特征选择...\n",
      "特征选择结果将保存在: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/特征选择结果\n",
      "开始过滤特征，排除实际应用中当天无法获得的特征...\n",
      "原始特征数量: 235\n",
      "可用特征数量: 235\n",
      "被过滤特征数量: 0\n",
      "保留的特征类型包括:\n",
      "  time: 79个特征\n",
      "  lag: 45个特征\n",
      "  ma: 61个特征\n",
      "  seasonal: 8个特征\n",
      "  other: 42个特征\n",
      "特征过滤结果已保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/特征选择结果/铝锭_特征过滤结果.csv\n",
      "清理数据...\n",
      "执行数据清理...\n",
      "  警告: seasonal_trend 包含过大值 (min=-182420061654.6377, max=110048650532.6462)\n",
      "删除了 0 行缺失数据。\n",
      "清理后训练集形状: (1216, 235), 测试集形状: (305, 235)\n",
      "使用分位数回归森林(QRF)评估特征重要性...\n",
      "使用递归特征消除(RFE)结合QRF选择特征...\n",
      "Fitting estimator with 235 features.\n",
      "Fitting estimator with 212 features.\n",
      "Fitting estimator with 189 features.\n",
      "Fitting estimator with 166 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 28 features.\n",
      "RFE-QRF选择的特征数量: 20\n",
      "使用XGBoost评估特征重要性...\n",
      "XGBoost选择的特征数量: 118\n",
      "使用LightGBM评估特征重要性...\n",
      "LightGBM选择的特征数量: 127\n",
      "使用RandomForest评估特征重要性...\n",
      "RandomForest选择的特征数量: 118\n",
      "使用GradientBoosting评估特征重要性...\n",
      "GradientBoosting选择的特征数量: 118\n",
      "综合所有方法选择最佳特征...\n",
      "基于模型性能选择最佳特征数量...\n",
      "    5个特征的平均RMSE: 134765.3463\n",
      "    10个特征的平均RMSE: 139948.2566\n",
      "    15个特征的平均RMSE: 139370.1380\n",
      "    20个特征的平均RMSE: 135880.5439\n",
      "    25个特征的平均RMSE: 135340.1379\n",
      "    30个特征的平均RMSE: 133907.5822\n",
      "    35个特征的平均RMSE: 134315.6168\n",
      "    40个特征的平均RMSE: 134507.9231\n",
      "    45个特征的平均RMSE: 134774.0174\n",
      "最佳特征数量: 30, 平均RMSE: 133907.5822\n",
      "最佳特征列表:\n",
      "  1. big_customer_quantity_ratio_inquiry_interaction\n",
      "  2. big_customer_ratio_inquiry_interaction\n",
      "  3. big_customer_ratio_demand_interaction\n",
      "  4. inquiry_quantity_lag1\n",
      "  5. weekday_factor\n",
      "  6. inquiry_count_ma6\n",
      "  7. demand_macd\n",
      "  8. demand_ma7\n",
      "  9. inquiry_count_ma5\n",
      "  10. inquiry_count_ma7\n",
      "  11. big_customer_quantity_ratio_price_interaction\n",
      "  12. big_customer_quantity_ratio_demand_interaction\n",
      "  13. inquiry_quantity_lag3\n",
      "  14. demand_lag14\n",
      "  15. big_customer_ratio_price_interaction\n",
      "  16. demand_ma5\n",
      "  17. demand_ma3\n",
      "  18. day_of_month\n",
      "  19. price_change_1d\n",
      "  20. demand_change_1d\n",
      "  21. residual\n",
      "  22. inquiry_count_ma3\n",
      "  23. last_month_big_customer_quantity_ratio_price_interaction\n",
      "  24. demand_ma2\n",
      "  25. demand_rsi\n",
      "  26. demand_change_7d\n",
      "  27. last_month_big_customer_ratio_ma7\n",
      "  28. inquiry_count_ma14\n",
      "  29. demand_inquiry_ratio\n",
      "  30. demand_lag60\n",
      "最佳特征已保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_最佳特征.csv\n",
      "选定特征数据集已保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_选定特征数据.xlsx\n",
      "\n",
      "铝锭 的特征选择完成!\n",
      "\n",
      "所有处理完成!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置中文字体\n",
    "# plt.rcParams['font.sans-serif'] = ['SimHei']  \n",
    "# plt.rcParams['axes.unicode_minus'] = False  \n",
    "\n",
    "# =============================================================================\n",
    "# 辅助函数 - 数据清理\n",
    "# =============================================================================\n",
    "def clean_data(df):\n",
    "    \"\"\"清理数据：处理无穷大值、过大值和缺失值\"\"\"\n",
    "    print(\"执行数据清理...\")\n",
    "    \n",
    "    # 处理无穷大值\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        # 替换无穷大值为NaN\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # 检查是否有过大值\n",
    "        col_min = df[col].min()\n",
    "        col_max = df[col].max()\n",
    "        \n",
    "        if col_max > 1e10 or col_min < -1e10:\n",
    "            print(f\"  警告: {col} 包含过大值 (min={col_min}, max={col_max})\")\n",
    "            \n",
    "            # 使用中位数替换过大值\n",
    "            median_val = df[col].median()\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: median_val if x > 1e10 or x < -1e10 else x\n",
    "            )\n",
    "    \n",
    "    # 填充NaN值\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            if df[col].dtype == 'object':\n",
    "                # 分类列用众数填充\n",
    "                mode_val = df[col].mode()[0]\n",
    "                df[col] = df[col].fillna(mode_val)\n",
    "            else:\n",
    "                # 数值列用中位数填充\n",
    "                median_val = df[col].median()\n",
    "                df[col] = df[col].fillna(median_val)\n",
    "    \n",
    "    # 删除有缺失值的行\n",
    "    original_rows = df.shape[0]\n",
    "    df = df.dropna()\n",
    "    dropped_rows = original_rows - df.shape[0]\n",
    "    print(f\"删除了 {dropped_rows} 行缺失数据。\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# 特征过滤函数 - 排除不能当天获得的特征\n",
    "# =============================================================================\n",
    "def filter_available_features(feature_cols):\n",
    "    \"\"\"\n",
    "    过滤出在实际应用中当天能获得的特征\n",
    "    排除不能当天获得的基础特征及其衍生特征\n",
    "    \"\"\"\n",
    "    print(\"开始过滤特征，排除实际应用中当天无法获得的特征...\")\n",
    "    \n",
    "    # 明确不能当天获得的基础特征\n",
    "    forbidden_base_features = {\n",
    "        'demand', 'avg_price', 'sales', 'inquiry_count', 'our_quote_avg', \n",
    "        'inquiry_quantity', 'price_std', 'big_customer_inquiries', \n",
    "        'big_customer_quantity', 'big_customer_ratio', 'big_customer_quantity_ratio'\n",
    "    }\n",
    "    \n",
    "    available_features = []\n",
    "    filtered_features = []\n",
    "    \n",
    "    for feature in feature_cols:\n",
    "        # 检查是否是禁用的基础特征\n",
    "        is_forbidden = False\n",
    "        \n",
    "        for forbidden in forbidden_base_features:\n",
    "            if feature == forbidden:  # 完全匹配禁用特征\n",
    "                is_forbidden = True\n",
    "                filtered_features.append(feature)\n",
    "                break\n",
    "            # # 检查是否是基于禁用特征的衍生特征（但没有足够的滞后）\n",
    "            # elif forbidden in feature:\n",
    "            #     # 如果包含禁用特征名但没有滞后保护，则排除\n",
    "            #     if not any(lag_word in feature for lag_word in ['lag', 'ma', 'shift', '_change_']):\n",
    "            #         is_forbidden = True\n",
    "            #         filtered_features.append(feature)\n",
    "            #         break\n",
    "            #     # 如果是lag1相关的特征，也排除（当天可能无法获得）\n",
    "            #     elif 'lag1' in feature:\n",
    "            #         is_forbidden = True\n",
    "            #         filtered_features.append(feature)\n",
    "            #         break\n",
    "        \n",
    "        if not is_forbidden:\n",
    "            available_features.append(feature)\n",
    "    \n",
    "    print(f\"原始特征数量: {len(feature_cols)}\")\n",
    "    print(f\"可用特征数量: {len(available_features)}\")\n",
    "    print(f\"被过滤特征数量: {len(filtered_features)}\")\n",
    "    \n",
    "    if filtered_features:\n",
    "        print(f\"被过滤的特征包括:\")\n",
    "        for i, feature in enumerate(filtered_features):\n",
    "            print(f\"  {i+1}. {feature}\")\n",
    "    \n",
    "    print(f\"保留的特征类型包括:\")\n",
    "    feature_types = {\n",
    "        'time': [],\n",
    "        'lag': [],\n",
    "        'ma': [],\n",
    "        'seasonal': [],\n",
    "        'other': []\n",
    "    }\n",
    "    \n",
    "    for feature in available_features:\n",
    "        if any(keyword in feature for keyword in ['year', 'month', 'day', 'quarter', 'week', 'is_', 'holiday']):\n",
    "            feature_types['time'].append(feature)\n",
    "        elif 'lag' in feature and 'lag1' not in feature:\n",
    "            feature_types['lag'].append(feature)\n",
    "        elif 'ma' in feature:\n",
    "            feature_types['ma'].append(feature)\n",
    "        elif any(keyword in feature for keyword in ['seasonal', 'trend', 'residual', 'factor']):\n",
    "            feature_types['seasonal'].append(feature)\n",
    "        else:\n",
    "            feature_types['other'].append(feature)\n",
    "    \n",
    "    for ftype, features in feature_types.items():\n",
    "        if features:\n",
    "            print(f\"  {ftype}: {len(features)}个特征\")\n",
    "    \n",
    "    return available_features, filtered_features\n",
    "\n",
    "# =============================================================================\n",
    "# 特征选择函数 - 使用分位数回归森林结合其他算法\n",
    "# =============================================================================\n",
    "def select_features_with_qrf(X, y, product_name, output_dir):\n",
    "    \"\"\"\n",
    "    使用分位数回归森林(QRF)结合其他算法进行特征选择\n",
    "    返回最佳特征子集\n",
    "    \"\"\"\n",
    "    print(f\"\\n开始为{product_name}执行特征选择...\")\n",
    "    \n",
    "    # 创建特征选择输出目录\n",
    "    feature_selection_dir = os.path.join(output_dir, \"特征选择结果\")\n",
    "    os.makedirs(feature_selection_dir, exist_ok=True)\n",
    "    print(f\"特征选择结果将保存在: {feature_selection_dir}\")\n",
    "    \n",
    "    # 1. 过滤可用特征\n",
    "    available_features, filtered_features = filter_available_features(X.columns.tolist())\n",
    "    \n",
    "    if len(available_features) == 0:\n",
    "        print(f\"警告: {product_name}没有可用特征，无法进行特征选择\")\n",
    "        return []\n",
    "    \n",
    "    # 保存过滤结果\n",
    "    filter_result_df = pd.DataFrame({\n",
    "        'available_features': pd.Series(available_features),\n",
    "        'filtered_features': pd.Series(filtered_features)\n",
    "    })\n",
    "    filter_result_path = os.path.join(feature_selection_dir, f'{product_name}_特征过滤结果.csv')\n",
    "    filter_result_df.to_csv(filter_result_path, index=False)\n",
    "    print(f\"特征过滤结果已保存至: {filter_result_path}\")\n",
    "    \n",
    "    # 2. 只使用可用特征\n",
    "    X_available = X[available_features]\n",
    "    \n",
    "    # 3. 清理数据\n",
    "    print(\"清理数据...\")\n",
    "    X_clean = clean_data(X_available.copy())\n",
    "    y_clean = y.copy()\n",
    "    \n",
    "    # 确保索引一致\n",
    "    y_clean = y_clean.loc[X_clean.index]\n",
    "\n",
    "    # 4. 时间序列划分（使用清理后的数据）\n",
    "    split_idx = int(len(X_clean) * 0.8)\n",
    "    X_train = X_clean.iloc[:split_idx]\n",
    "    X_test = X_clean.iloc[split_idx:]\n",
    "    y_train = y_clean.iloc[:split_idx]\n",
    "    y_test = y_clean.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"清理后训练集形状: {X_train.shape}, 测试集形状: {X_test.shape}\")\n",
    "    \n",
    "    # 5. 使用分位数回归森林(QRF)进行特征重要性评估\n",
    "    print(\"使用分位数回归森林(QRF)评估特征重要性...\")\n",
    "    qrf = RandomForestQuantileRegressor(\n",
    "        n_estimators=300,\n",
    "        min_samples_leaf=5,\n",
    "        max_depth=10,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    qrf.fit(X_train, y_train)\n",
    "    \n",
    "    # 获取特征重要性\n",
    "    qrf_importances = pd.Series(\n",
    "        qrf.feature_importances_, \n",
    "        index=X_clean.columns\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    # 可视化QRF特征重要性\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    qrf_importances.head(30).plot(kind='barh')\n",
    "    plt.title(f'{product_name} - QRF特征重要性 (Top 30, 仅可用特征)')\n",
    "    plt.xlabel('重要性分数')\n",
    "    plt.ylabel('特征')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(feature_selection_dir, f'{product_name}_QRF特征重要性.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. 使用递归特征消除(RFE)结合QRF\n",
    "    print(\"使用递归特征消除(RFE)结合QRF选择特征...\")\n",
    "    n_features_to_select = min(20, len(available_features))  # 不超过可用特征数量\n",
    "    rfe_qrf = RFE(\n",
    "        estimator=clone(qrf),\n",
    "        n_features_to_select=n_features_to_select,\n",
    "        step=max(1, len(available_features) // 10),  # 动态调整步长\n",
    "        verbose=1\n",
    "    )\n",
    "    rfe_qrf.fit(X_train, y_train)\n",
    "    \n",
    "    # 获取RFE选择的特征\n",
    "    rfe_qrf_features = X_clean.columns[rfe_qrf.support_].tolist()\n",
    "    print(f\"RFE-QRF选择的特征数量: {len(rfe_qrf_features)}\")\n",
    "    \n",
    "    # 7. 使用其他算法评估特征重要性\n",
    "    algorithms = {\n",
    "        \"XGBoost\": XGBRegressor(random_state=42, verbosity=0),\n",
    "        \"LightGBM\": lgb.LGBMRegressor(random_state=42, verbosity=-1),\n",
    "        \"RandomForest\": RandomForestRegressor(random_state=42),\n",
    "        \"GradientBoosting\": GradientBoostingRegressor(random_state=42)\n",
    "    }\n",
    "    \n",
    "    algorithm_importances = {}\n",
    "    algorithm_features = {}\n",
    "    \n",
    "    for name, model in algorithms.items():\n",
    "        print(f\"使用{name}评估特征重要性...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # 获取特征重要性\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            importances = np.abs(model.coef_)\n",
    "        else:\n",
    "            importances = np.zeros(X_train.shape[1])\n",
    "        \n",
    "        # 存储重要性\n",
    "        algorithm_importances[name] = pd.Series(\n",
    "            importances, \n",
    "            index=X_clean.columns\n",
    "        ).sort_values(ascending=False)\n",
    "        \n",
    "        # 可视化\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        algorithm_importances[name].head(30).plot(kind='barh')\n",
    "        plt.title(f'{product_name} - {name}特征重要性 (Top 30, 仅可用特征)')\n",
    "        plt.xlabel('重要性分数')\n",
    "        plt.ylabel('特征')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(feature_selection_dir, f'{product_name}_{name}特征重要性.png'), dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # 使用SelectFromModel选择特征\n",
    "        selector = SelectFromModel(\n",
    "            model, \n",
    "            threshold=\"median\",  # 选择重要性高于中位数的特征\n",
    "            prefit=True\n",
    "        )\n",
    "        selected_features = X_clean.columns[selector.get_support()].tolist()\n",
    "        algorithm_features[name] = selected_features\n",
    "        print(f\"{name}选择的特征数量: {len(selected_features)}\")\n",
    "    \n",
    "    # 8. 综合所有方法选择最佳特征\n",
    "    print(\"综合所有方法选择最佳特征...\")\n",
    "    \n",
    "    # 创建特征得分表\n",
    "    feature_scores = pd.DataFrame(index=X_clean.columns)\n",
    "    \n",
    "    # 添加QRF重要性得分\n",
    "    feature_scores['QRF'] = qrf_importances\n",
    "    \n",
    "    # 添加其他算法的重要性得分\n",
    "    for name in algorithms.keys():\n",
    "        feature_scores[name] = algorithm_importances[name]\n",
    "    \n",
    "    # 标准化得分 (0-1范围)\n",
    "    feature_scores = (feature_scores - feature_scores.min()) / (feature_scores.max() - feature_scores.min())\n",
    "    \n",
    "    # 计算平均得分\n",
    "    feature_scores['Average'] = feature_scores.mean(axis=1)\n",
    "    \n",
    "    # 按平均得分排序\n",
    "    feature_scores = feature_scores.sort_values('Average', ascending=False)\n",
    "    \n",
    "    # 保存特征得分\n",
    "    feature_scores.to_csv(os.path.join(feature_selection_dir, f'{product_name}_特征得分.csv'))\n",
    "\n",
    "    def evaluate_features_with_multiple_models(X_train_sel, X_test_sel, y_train, y_test):\n",
    "        \"\"\"\n",
    "        使用多个模型评估特征集合的性能，返回平均RMSE\n",
    "        \"\"\"\n",
    "        models = {\n",
    "            'QRF': RandomForestQuantileRegressor(n_estimators=300,min_samples_leaf=5, max_depth=10,n_jobs=-1,random_state=42),\n",
    "            'RandomForest': RandomForestRegressor(random_state=42),\n",
    "            'XGBoost': XGBRegressor(random_state=42, verbosity=0),\n",
    "            'LightGBM': lgb.LGBMRegressor(random_state=42, verbosity=-1),\n",
    "            \"GradientBoosting\": GradientBoostingRegressor(random_state=42)\n",
    "        }\n",
    "        \n",
    "        rmse_scores = []\n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                model.fit(X_train_sel, y_train)\n",
    "                y_pred = model.predict(X_test_sel)\n",
    "                rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "                rmse_scores.append(rmse)\n",
    "            except Exception as e:\n",
    "                print(f\"    {name}模型评估失败: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # 返回平均RMSE\n",
    "        return np.mean(rmse_scores) if rmse_scores else float('inf')\n",
    "\n",
    "    # 9. 基于性能选择最佳特征数量\n",
    "    print(\"基于模型性能选择最佳特征数量...\")\n",
    "    \n",
    "    # 测试不同特征数量的性能\n",
    "    max_features = min(50, len(feature_scores))\n",
    "    results = []\n",
    "    best_score = float('inf')\n",
    "    best_features = []\n",
    "    \n",
    "    for n_features in range(5, max_features, 5):\n",
    "        if n_features > len(feature_scores):\n",
    "            break\n",
    "            \n",
    "        # 选择前n_features个特征\n",
    "        selected_features = feature_scores.index[:n_features].tolist()\n",
    "        X_train_sel = X_train[selected_features]\n",
    "        X_test_sel = X_test[selected_features]\n",
    "        \n",
    "        avg_rmse = evaluate_features_with_multiple_models(\n",
    "            X_train_sel, X_test_sel, y_train, y_test\n",
    "        )\n",
    "        \n",
    "        # 存储结果\n",
    "        results.append({\n",
    "            'n_features': n_features,\n",
    "            'avg_rmse': avg_rmse, \n",
    "            'features': selected_features\n",
    "        })\n",
    "        \n",
    "        print(f\"    {n_features}个特征的平均RMSE: {avg_rmse:.4f}\")\n",
    "        \n",
    "        # 更新最佳特征\n",
    "        if avg_rmse < best_score:\n",
    "            best_score = avg_rmse\n",
    "            best_features = selected_features\n",
    "    \n",
    "    # 将结果转换为DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # 可视化特征数量与性能的关系\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(results_df['n_features'], results_df['avg_rmse'], 'o-')\n",
    "    plt.title(f'{product_name} - 特征数量与模型性能 (仅可用特征)')\n",
    "    plt.xlabel('特征数量')\n",
    "    plt.ylabel('平均RMSE')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(feature_selection_dir, f'{product_name}_特征数量与性能.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 保存结果\n",
    "    results_df.to_csv(os.path.join(feature_selection_dir, f'{product_name}_特征选择结果.csv'), index=False)\n",
    "    \n",
    "    print(f\"最佳特征数量: {len(best_features)}, 平均RMSE: {best_score:.4f}\")\n",
    "    print(f\"最佳特征列表:\")\n",
    "    for i, feature in enumerate(best_features, 1):\n",
    "        print(f\"  {i}. {feature}\")\n",
    "    \n",
    "    return best_features\n",
    "\n",
    "# =============================================================================\n",
    "# 主程序\n",
    "# =============================================================================\n",
    "\n",
    "# 输入输出路径\n",
    "output_dir = r\"/Users/bytedance/Desktop/特征工程补充/结果\"\n",
    "\n",
    "# 处理三个品种\n",
    "products = ['电解镍', '高碳铬铁', '铝锭']\n",
    "\n",
    "for product in products:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"开始处理 {product} 的特征选择\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 创建产品输出目录\n",
    "    product_output_dir = os.path.join(output_dir, f\"{product}预测结果\")\n",
    "    os.makedirs(product_output_dir, exist_ok=True)\n",
    "    print(f\"输出目录: {product_output_dir}\")\n",
    "    \n",
    "    # 1. 读取特征工程后的数据\n",
    "    feature_path = os.path.join(product_output_dir, f\"{product}_特征工程数据.xlsx\")\n",
    "    if not os.path.exists(feature_path):\n",
    "        print(f\"警告: 找不到 {product} 的特征工程数据文件: {feature_path}\")\n",
    "        continue\n",
    "    \n",
    "    feature_df = pd.read_excel(feature_path)\n",
    "    \n",
    "    # 检查数据是否为空\n",
    "    if feature_df.empty:\n",
    "        print(f\"警告: {product} 的特征工程数据为空，跳过处理\")\n",
    "        continue\n",
    "    \n",
    "    # 2. 准备特征和目标变量\n",
    "    # 排除非特征列（包括不能当天获得的特征）\n",
    "    exclude_cols = [\n",
    "        'date', 'demand', 'next_month_demand', 'holiday_name', 'sales',\n",
    "        # 以下特征也应该排除，因为是当天无法获得的\n",
    "        'avg_price', 'inquiry_count', 'our_quote_avg', 'inquiry_quantity', \n",
    "        'price_std', 'big_customer_inquiries', 'big_customer_quantity', \n",
    "        'big_customer_ratio', 'big_customer_quantity_ratio'\n",
    "    ]\n",
    "    \n",
    "    feature_cols = [col for col in feature_df.columns if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"初始候选特征数量: {len(feature_cols)}\")\n",
    "    \n",
    "    X = feature_df[feature_cols]\n",
    "    y = feature_df['demand']\n",
    "    \n",
    "    # 3. 执行特征选择（包含特征过滤）\n",
    "    best_features = select_features_with_qrf(X, y, product, product_output_dir)\n",
    "    \n",
    "    if not best_features:\n",
    "        print(f\"警告: {product}没有选出最佳特征，跳过后续处理\")\n",
    "        continue\n",
    "    \n",
    "    # 4. 保存最佳特征子集\n",
    "    best_features_df = pd.DataFrame({'feature': best_features})\n",
    "    best_features_path = os.path.join(product_output_dir, f\"{product}_最佳特征.csv\")\n",
    "    best_features_df.to_csv(best_features_path, index=False)\n",
    "    print(f\"最佳特征已保存至: {best_features_path}\")\n",
    "    \n",
    "    # 5. 使用最佳特征创建新数据集\n",
    "    selected_features_df = feature_df[['date', 'demand'] + best_features]\n",
    "    selected_features_path = os.path.join(product_output_dir, f\"{product}_选定特征数据.xlsx\")\n",
    "    selected_features_df.to_excel(selected_features_path, index=False)\n",
    "    print(f\"选定特征数据集已保存至: {selected_features_path}\")\n",
    "    \n",
    "    print(f\"\\n{product} 的特征选择完成!\")\n",
    "\n",
    "print(\"\\n所有处理完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675464ae",
   "metadata": {},
   "source": [
    "### 4. 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9fd3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "超参数搜索: 启用\n",
      "搜索策略: smart\n",
      "每模型最大时间: 180秒\n",
      "\n",
      "================================================================================\n",
      "开始处理 电解镍 的2024年逐日预测比较\n",
      "================================================================================\n",
      "输出目录: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果\n",
      "已加载电解镍的选定特征: 40个\n",
      "已加载 电解镍 的特征工程数据，形状: (1521, 248)\n",
      "\n",
      "开始为电解镍执行逐日预测与比较...\n",
      "训练集大小: 1461, 测试集大小: 60\n",
      "执行无数据泄漏的数据清理...\n",
      "清理后训练集大小: 1461, 测试集大小: 60\n",
      "使用特征数量: 40\n",
      "\n",
      "使用超参数搜索训练模型（策略: smart）...\n",
      "开始超参数搜索训练（策略: smart）...\n",
      "\n",
      "训练 RandomForest 模型...\n",
      "  RandomForest 使用 32 个特征\n",
      "    数据量: 1461\n",
      "    使用随机搜索（数据量 >= 200）\n",
      "    搜索完成，用时: 21.6秒\n",
      "    最佳参数: {'max_depth': 8, 'min_samples_leaf': 4, 'min_samples_split': 7, 'n_estimators': 441}\n",
      "    最佳得分: -11966.0613\n",
      "  RandomForest 搜索完成，用时: 21.6秒\n",
      "\n",
      "训练 XGBoost 模型...\n",
      "  XGBoost 使用 32 个特征\n",
      "    数据量: 1461\n",
      "    使用随机搜索（数据量 >= 200）\n",
      "    搜索完成，用时: 187.9秒\n",
      "    最佳参数: {'colsample_bytree': np.float64(0.8540922615763339), 'gamma': np.float64(0.2252496259847715), 'learning_rate': np.float64(0.00626017131018732), 'max_depth': 4, 'min_child_weight': 4, 'n_estimators': 313, 'reg_alpha': np.float64(0.16167946962329224), 'reg_lambda': np.float64(1.2615344229334267), 'subsample': np.float64(0.6793016342019151)}\n",
      "    最佳得分: -12131.5623\n",
      "  XGBoost 搜索完成，用时: 187.9秒\n",
      "\n",
      "训练 LightGBM 模型...\n",
      "  LightGBM 使用 32 个特征\n",
      "    数据量: 1461\n",
      "    使用随机搜索（数据量 >= 200）\n",
      "    搜索完成，用时: 182.1秒\n",
      "    最佳参数: {'colsample_bytree': np.float64(0.6995800817189007), 'learning_rate': np.float64(0.006485458640413423), 'max_depth': 4, 'n_estimators': 643, 'num_leaves': 20, 'reg_alpha': np.float64(0.03974313630683449), 'reg_lambda': np.float64(0.513805292809006), 'subsample': np.float64(0.8946384285364503)}\n",
      "    最佳得分: -12228.6142\n",
      "  LightGBM 搜索完成，用时: 182.1秒\n",
      "\n",
      "训练 QRF 模型...\n",
      "  QRF 使用 32 个特征\n",
      "    数据量: 1461\n",
      "    使用随机搜索（数据量 >= 200）\n",
      "    搜索完成，用时: 22.2秒\n",
      "    最佳参数: {'max_depth': 8, 'min_samples_leaf': 4, 'n_estimators': 221}\n",
      "    最佳得分: -11326.5152\n",
      "  QRF 搜索完成，用时: 22.2秒\n",
      "\n",
      "训练 SVR 模型...\n",
      "  SVR 使用 15 个特征\n",
      "    数据量: 1461\n",
      "    使用网格搜索（强制模式）\n",
      "    搜索完成，用时: 7.9秒\n",
      "    最佳参数: {'C': 100, 'epsilon': 0.1, 'gamma': 0.04, 'kernel': 'rbf'}\n",
      "    最佳得分: -0.8704\n",
      "  SVR 搜索完成，用时: 7.9秒\n",
      "最佳参数已保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_最佳参数.xlsx\n",
      "\n",
      "创建 RandomForest 预测器...\n",
      "  执行 RandomForest 逐日预测...\n",
      "  RandomForest 逐日预测性能: MAE=9001.64, WMAPE=50.70%, R²=0.3190\n",
      "\n",
      "创建 XGBoost 预测器...\n",
      "  执行 XGBoost 逐日预测...\n",
      "  XGBoost 逐日预测性能: MAE=9287.80, WMAPE=49.37%, R²=0.3099\n",
      "\n",
      "创建 LightGBM 预测器...\n",
      "  执行 LightGBM 逐日预测...\n",
      "  LightGBM 逐日预测性能: MAE=9050.29, WMAPE=50.86%, R²=0.2860\n",
      "\n",
      "创建 QRF 预测器...\n",
      "  执行 QRF 逐日预测...\n",
      "  QRF 逐日预测性能: MAE=8352.05, WMAPE=48.77%, R²=0.3292\n",
      "\n",
      "创建 SVR 预测器...\n",
      "  执行 SVR 逐日预测...\n",
      "  SVR 逐日预测性能: MAE=13871.27, WMAPE=84.97%, R²=-0.5282\n",
      "\n",
      "训练深度学习模型（启用搜索）...\n",
      "深度学习模型使用 10 个特征\n",
      "使用序列长度: 7\n",
      "\n",
      "训练 LSTM 模型（搜索中）...\n",
      "  尝试配置 1/5: {'units1': 32, 'units2': 16, 'dropout': 0.2, 'lr': 0.01, 'dense': 8}\n",
      "    验证损失: 0.0239\n",
      "  尝试配置 2/5: {'units1': 24, 'units2': 12, 'dropout': 0.25, 'lr': 0.005, 'dense': 6}\n",
      "    验证损失: 0.0239\n",
      "  尝试配置 3/5: {'units1': 48, 'units2': 24, 'dropout': 0.2, 'lr': 0.01, 'dense': 12}\n",
      "    验证损失: 0.0221\n",
      "  尝试配置 4/5: {'units1': 32, 'units2': 16, 'dropout': 0.3, 'lr': 0.005, 'dense': 8}\n",
      "    验证损失: 0.0245\n",
      "  尝试配置 5/5: {'units1': 40, 'units2': 20, 'dropout': 0.2, 'lr': 0.008, 'dense': 10}\n",
      "    验证损失: 0.0235\n",
      "  LSTM最佳配置: {'units1': 48, 'units2': 24, 'dropout': 0.2, 'lr': 0.01, 'dense': 12}, 验证损失: 0.0221\n",
      "  执行 LSTM 逐日序列预测...\n",
      "  LSTM 逐日预测性能: MAE=10614.55, WMAPE=55.32%, R²=0.1128\n",
      "\n",
      "训练基线模型...\n",
      "  LinearRegression 基线性能: MAE=8755.63, WMAPE=48.32%, R²=0.3410\n",
      "\n",
      "收集所有模型的逐日预测结果...\n",
      "\n",
      "创建改进的动态加权集成预测...\n",
      "基于性能的集成权重:\n",
      "  RandomForest: 0.206 (WMAPE: 50.70%)\n",
      "  XGBoost: 0.205 (WMAPE: 49.37%)\n",
      "  LightGBM: 0.202 (WMAPE: 50.86%)\n",
      "  QRF: 0.210 (WMAPE: 48.77%)\n",
      "  LSTM: 0.177 (WMAPE: 55.32%)\n",
      "Improved_Ensemble 预测性能: MAE=8890.19, WMAPE=48.77%, R²=0.3231\n",
      "\n",
      "执行13周移动平均法预测...\n",
      "\n",
      "基于WMAPE(40%)+MAE(30%)+R²(30%)的综合排名:\n",
      "  1. SVR: 7.00分 (WMAPE: 22.31%, MAE: 23327, R²: 0.6940)\n",
      "  2. RandomForest: 5.10分 (WMAPE: 28.64%, MAE: 29939, R²: 0.3218)\n",
      "  3. LightGBM: 5.00分 (WMAPE: 29.05%, MAE: 30369, R²: 0.4116)\n",
      "  4. XGBoost: 3.90分 (WMAPE: 30.05%, MAE: 31420, R²: 0.4403)\n",
      "  5. QRF: 3.40分 (WMAPE: 29.62%, MAE: 30963, R²: 0.2950)\n",
      "  6. Improved_Ensemble: 2.60分 (WMAPE: 30.69%, MAE: 32089, R²: 0.3530)\n",
      "  7. LSTM: 1.00分 (WMAPE: 47.61%, MAE: 49774, R²: -0.2287)\n",
      "\n",
      "电解镍逐日预测转周度汇总性能比较:\n",
      "============================================================\n",
      "\n",
      "MAE排名:\n",
      "  1. SVR: 23327\n",
      "  2. RandomForest: 29939\n",
      "  3. LightGBM: 30369\n",
      "  4. LinearRegression: 30672\n",
      "  5. QRF: 30963\n",
      "  6. XGBoost: 31420\n",
      "  7. Improved_Ensemble: 32089\n",
      "  8. 13周移动平均: 42913\n",
      "  9. LSTM: 49774\n",
      "\n",
      "WMAPE (%)排名:\n",
      "  1. SVR: 22.31%\n",
      "  2. RandomForest: 28.64%\n",
      "  3. LightGBM: 29.05%\n",
      "  4. LinearRegression: 29.34%\n",
      "  5. QRF: 29.62%\n",
      "  6. XGBoost: 30.05%\n",
      "  7. Improved_Ensemble: 30.69%\n",
      "  8. 13周移动平均: 41.05%\n",
      "  9. LSTM: 47.61%\n",
      "\n",
      "MAPE (%)排名:\n",
      "  1. SVR: 40.46%\n",
      "  2. LightGBM: 56.12%\n",
      "  3. QRF: 56.84%\n",
      "  4. RandomForest: 57.84%\n",
      "  5. LinearRegression: 59.81%\n",
      "  6. Improved_Ensemble: 61.61%\n",
      "  7. XGBoost: 71.30%\n",
      "  8. LSTM: 110.30%\n",
      "  9. 13周移动平均: 180.00%\n",
      "\n",
      "RMSE排名:\n",
      "  1. SVR: 29193\n",
      "  2. LinearRegression: 38864\n",
      "  3. XGBoost: 39480\n",
      "  4. LightGBM: 40479\n",
      "  5. Improved_Ensemble: 42446\n",
      "  6. RandomForest: 43457\n",
      "  7. QRF: 44308\n",
      "  8. 13周移动平均: 56860\n",
      "  9. LSTM: 58495\n",
      "\n",
      "R²排名:\n",
      "  1. SVR: 0.6940\n",
      "  2. LinearRegression: 0.4576\n",
      "  3. XGBoost: 0.4403\n",
      "  4. LightGBM: 0.4116\n",
      "  5. Improved_Ensemble: 0.3530\n",
      "  6. RandomForest: 0.3218\n",
      "  7. QRF: 0.2950\n",
      "  8. 13周移动平均: -0.1610\n",
      "  9. LSTM: -0.2287\n",
      "\n",
      "预测比较图表保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_2024年逐日预测汇总比较.png\n",
      "详细结果保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_2024年逐日预测汇总结果.xlsx\n",
      "逐日预测详细结果保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_2024年逐日预测详细结果.xlsx\n",
      "模型性能对比保存至: /Users/bytedance/Desktop/特征工程补充/结果/电解镍预测结果/电解镍_逐日预测模型性能对比.xlsx\n",
      "\n",
      "电解镍 的逐日预测比较完成!\n",
      "\n",
      "================================================================================\n",
      "开始处理 高碳铬铁 的2024年逐日预测比较\n",
      "================================================================================\n",
      "输出目录: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果\n",
      "已加载高碳铬铁的选定特征: 10个\n",
      "已加载 高碳铬铁 的特征工程数据，形状: (1521, 248)\n",
      "\n",
      "开始为高碳铬铁执行逐日预测与比较...\n",
      "训练集大小: 1461, 测试集大小: 60\n",
      "执行无数据泄漏的数据清理...\n",
      "清理后训练集大小: 1461, 测试集大小: 60\n",
      "使用特征数量: 10\n",
      "\n",
      "使用超参数搜索训练模型（策略: smart）...\n",
      "开始超参数搜索训练（策略: smart）...\n",
      "\n",
      "训练 RandomForest 模型...\n",
      "  RandomForest 使用 10 个特征\n",
      "    数据量: 1461\n",
      "    使用随机搜索（数据量 >= 200）\n",
      "    搜索完成，用时: 16.1秒\n",
      "    最佳参数: {'max_depth': 11, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 463}\n",
      "    最佳得分: -48211.0733\n",
      "  RandomForest 搜索完成，用时: 16.1秒\n",
      "\n",
      "训练 XGBoost 模型...\n",
      "  XGBoost 使用 10 个特征\n",
      "    数据量: 1461\n",
      "    使用随机搜索（数据量 >= 200）\n",
      "    搜索完成，用时: 189.4秒\n",
      "    最佳参数: {'colsample_bytree': np.float64(0.8816734307889973), 'gamma': np.float64(0.0993578407670862), 'learning_rate': np.float64(0.005524601126742228), 'max_depth': 6, 'min_child_weight': 3, 'n_estimators': 892, 'reg_alpha': np.float64(0.14226839054973), 'reg_lambda': np.float64(2.475438851328014), 'subsample': np.float64(0.8317879924343035)}\n",
      "    最佳得分: -49852.8750\n",
      "  XGBoost 搜索完成，用时: 189.4秒\n",
      "\n",
      "训练 LightGBM 模型...\n",
      "  LightGBM 使用 10 个特征\n",
      "    数据量: 1461\n",
      "    使用随机搜索（数据量 >= 200）\n",
      "    搜索完成，用时: 178.1秒\n",
      "    最佳参数: {'colsample_bytree': np.float64(0.6995800817189007), 'learning_rate': np.float64(0.006485458640413423), 'max_depth': 4, 'n_estimators': 643, 'num_leaves': 20, 'reg_alpha': np.float64(0.03974313630683449), 'reg_lambda': np.float64(0.513805292809006), 'subsample': np.float64(0.8946384285364503)}\n",
      "    最佳得分: -50421.5872\n",
      "  LightGBM 搜索完成，用时: 178.1秒\n",
      "\n",
      "训练 QRF 模型...\n",
      "  QRF 使用 10 个特征\n",
      "    数据量: 1461\n",
      "    使用随机搜索（数据量 >= 200）\n",
      "    搜索完成，用时: 17.9秒\n",
      "    最佳参数: {'max_depth': 8, 'min_samples_leaf': 4, 'n_estimators': 221}\n",
      "    最佳得分: -45830.6434\n",
      "  QRF 搜索完成，用时: 17.9秒\n",
      "\n",
      "训练 SVR 模型...\n",
      "  SVR 使用 10 个特征\n",
      "    数据量: 1461\n",
      "    使用网格搜索（强制模式）\n",
      "    搜索完成，用时: 17.4秒\n",
      "    最佳参数: {'C': 100, 'epsilon': 0.1, 'gamma': 0.04, 'kernel': 'rbf'}\n",
      "    最佳得分: -0.5998\n",
      "  SVR 搜索完成，用时: 17.4秒\n",
      "最佳参数已保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_最佳参数.xlsx\n",
      "\n",
      "创建 RandomForest 预测器...\n",
      "  执行 RandomForest 逐日预测...\n",
      "  RandomForest 逐日预测性能: MAE=54789.01, WMAPE=53.44%, R²=0.3901\n",
      "\n",
      "创建 XGBoost 预测器...\n",
      "  执行 XGBoost 逐日预测...\n",
      "  XGBoost 逐日预测性能: MAE=54579.69, WMAPE=54.00%, R²=0.4419\n",
      "\n",
      "创建 LightGBM 预测器...\n",
      "  执行 LightGBM 逐日预测...\n",
      "  LightGBM 逐日预测性能: MAE=54216.00, WMAPE=53.53%, R²=0.4356\n",
      "\n",
      "创建 QRF 预测器...\n",
      "  执行 QRF 逐日预测...\n",
      "  QRF 逐日预测性能: MAE=54388.59, WMAPE=55.08%, R²=0.4071\n",
      "\n",
      "创建 SVR 预测器...\n",
      "  执行 SVR 逐日预测...\n",
      "  SVR 逐日预测性能: MAE=63694.00, WMAPE=63.84%, R²=0.2121\n",
      "\n",
      "训练深度学习模型（启用搜索）...\n",
      "深度学习模型使用 6 个特征\n",
      "使用序列长度: 7\n",
      "\n",
      "训练 LSTM 模型（搜索中）...\n",
      "  尝试配置 1/5: {'units1': 32, 'units2': 16, 'dropout': 0.2, 'lr': 0.01, 'dense': 8}\n",
      "    验证损失: 0.0169\n",
      "  尝试配置 2/5: {'units1': 24, 'units2': 12, 'dropout': 0.25, 'lr': 0.005, 'dense': 6}\n",
      "    验证损失: 0.0171\n",
      "  尝试配置 3/5: {'units1': 48, 'units2': 24, 'dropout': 0.2, 'lr': 0.01, 'dense': 12}\n",
      "    验证损失: 0.0167\n",
      "  尝试配置 4/5: {'units1': 32, 'units2': 16, 'dropout': 0.3, 'lr': 0.005, 'dense': 8}\n",
      "    验证损失: 0.0169\n",
      "  尝试配置 5/5: {'units1': 40, 'units2': 20, 'dropout': 0.2, 'lr': 0.008, 'dense': 10}\n",
      "    验证损失: 0.0162\n",
      "  LSTM最佳配置: {'units1': 40, 'units2': 20, 'dropout': 0.2, 'lr': 0.008, 'dense': 10}, 验证损失: 0.0162\n",
      "  执行 LSTM 逐日序列预测...\n",
      "  LSTM 逐日预测性能: MAE=54947.62, WMAPE=49.60%, R²=0.4139\n",
      "\n",
      "训练基线模型...\n",
      "  LinearRegression 基线性能: MAE=57841.92, WMAPE=56.46%, R²=0.3808\n",
      "\n",
      "收集所有模型的逐日预测结果...\n",
      "\n",
      "创建改进的动态加权集成预测...\n",
      "基于性能的集成权重:\n",
      "  RandomForest: 0.169 (WMAPE: 53.44%)\n",
      "  XGBoost: 0.174 (WMAPE: 54.00%)\n",
      "  LightGBM: 0.174 (WMAPE: 53.53%)\n",
      "  QRF: 0.170 (WMAPE: 55.08%)\n",
      "  SVR: 0.138 (WMAPE: 63.84%)\n",
      "  LSTM: 0.174 (WMAPE: 49.60%)\n",
      "Improved_Ensemble 预测性能: MAE=53433.36, WMAPE=51.94%, R²=0.4354\n",
      "\n",
      "执行13周移动平均法预测...\n",
      "\n",
      "基于WMAPE(40%)+MAE(30%)+R²(30%)的综合排名:\n",
      "  1. LightGBM: 7.00分 (WMAPE: 35.80%, MAE: 216336, R²: 0.4154)\n",
      "  2. XGBoost: 5.70分 (WMAPE: 36.95%, MAE: 223267, R²: 0.3939)\n",
      "  3. LSTM: 5.30分 (WMAPE: 37.10%, MAE: 224147, R²: 0.4136)\n",
      "  4. Improved_Ensemble: 4.00分 (WMAPE: 37.10%, MAE: 224171, R²: 0.3684)\n",
      "  5. RandomForest: 2.70分 (WMAPE: 38.19%, MAE: 230765, R²: 0.2179)\n",
      "  6. QRF: 2.30分 (WMAPE: 41.09%, MAE: 248295, R²: 0.3005)\n",
      "  7. SVR: 1.00分 (WMAPE: 46.70%, MAE: 282185, R²: 0.1741)\n",
      "\n",
      "高碳铬铁逐日预测转周度汇总性能比较:\n",
      "============================================================\n",
      "\n",
      "MAE排名:\n",
      "  1. LightGBM: 216336\n",
      "  2. XGBoost: 223267\n",
      "  3. LSTM: 224147\n",
      "  4. Improved_Ensemble: 224171\n",
      "  5. RandomForest: 230765\n",
      "  6. QRF: 248295\n",
      "  7. LinearRegression: 258314\n",
      "  8. SVR: 282185\n",
      "  9. 13周移动平均: 306424\n",
      "\n",
      "WMAPE (%)排名:\n",
      "  1. LightGBM: 35.80%\n",
      "  2. XGBoost: 36.95%\n",
      "  3. LSTM: 37.10%\n",
      "  4. Improved_Ensemble: 37.10%\n",
      "  5. RandomForest: 38.19%\n",
      "  6. QRF: 41.09%\n",
      "  7. LinearRegression: 42.75%\n",
      "  8. SVR: 46.70%\n",
      "  9. 13周移动平均: 50.71%\n",
      "\n",
      "MAPE (%)排名:\n",
      "  1. QRF: 110.45%\n",
      "  2. RandomForest: 128.58%\n",
      "  3. SVR: 143.21%\n",
      "  4. Improved_Ensemble: 183.07%\n",
      "  5. XGBoost: 190.13%\n",
      "  6. LightGBM: 211.12%\n",
      "  7. LinearRegression: 252.05%\n",
      "  8. LSTM: 334.09%\n",
      "  9. 13周移动平均: 1201.06%\n",
      "\n",
      "RMSE排名:\n",
      "  1. LightGBM: 277178\n",
      "  2. LSTM: 277598\n",
      "  3. XGBoost: 282210\n",
      "  4. Improved_Ensemble: 288093\n",
      "  5. LinearRegression: 294869\n",
      "  6. QRF: 303194\n",
      "  7. RandomForest: 320585\n",
      "  8. SVR: 329446\n",
      "  9. 13周移动平均: 410673\n",
      "\n",
      "R²排名:\n",
      "  1. LightGBM: 0.4154\n",
      "  2. LSTM: 0.4136\n",
      "  3. XGBoost: 0.3939\n",
      "  4. Improved_Ensemble: 0.3684\n",
      "  5. LinearRegression: 0.3384\n",
      "  6. QRF: 0.3005\n",
      "  7. RandomForest: 0.2179\n",
      "  8. SVR: 0.1741\n",
      "  9. 13周移动平均: -0.2834\n",
      "\n",
      "预测比较图表保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_2024年逐日预测汇总比较.png\n",
      "详细结果保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_2024年逐日预测汇总结果.xlsx\n",
      "逐日预测详细结果保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_2024年逐日预测详细结果.xlsx\n",
      "模型性能对比保存至: /Users/bytedance/Desktop/特征工程补充/结果/高碳铬铁预测结果/高碳铬铁_逐日预测模型性能对比.xlsx\n",
      "\n",
      "高碳铬铁 的逐日预测比较完成!\n",
      "\n",
      "================================================================================\n",
      "开始处理 铝锭 的2024年逐日预测比较\n",
      "================================================================================\n",
      "输出目录: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果\n",
      "已加载铝锭的选定特征: 30个\n",
      "已加载 铝锭 的特征工程数据，形状: (1521, 248)\n",
      "\n",
      "开始为铝锭执行逐日预测与比较...\n",
      "训练集大小: 1461, 测试集大小: 60\n",
      "执行无数据泄漏的数据清理...\n",
      "清理后训练集大小: 1461, 测试集大小: 60\n",
      "使用特征数量: 30\n",
      "\n",
      "使用超参数搜索训练模型（策略: smart）...\n",
      "开始超参数搜索训练（策略: smart）...\n",
      "\n",
      "训练 RandomForest 模型...\n",
      "  RandomForest 使用 24 个特征\n",
      "    数据量: 1461\n",
      "    使用随机搜索（数据量 >= 200）\n",
      "    搜索完成，用时: 18.6秒\n",
      "    最佳参数: {'max_depth': 8, 'min_samples_leaf': 3, 'min_samples_split': 3, 'n_estimators': 587}\n",
      "    最佳得分: -95347.1072\n",
      "  RandomForest 搜索完成，用时: 18.6秒\n",
      "\n",
      "训练 XGBoost 模型...\n",
      "  XGBoost 使用 24 个特征\n",
      "    数据量: 1461\n",
      "    使用随机搜索（数据量 >= 200）\n",
      "    搜索完成，用时: 189.1秒\n",
      "    最佳参数: {'colsample_bytree': np.float64(0.7207954759246867), 'gamma': np.float64(0.12803416138066198), 'learning_rate': np.float64(0.008841191006150977), 'max_depth': 10, 'min_child_weight': 7, 'n_estimators': 853, 'reg_alpha': np.float64(0.08786730037315403), 'reg_lambda': np.float64(1.0042980058384905), 'subsample': np.float64(0.9187290787020559)}\n",
      "    最佳得分: -97078.0602\n",
      "  XGBoost 搜索完成，用时: 189.1秒\n",
      "\n",
      "训练 LightGBM 模型...\n",
      "  LightGBM 使用 24 个特征\n",
      "    数据量: 1461\n",
      "    使用随机搜索（数据量 >= 200）\n",
      "    搜索完成，用时: 166.2秒\n",
      "    最佳参数: {'colsample_bytree': np.float64(0.6995800817189007), 'learning_rate': np.float64(0.006485458640413423), 'max_depth': 4, 'n_estimators': 643, 'num_leaves': 20, 'reg_alpha': np.float64(0.03974313630683449), 'reg_lambda': np.float64(0.513805292809006), 'subsample': np.float64(0.8946384285364503)}\n",
      "    最佳得分: -97252.6943\n",
      "  LightGBM 搜索完成，用时: 166.2秒\n",
      "\n",
      "训练 QRF 模型...\n",
      "  QRF 使用 24 个特征\n",
      "    数据量: 1461\n",
      "    使用随机搜索（数据量 >= 200）\n",
      "    搜索完成，用时: 20.4秒\n",
      "    最佳参数: {'max_depth': 11, 'min_samples_leaf': 2, 'n_estimators': 493}\n",
      "    最佳得分: -90832.1750\n",
      "  QRF 搜索完成，用时: 20.4秒\n",
      "\n",
      "训练 SVR 模型...\n",
      "  SVR 使用 15 个特征\n",
      "    数据量: 1461\n",
      "    使用网格搜索（强制模式）\n",
      "    搜索完成，用时: 11.9秒\n",
      "    最佳参数: {'C': 100, 'epsilon': 0.1, 'gamma': 0.04, 'kernel': 'rbf'}\n",
      "    最佳得分: -0.6621\n",
      "  SVR 搜索完成，用时: 11.9秒\n",
      "最佳参数已保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_最佳参数.xlsx\n",
      "\n",
      "创建 RandomForest 预测器...\n",
      "  执行 RandomForest 逐日预测...\n",
      "  RandomForest 逐日预测性能: MAE=59518.52, WMAPE=59.38%, R²=0.3506\n",
      "\n",
      "创建 XGBoost 预测器...\n",
      "  执行 XGBoost 逐日预测...\n",
      "  XGBoost 逐日预测性能: MAE=63843.19, WMAPE=65.14%, R²=0.2535\n",
      "\n",
      "创建 LightGBM 预测器...\n",
      "  执行 LightGBM 逐日预测...\n",
      "  LightGBM 逐日预测性能: MAE=62427.66, WMAPE=56.38%, R²=0.2344\n",
      "\n",
      "创建 QRF 预测器...\n",
      "  执行 QRF 逐日预测...\n",
      "  QRF 逐日预测性能: MAE=48019.16, WMAPE=53.93%, R²=0.4231\n",
      "\n",
      "创建 SVR 预测器...\n",
      "  执行 SVR 逐日预测...\n",
      "  SVR 逐日预测性能: MAE=54904.20, WMAPE=56.71%, R²=0.2434\n",
      "\n",
      "训练深度学习模型（启用搜索）...\n",
      "深度学习模型使用 7 个特征\n",
      "使用序列长度: 7\n",
      "\n",
      "训练 LSTM 模型（搜索中）...\n",
      "  尝试配置 1/5: {'units1': 32, 'units2': 16, 'dropout': 0.2, 'lr': 0.01, 'dense': 8}\n",
      "    验证损失: 0.0065\n",
      "  尝试配置 2/5: {'units1': 24, 'units2': 12, 'dropout': 0.25, 'lr': 0.005, 'dense': 6}\n",
      "    验证损失: 0.0063\n",
      "  尝试配置 3/5: {'units1': 48, 'units2': 24, 'dropout': 0.2, 'lr': 0.01, 'dense': 12}\n",
      "    验证损失: 0.0063\n",
      "  尝试配置 4/5: {'units1': 32, 'units2': 16, 'dropout': 0.3, 'lr': 0.005, 'dense': 8}\n",
      "    验证损失: 0.0064\n",
      "  尝试配置 5/5: {'units1': 40, 'units2': 20, 'dropout': 0.2, 'lr': 0.008, 'dense': 10}\n",
      "    验证损失: 0.0063\n",
      "  LSTM最佳配置: {'units1': 40, 'units2': 20, 'dropout': 0.2, 'lr': 0.008, 'dense': 10}, 验证损失: 0.0063\n",
      "  执行 LSTM 逐日序列预测...\n",
      "  LSTM 逐日预测性能: MAE=71899.67, WMAPE=54.00%, R²=0.0660\n",
      "\n",
      "训练基线模型...\n",
      "  LinearRegression 基线性能: MAE=64089.49, WMAPE=58.10%, R²=-0.0127\n",
      "\n",
      "收集所有模型的逐日预测结果...\n",
      "\n",
      "创建改进的动态加权集成预测...\n",
      "基于性能的集成权重:\n",
      "  RandomForest: 0.176 (WMAPE: 59.38%)\n",
      "  XGBoost: 0.158 (WMAPE: 65.14%)\n",
      "  LightGBM: 0.163 (WMAPE: 56.38%)\n",
      "  QRF: 0.196 (WMAPE: 53.93%)\n",
      "  SVR: 0.167 (WMAPE: 56.71%)\n",
      "  LSTM: 0.139 (WMAPE: 54.00%)\n",
      "Improved_Ensemble 预测性能: MAE=58214.36, WMAPE=56.16%, R²=0.3551\n",
      "\n",
      "执行13周移动平均法预测...\n",
      "\n",
      "基于WMAPE(40%)+MAE(30%)+R²(30%)的综合排名:\n",
      "  1. QRF: 7.00分 (WMAPE: 20.21%, MAE: 112862, R²: 0.7771)\n",
      "  2. SVR: 5.40分 (WMAPE: 27.74%, MAE: 154907, R²: 0.5376)\n",
      "  3. Improved_Ensemble: 5.30分 (WMAPE: 30.60%, MAE: 170882, R²: 0.5642)\n",
      "  4. RandomForest: 4.30分 (WMAPE: 32.51%, MAE: 181513, R²: 0.5389)\n",
      "  5. XGBoost: 3.00分 (WMAPE: 34.03%, MAE: 189993, R²: 0.5285)\n",
      "  6. LightGBM: 2.00分 (WMAPE: 44.29%, MAE: 247304, R²: 0.1748)\n",
      "  7. LSTM: 1.00分 (WMAPE: 58.42%, MAE: 326205, R²: -0.6140)\n",
      "\n",
      "铝锭逐日预测转周度汇总性能比较:\n",
      "============================================================\n",
      "\n",
      "MAE排名:\n",
      "  1. QRF: 112862\n",
      "  2. SVR: 154907\n",
      "  3. Improved_Ensemble: 170882\n",
      "  4. RandomForest: 181513\n",
      "  5. XGBoost: 189993\n",
      "  6. LightGBM: 247304\n",
      "  7. LinearRegression: 296897\n",
      "  8. LSTM: 326205\n",
      "  9. 13周移动平均: 377428\n",
      "\n",
      "WMAPE (%)排名:\n",
      "  1. QRF: 20.21%\n",
      "  2. SVR: 27.74%\n",
      "  3. Improved_Ensemble: 30.60%\n",
      "  4. RandomForest: 32.51%\n",
      "  5. XGBoost: 34.03%\n",
      "  6. LightGBM: 44.29%\n",
      "  7. LinearRegression: 53.17%\n",
      "  8. LSTM: 58.42%\n",
      "  9. 13周移动平均: 67.59%\n",
      "\n",
      "MAPE (%)排名:\n",
      "  1. QRF: 66.26%\n",
      "  2. SVR: 100.05%\n",
      "  3. XGBoost: 231.77%\n",
      "  4. RandomForest: 261.36%\n",
      "  5. Improved_Ensemble: 278.16%\n",
      "  6. LightGBM: 388.79%\n",
      "  7. LinearRegression: 469.89%\n",
      "  8. LSTM: 759.75%\n",
      "  9. 13周移动平均: 893.80%\n",
      "\n",
      "RMSE排名:\n",
      "  1. QRF: 150002\n",
      "  2. Improved_Ensemble: 209748\n",
      "  3. RandomForest: 215755\n",
      "  4. SVR: 216058\n",
      "  5. XGBoost: 218176\n",
      "  6. LightGBM: 288612\n",
      "  7. LinearRegression: 363042\n",
      "  8. LSTM: 403641\n",
      "  9. 13周移动平均: 473065\n",
      "\n",
      "R²排名:\n",
      "  1. QRF: 0.7771\n",
      "  2. Improved_Ensemble: 0.5642\n",
      "  3. RandomForest: 0.5389\n",
      "  4. SVR: 0.5376\n",
      "  5. XGBoost: 0.5285\n",
      "  6. LightGBM: 0.1748\n",
      "  7. LinearRegression: -0.3056\n",
      "  8. LSTM: -0.6140\n",
      "  9. 13周移动平均: -1.2169\n",
      "\n",
      "预测比较图表保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_2024年逐日预测汇总比较.png\n",
      "详细结果保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_2024年逐日预测汇总结果.xlsx\n",
      "逐日预测详细结果保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_2024年逐日预测详细结果.xlsx\n",
      "模型性能对比保存至: /Users/bytedance/Desktop/特征工程补充/结果/铝锭预测结果/铝锭_逐日预测模型性能对比.xlsx\n",
      "\n",
      "铝锭 的逐日预测比较完成!\n",
      "\n",
      "================================================================================\n",
      "生成逐日预测汇总报告\n",
      "================================================================================\n",
      "\n",
      "生成逐日预测汇总报告...\n",
      "  逐日预测汇总结果已保存至: /Users/bytedance/Desktop/特征工程补充/结果/逐日预测汇总结果.xlsx\n",
      "\n",
      "基于MAE的各产品最佳模型:\n",
      "- 电解镍: SVR (MAE: 23327)\n",
      "- 高碳铬铁: LightGBM (MAE: 216336)\n",
      "- 铝锭: QRF (MAE: 112862)\n",
      "\n",
      "基于RMSE的各产品最佳模型:\n",
      "- 电解镍: SVR (RMSE: 29193)\n",
      "- 高碳铬铁: LightGBM (RMSE: 277178)\n",
      "- 铝锭: QRF (RMSE: 150002)\n",
      "\n",
      "基于MAPE (%)的各产品最佳模型:\n",
      "- 电解镍: SVR (MAPE (%): 40.46%)\n",
      "- 高碳铬铁: QRF (MAPE (%): 110.45%)\n",
      "- 铝锭: QRF (MAPE (%): 66.26%)\n",
      "\n",
      "基于WMAPE (%)的各产品最佳模型:\n",
      "- 电解镍: SVR (WMAPE (%): 22.31%)\n",
      "- 高碳铬铁: LightGBM (WMAPE (%): 35.80%)\n",
      "- 铝锭: QRF (WMAPE (%): 20.21%)\n",
      "\n",
      "基于R²的各产品最佳模型:\n",
      "- 电解镍: SVR (R²: 0.6940)\n",
      "- 高碳铬铁: LightGBM (R²: 0.4154)\n",
      "- 铝锭: QRF (R²: 0.7771)\n",
      "  逐日预测最佳模型汇总已保存至: /Users/bytedance/Desktop/特征工程补充/结果/逐日预测最佳模型汇总.xlsx\n",
      "  可用指标: ['MAE', 'RMSE', 'R²', 'MAPE (%)', 'WMAPE (%)']\n",
      "  将显示的指标: ['MAE', 'RMSE', 'MAPE (%)', 'WMAPE (%)', 'R²']\n",
      "  逐日预测各产品独立指标热力图已保存至: /Users/bytedance/Desktop/特征工程补充/结果/逐日预测各产品独立指标热力图.png\n",
      "\n",
      "逐日预测模型一致性分析 (基于WMAPE40%+MAE30%+R²30%):\n",
      "- 电解镍: MAE→SVR | WMAPE (%)→SVR | R²→SVR | 其他: RMSE→SVR, MAPE (%)→SVR\n",
      "  → 推荐: SVR (一致性: 5/5, 加权: 1.00)\n",
      "- 高碳铬铁: MAE→LightGBM | WMAPE (%)→LightGBM | R²→LightGBM | 其他: RMSE→LightGBM, MAPE (%)→QRF\n",
      "  → 推荐: LightGBM (一致性: 4/5, 加权: 0.95)\n",
      "- 铝锭: MAE→QRF | WMAPE (%)→QRF | R²→QRF | 其他: RMSE→QRF, MAPE (%)→QRF\n",
      "  → 推荐: QRF (一致性: 5/5, 加权: 1.00)\n",
      "  逐日预测模型一致性分析已保存至: /Users/bytedance/Desktop/特征工程补充/结果/逐日预测模型一致性分析.xlsx\n",
      "\n",
      "最终模型推荐汇总:\n",
      "- 电解镍: 推荐 SVR (综合得分: 7.00)\n",
      "  前三名: SVR(7.0分), RandomForest(5.1分), LightGBM(5.0分)\n",
      "- 高碳铬铁: 推荐 LightGBM (综合得分: 7.00)\n",
      "  前三名: LightGBM(7.0分), XGBoost(5.7分), LSTM(5.3分)\n",
      "- 铝锭: 推荐 QRF (综合得分: 7.00)\n",
      "  前三名: QRF(7.0分), SVR(5.4分), Improved_Ensemble(5.3分)\n",
      "  最终模型推荐汇总已保存至: /Users/bytedance/Desktop/特征工程补充/结果/最终模型推荐汇总.xlsx\n",
      "\n",
      "逐日预测汇总报告已保存到: /Users/bytedance/Desktop/特征工程补充/结果\n",
      "\n",
      "所有逐日预测处理完成!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "import holidays\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
    "from scipy.stats import uniform, randint\n",
    "import time\n",
    "\n",
    "# A/B 特征与测试期B类填充工具\n",
    "from ab_feature_utils import BFeatureFillerDaily, separate_a_b_features\n",
    "\n",
    "# 深度学习模型导入\n",
    "try:\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    import tensorflow as tf\n",
    "    tf.get_logger().setLevel('ERROR')  # 减少TensorFlow日志\n",
    "    KERAS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"警告: TensorFlow/Keras未安装，LSTM、CNN模型将不可用\")\n",
    "    KERAS_AVAILABLE = False\n",
    "\n",
    "# ARIMAX模型导入\n",
    "try:\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    STATSMODELS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"警告: statsmodels未安装，ARIMAX模型将不可用\")\n",
    "    STATSMODELS_AVAILABLE = False\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 辅助函数\n",
    "# =============================================================================\n",
    "def calculate_streamlined_metrics(y_true, y_pred):\n",
    "    \"\"\"计算精简的评估指标\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    # 基础指标\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # 原始MAPE（处理除零问题）\n",
    "    epsilon = 1e-10\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "    \n",
    "    # 加权MAPE（主要业务指标）\n",
    "    weights = y_true / (y_true.sum() + epsilon)\n",
    "    weighted_ape = np.abs((y_true - y_pred) / (y_true + epsilon)) * weights\n",
    "    wmape = weighted_ape.sum() * 100\n",
    "    \n",
    "    return {\n",
    "        'MAE': mae, \n",
    "        'RMSE': rmse, \n",
    "        'R²': r2, \n",
    "        'MAPE (%)': mape,  # 改为原始MAPE\n",
    "        'WMAPE (%)': wmape\n",
    "    }\n",
    "\n",
    "def select_model_specific_features(features, X_train, y_train, model_type, model_name):\n",
    "    \"\"\"根据模型类型选择最适合的特征数量\"\"\"\n",
    "    from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "    from sklearn.feature_selection import mutual_info_regression\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if model_type == 'linear':\n",
    "        k = min(12, max(8, len(features) // 3))\n",
    "        selector = SelectKBest(score_func=f_regression, k=k)\n",
    "        X_selected = selector.fit_transform(X_train, y_train)\n",
    "        selected_mask = selector.get_support()\n",
    "        selected_features = [features[i] for i in range(len(features)) if selected_mask[i]]\n",
    "        \n",
    "        if len(selected_features) > 10:\n",
    "            estimator = LinearRegression()\n",
    "            rfe = RFE(estimator, n_features_to_select=10)\n",
    "            rfe.fit(X_selected, y_train)\n",
    "            final_mask = rfe.get_support()\n",
    "            selected_features = [selected_features[i] for i in range(len(selected_features)) if final_mask[i]]\n",
    "        \n",
    "        return selected_features\n",
    "    \n",
    "    elif model_type == 'tree':\n",
    "        k = min(len(features), max(20, int(len(features) * 0.8)))\n",
    "        rf_temp = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "        rf_temp.fit(X_train, y_train)\n",
    "        \n",
    "        importance_scores = rf_temp.feature_importances_\n",
    "        feature_importance = list(zip(features, importance_scores))\n",
    "        feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        selected_features = [feat[0] for feat in feature_importance[:k]]\n",
    "        return selected_features\n",
    "    \n",
    "    elif model_type in ['svm', 'neural']:\n",
    "        k = min(15, max(10, len(features) // 2))\n",
    "        selector = SelectKBest(score_func=mutual_info_regression, k=k)\n",
    "        selector.fit(X_train, y_train)\n",
    "        selected_mask = selector.get_support()\n",
    "        selected_features = [features[i] for i in range(len(features)) if selected_mask[i]]\n",
    "        return selected_features\n",
    "    \n",
    "    elif model_type == 'deep':\n",
    "        k = min(10, max(6, len(features) // 4))\n",
    "        selector = SelectKBest(score_func=f_regression, k=k)\n",
    "        selector.fit(X_train, y_train)\n",
    "        selected_mask = selector.get_support()\n",
    "        selected_features = [features[i] for i in range(len(features)) if selected_mask[i]]\n",
    "        return selected_features\n",
    "    \n",
    "    else:\n",
    "        return features\n",
    "\n",
    "def get_preprocessed_data(X_train, X_test, y_train, model_type='tree'):\n",
    "    \"\"\"根据模型类型选择合适的预处理方法\"\"\"\n",
    "    \n",
    "    if model_type == 'tree':\n",
    "        scaler_X = RobustScaler()\n",
    "        scaler_y = None\n",
    "        \n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_X.transform(X_test)\n",
    "        y_train_scaled = y_train.copy()\n",
    "        \n",
    "    elif model_type == 'neural':\n",
    "        scaler_X = StandardScaler()\n",
    "        try:\n",
    "            scaler_y = PowerTransformer(method='yeo-johnson')\n",
    "            y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "        except:\n",
    "            scaler_y = StandardScaler()\n",
    "            y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_X.transform(X_test)\n",
    "        \n",
    "    elif model_type == 'svm':\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "        \n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_X.transform(X_test)\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    elif model_type == 'linear':\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = None\n",
    "        \n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_X.transform(X_test)\n",
    "        y_train_scaled = y_train.copy()\n",
    "        \n",
    "    elif model_type == 'deep':\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "        \n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_X.transform(X_test)\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train_scaled, scaler_X, scaler_y\n",
    "\n",
    "def leak_free_fix_data_issues(train_df, test_df=None):\n",
    "    \"\"\"无数据泄漏的数据修复方法\"\"\"\n",
    "    print(\"执行无数据泄漏的数据清理...\")\n",
    "    \n",
    "    stats = {}\n",
    "    train_cleaned = train_df.copy()\n",
    "    \n",
    "    # 1. 处理负值需求\n",
    "    if 'demand' in train_cleaned.columns:\n",
    "        min_demand = train_cleaned['demand'].min()\n",
    "        if min_demand < 0:\n",
    "            print(f\"  修复负值需求: 最小值={min_demand}\")\n",
    "            train_cleaned['demand'] = train_cleaned['demand'].clip(lower=0)\n",
    "    \n",
    "    # 2. 处理无穷大值\n",
    "    numeric_cols = train_cleaned.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col == 'date':\n",
    "            continue\n",
    "            \n",
    "        inf_count = np.isinf(train_cleaned[col]).sum()\n",
    "        if inf_count > 0:\n",
    "            finite_values = train_cleaned[col][np.isfinite(train_cleaned[col])]\n",
    "            if len(finite_values) > 0:\n",
    "                fill_value = finite_values.median()\n",
    "            else:\n",
    "                fill_value = 0\n",
    "            stats[f'{col}_inf_fill'] = fill_value\n",
    "            \n",
    "            train_cleaned[col] = train_cleaned[col].replace([np.inf, -np.inf], fill_value)\n",
    "    \n",
    "    # 3. 处理极值\n",
    "    for col in numeric_cols:\n",
    "        if col in ['date', 'demand']:\n",
    "            continue\n",
    "            \n",
    "        finite_values = train_cleaned[col][np.isfinite(train_cleaned[col])]\n",
    "        if len(finite_values) > 10:\n",
    "            Q1 = finite_values.quantile(0.25)\n",
    "            Q3 = finite_values.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            if IQR > 0:\n",
    "                lower_bound = Q1 - 3 * IQR\n",
    "                upper_bound = Q3 + 3 * IQR\n",
    "                stats[f'{col}_lower_bound'] = lower_bound\n",
    "                stats[f'{col}_upper_bound'] = upper_bound\n",
    "                \n",
    "                extreme_mask = (train_cleaned[col] < lower_bound) | (train_cleaned[col] > upper_bound)\n",
    "                extreme_count = extreme_mask.sum()\n",
    "                if extreme_count > 0:\n",
    "                    train_cleaned.loc[extreme_mask, col] = np.clip(train_cleaned[col], lower_bound, upper_bound)\n",
    "        else:\n",
    "            stats[f'{col}_lower_bound'] = -1e10\n",
    "            stats[f'{col}_upper_bound'] = 1e10\n",
    "    \n",
    "    # 4. 处理NaN值\n",
    "    for col in train_cleaned.columns:\n",
    "        if train_cleaned[col].isna().any():\n",
    "            if col == 'date':\n",
    "                train_cleaned = train_cleaned.dropna(subset=['date'])\n",
    "            elif col == 'demand':\n",
    "                train_cleaned[col] = train_cleaned[col].fillna(method='ffill').fillna(0)\n",
    "            elif train_cleaned[col].dtype == 'object':\n",
    "                train_cleaned[col] = train_cleaned[col].fillna('unknown')\n",
    "            else:\n",
    "                fill_value = train_cleaned[col].median() if train_cleaned[col].notna().sum() > 0 else 0\n",
    "                stats[f'{col}_nan_fill'] = fill_value\n",
    "                train_cleaned[col] = train_cleaned[col].fillna(method='ffill').fillna(fill_value)\n",
    "    \n",
    "    # 5. 确保日期列有效\n",
    "    if 'date' in train_cleaned.columns:\n",
    "        train_cleaned['date'] = pd.to_datetime(train_cleaned['date'], errors='coerce')\n",
    "        train_cleaned = train_cleaned.dropna(subset=['date'])\n",
    "    \n",
    "    # 如果提供了测试集，使用训练集统计量处理测试集\n",
    "    if test_df is not None:\n",
    "        test_cleaned = test_df.copy()\n",
    "        \n",
    "        # 使用相同的处理逻辑，但用训练集的统计量\n",
    "        if 'demand' in test_cleaned.columns:\n",
    "            test_cleaned['demand'] = test_cleaned['demand'].clip(lower=0)\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col == 'date':\n",
    "                continue\n",
    "                \n",
    "            inf_count = np.isinf(test_cleaned[col]).sum()\n",
    "            if inf_count > 0:\n",
    "                fill_value = stats.get(f'{col}_inf_fill', 0)\n",
    "                test_cleaned[col] = test_cleaned[col].replace([np.inf, -np.inf], fill_value)\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col in ['date', 'demand']:\n",
    "                continue\n",
    "                \n",
    "            lower_bound = stats.get(f'{col}_lower_bound', -1e10)\n",
    "            upper_bound = stats.get(f'{col}_upper_bound', 1e10)\n",
    "            \n",
    "            extreme_mask = (test_cleaned[col] < lower_bound) | (test_cleaned[col] > upper_bound)\n",
    "            extreme_count = extreme_mask.sum()\n",
    "            if extreme_count > 0:\n",
    "                test_cleaned.loc[extreme_mask, col] = np.clip(test_cleaned[col], lower_bound, upper_bound)\n",
    "        \n",
    "        for col in test_cleaned.columns:\n",
    "            if test_cleaned[col].isna().any():\n",
    "                if col == 'date':\n",
    "                    test_cleaned = test_cleaned.dropna(subset=['date'])\n",
    "                elif col == 'demand':\n",
    "                    test_cleaned[col] = test_cleaned[col].fillna(method='ffill').fillna(0)\n",
    "                elif test_cleaned[col].dtype == 'object':\n",
    "                    test_cleaned[col] = test_cleaned[col].fillna('unknown')\n",
    "                else:\n",
    "                    fill_value = stats.get(f'{col}_nan_fill', 0)\n",
    "                    test_cleaned[col] = test_cleaned[col].fillna(method='ffill').fillna(fill_value)\n",
    "        \n",
    "        if 'date' in test_cleaned.columns:\n",
    "            test_cleaned['date'] = pd.to_datetime(test_cleaned['date'], errors='coerce')\n",
    "            test_cleaned = test_cleaned.dropna(subset=['date'])\n",
    "        \n",
    "        return train_cleaned, test_cleaned\n",
    "    \n",
    "    else:\n",
    "        return train_cleaned, stats\n",
    "\n",
    "def load_selected_features(product, output_dir):\n",
    "    \"\"\"加载已选择的最佳特征\"\"\"\n",
    "    product_output_dir = os.path.join(output_dir, f\"{product}预测结果\")\n",
    "    best_features_path = os.path.join(product_output_dir, f\"{product}_最佳特征.csv\")\n",
    "    \n",
    "    if os.path.exists(best_features_path):\n",
    "        best_features_df = pd.read_csv(best_features_path)\n",
    "        return best_features_df['feature'].tolist()\n",
    "    else:\n",
    "        print(f\"警告: 找不到{product}的最佳特征文件，使用默认特征\")\n",
    "        return [\n",
    "            'is_holiday', 'is_Monday', 'is_Tuesday', 'is_Wednesday', 'is_Thursday', \n",
    "            'is_Friday', 'is_Saturday', 'is_Sunday', 'is_weekend', 'demand_lag14', \n",
    "            'demand_ma3', 'demand_ma7', 'seasonal_factor', 'weekday_factor', \n",
    "            'is_future_volatile', 'trend', 'residual', 'seasonal'\n",
    "        ]\n",
    "\n",
    "# =============================================================================\n",
    "# 核心类：逐日预测器\n",
    "# =============================================================================\n",
    "class DailyPredictor:\n",
    "    \"\"\"逐日预测器类 - 模拟实际应用场景\"\"\"\n",
    "    \n",
    "    def __init__(self, model, scaler_X, scaler_y, features, model_type, model_name):\n",
    "        self.model = model\n",
    "        self.scaler_X = scaler_X\n",
    "        self.scaler_y = scaler_y\n",
    "        self.features = features\n",
    "        self.model_type = model_type\n",
    "        self.model_name = model_name\n",
    "        self.prediction_history = []\n",
    "    \n",
    "    def predict_single_day(self, daily_features_dict):\n",
    "        \"\"\"预测单天需求\"\"\"\n",
    "        try:\n",
    "            # 构造特征向量\n",
    "            feature_vector = []\n",
    "            for feature in self.features:\n",
    "                value = daily_features_dict.get(feature, 0)\n",
    "                if pd.isna(value) or np.isinf(value):\n",
    "                    value = 0\n",
    "                feature_vector.append(value)\n",
    "            \n",
    "            feature_vector = np.array(feature_vector).reshape(1, -1)\n",
    "            \n",
    "            # 预处理\n",
    "            feature_scaled = self.scaler_X.transform(feature_vector)\n",
    "            \n",
    "            # 预测\n",
    "            if self.model_type == 'deep':\n",
    "                pred_scaled = self.model.predict(feature_scaled, verbose=0)[0]\n",
    "            else:\n",
    "                pred_scaled = self.model.predict(feature_scaled)[0]\n",
    "            \n",
    "            # 逆变换\n",
    "            if self.scaler_y is not None:\n",
    "                pred = self.scaler_y.inverse_transform([[pred_scaled]])[0, 0]\n",
    "            else:\n",
    "                pred = pred_scaled\n",
    "            \n",
    "            # 确保非负\n",
    "            pred = max(0, pred)\n",
    "            \n",
    "            # 记录预测历史\n",
    "            self.prediction_history.append(pred)\n",
    "            \n",
    "            return pred\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  {self.model_name} 单日预测失败: {str(e)}\")\n",
    "            return 0\n",
    "    \n",
    "    def predict_test_period(self, test_data, train_data=None):\n",
    "        \"\"\"逐日预测整个测试期间（测试期仅使用A类特征+预测填充的B类特征）。\"\"\"\n",
    "        # 兼容：未提供训练数据时，退回原逻辑\n",
    "        if train_data is None:\n",
    "            predictions = []\n",
    "            for i in range(len(test_data)):\n",
    "                daily_features = test_data.iloc[i][self.features].to_dict()\n",
    "                pred = self.predict_single_day(daily_features)\n",
    "                predictions.append(pred)\n",
    "            return np.array(predictions)\n",
    "\n",
    "        try:\n",
    "            test_sorted = test_data.copy()\n",
    "            if 'date' in test_sorted.columns:\n",
    "                test_sorted = test_sorted.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "            a_feats, b_feats = separate_a_b_features(self.features)\n",
    "\n",
    "            filler = BFeatureFillerDaily(\n",
    "                train_df=train_data[['date', 'demand'] + [c for c in self.features if c in train_data.columns]].copy(),\n",
    "                features=self.features,\n",
    "                a_features=a_feats,\n",
    "                b_features=b_feats,\n",
    "                date_col='date',\n",
    "            )\n",
    "\n",
    "            predictions = []\n",
    "            for i in range(len(test_sorted)):\n",
    "                row = test_sorted.iloc[i]\n",
    "                feat_dict = filler.build_features_for_row(row)\n",
    "                pred = self.predict_single_day(feat_dict)\n",
    "                predictions.append(pred)\n",
    "                filler.update_with_prediction(pred)\n",
    "\n",
    "            return np.array(predictions)\n",
    "        except Exception as e:\n",
    "            print(f\"  {self.model_name} 使用A/B填充预测失败，退回原逻辑: {str(e)}\")\n",
    "            predictions = []\n",
    "            for i in range(len(test_data)):\n",
    "                daily_features = test_data.iloc[i][self.features].to_dict()\n",
    "                pred = self.predict_single_day(daily_features)\n",
    "                predictions.append(pred)\n",
    "            return np.array(predictions)\n",
    "\n",
    "class SequencePredictor:\n",
    "    \"\"\"序列预测器类 - 用于LSTM/GRU等需要序列数据的模型\"\"\"\n",
    "    \n",
    "    def __init__(self, model, scaler_X, scaler_y, features, sequence_length, model_name):\n",
    "        self.model = model\n",
    "        self.scaler_X = scaler_X\n",
    "        self.scaler_y = scaler_y\n",
    "        self.features = features\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model_name = model_name\n",
    "        self.feature_history = []  # 存储特征历史\n",
    "    \n",
    "    def add_daily_features(self, daily_features_dict):\n",
    "        \"\"\"添加当天特征到历史记录\"\"\"\n",
    "        feature_vector = []\n",
    "        for feature in self.features:\n",
    "            value = daily_features_dict.get(feature, 0)\n",
    "            if pd.isna(value) or np.isinf(value):\n",
    "                value = 0\n",
    "            feature_vector.append(value)\n",
    "        \n",
    "        # 预处理特征\n",
    "        feature_scaled = self.scaler_X.transform([feature_vector])[0]\n",
    "        \n",
    "        # 添加到历史\n",
    "        self.feature_history.append(feature_scaled)\n",
    "        \n",
    "        # 保持序列长度\n",
    "        if len(self.feature_history) > self.sequence_length:\n",
    "            self.feature_history.pop(0)\n",
    "    \n",
    "    def predict_single_day(self):\n",
    "        \"\"\"基于历史序列预测当天需求\"\"\"\n",
    "        try:\n",
    "            if len(self.feature_history) < self.sequence_length:\n",
    "                return 0  # 历史数据不足\n",
    "            \n",
    "            # 构造序列\n",
    "            sequence = np.array(self.feature_history[-self.sequence_length:]).reshape(1, self.sequence_length, -1)\n",
    "            \n",
    "            # 预测\n",
    "            pred_scaled = self.model.predict(sequence, verbose=0)[0, 0]\n",
    "            \n",
    "            # 逆变换\n",
    "            if self.scaler_y is not None:\n",
    "                pred = self.scaler_y.inverse_transform([[pred_scaled]])[0, 0]\n",
    "            else:\n",
    "                pred = pred_scaled\n",
    "            \n",
    "            return max(0, pred)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  {self.model_name} 序列预测失败: {str(e)}\")\n",
    "            return 0\n",
    "    \n",
    "    def predict_test_period(self, test_data, train_data):\n",
    "        \"\"\"逐日预测整个测试期间\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        # 用训练数据的最后几天初始化历史\n",
    "        if len(train_data) >= self.sequence_length:\n",
    "            for i in range(-self.sequence_length, 0):\n",
    "                daily_features = train_data.iloc[i][self.features].to_dict()\n",
    "                self.add_daily_features(daily_features)\n",
    "        \n",
    "        # 逐日预测测试数据\n",
    "        for i in range(len(test_data)):\n",
    "            # 预测当天\n",
    "            pred = self.predict_single_day()\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            # 添加当天特征到历史（用于下一天预测）\n",
    "            daily_features = test_data.iloc[i][self.features].to_dict()\n",
    "            self.add_daily_features(daily_features)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n",
    "# =============================================================================\n",
    "# 改进的集成和评估函数\n",
    "# =============================================================================\n",
    "def create_performance_based_ensemble(model_predictions, model_performance, exclude_baselines=True):\n",
    "    \"\"\"基于性能创建集成预测，排除基线模型\"\"\"\n",
    "    \n",
    "    # 排除基线模型\n",
    "    baseline_models = ['LinearRegression', '13周移动平均'] if exclude_baselines else []\n",
    "    \n",
    "    # 过滤模型：排除基线模型和表现极差的模型\n",
    "    filtered_predictions = {}\n",
    "    valid_performances = {}\n",
    "    \n",
    "    for name, pred in model_predictions.items():\n",
    "        if name not in baseline_models and name in model_performance:\n",
    "            # 排除WMAPE > 80%的模型（表现极差）\n",
    "            wmape = model_performance[name].get('WMAPE (%)', float('inf'))\n",
    "            if wmape < 80:\n",
    "                filtered_predictions[name] = pred\n",
    "                valid_performances[name] = model_performance[name]\n",
    "    \n",
    "    if len(filtered_predictions) < 2:\n",
    "        print(\"可用于集成的模型数量不足，使用所有非基线模型\")\n",
    "        # 如果过滤后模型太少，放宽条件\n",
    "        for name, pred in model_predictions.items():\n",
    "            if name not in baseline_models and name in model_performance:\n",
    "                filtered_predictions[name] = pred\n",
    "                valid_performances[name] = model_performance[name]\n",
    "    \n",
    "    if len(filtered_predictions) < 2:\n",
    "        return None, None\n",
    "    \n",
    "    # 基于多指标计算权重\n",
    "    def calculate_composite_score(metrics):\n",
    "        \"\"\"计算综合得分\"\"\"\n",
    "        wmape = metrics.get('WMAPE (%)', 100)\n",
    "        mae = metrics.get('MAE', float('inf'))\n",
    "        r2 = max(0, metrics.get('R²', 0))  # R²可能为负，设下限为0\n",
    "        \n",
    "        # 归一化分数（越小越好的指标转换为越大越好）\n",
    "        wmape_score = 1.0 / (1.0 + wmape / 50)  # 50%为基准\n",
    "        mae_score = 1.0 / (1.0 + mae / 50000)   # 根据实际MAE调整基准\n",
    "        r2_score = r2\n",
    "        \n",
    "        # 加权组合：WMAPE(40%) + MAE(30%) + R²(30%)\n",
    "        composite = 0.4 * wmape_score + 0.3 * mae_score + 0.3 * r2_score\n",
    "        return composite\n",
    "    \n",
    "    # 计算权重\n",
    "    scores = {name: calculate_composite_score(metrics) \n",
    "              for name, metrics in valid_performances.items()}\n",
    "    \n",
    "    total_score = sum(scores.values())\n",
    "    if total_score == 0:\n",
    "        # 如果所有得分都是0，使用等权重\n",
    "        weights = {name: 1.0/len(scores) for name in scores.keys()}\n",
    "    else:\n",
    "        weights = {name: score / total_score for name, score in scores.items()}\n",
    "    \n",
    "    # 计算集成预测\n",
    "    pred_lengths = [len(pred) for pred in filtered_predictions.values()]\n",
    "    base_length = min(pred_lengths)\n",
    "    \n",
    "    ensemble_pred = np.zeros(base_length)\n",
    "    \n",
    "    print(\"基于性能的集成权重:\")\n",
    "    for model_name, pred in filtered_predictions.items():\n",
    "        weight = weights[model_name]\n",
    "        wmape = valid_performances[model_name]['WMAPE (%)']\n",
    "        print(f\"  {model_name}: {weight:.3f} (WMAPE: {wmape:.2f}%)\")\n",
    "        \n",
    "        pred_adj = pred[:base_length]\n",
    "        ensemble_pred += weight * pred_adj\n",
    "    \n",
    "    return np.maximum(ensemble_pred, 0), weights\n",
    "\n",
    "def calculate_model_ranking(model_performance, exclude_baselines=True):\n",
    "    \"\"\"计算模型综合排名\"\"\"\n",
    "    \n",
    "    # 排除基线模型\n",
    "    baseline_models = ['LinearRegression', '13周移动平均'] if exclude_baselines else []\n",
    "    filtered_performance = {k: v for k, v in model_performance.items() if k not in baseline_models}\n",
    "    \n",
    "    if not filtered_performance:\n",
    "        return []\n",
    "    \n",
    "    # 主要指标权重\n",
    "    weights = {\n",
    "        'WMAPE (%)': 0.4,    # 业务最关心的指标\n",
    "        'MAE': 0.3,          # 直观的误差指标\n",
    "        'R²': 0.3            # 拟合度指标\n",
    "    }\n",
    "    \n",
    "    # 计算综合排名\n",
    "    models = list(filtered_performance.keys())\n",
    "    rankings = {}\n",
    "    \n",
    "    for metric, weight in weights.items():\n",
    "        if metric == 'R²':\n",
    "            # R²越大越好\n",
    "            sorted_models = sorted(models, \n",
    "                                 key=lambda x: filtered_performance[x].get(metric, -1), \n",
    "                                 reverse=True)\n",
    "        else:\n",
    "            # 其他指标越小越好\n",
    "            sorted_models = sorted(models, \n",
    "                                 key=lambda x: filtered_performance[x].get(metric, float('inf')))\n",
    "        \n",
    "        # 分配排名分数（第一名得分最高）\n",
    "        for i, model in enumerate(sorted_models):\n",
    "            if model not in rankings:\n",
    "                rankings[model] = 0\n",
    "            rankings[model] += weight * (len(models) - i)\n",
    "    \n",
    "    # 按综合得分排序\n",
    "    final_ranking = sorted(rankings.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\n基于WMAPE(40%)+MAE(30%)+R²(30%)的综合排名:\")\n",
    "    for i, (model, score) in enumerate(final_ranking, 1):\n",
    "        wmape = filtered_performance[model].get('WMAPE (%)', 'N/A')\n",
    "        mae = filtered_performance[model].get('MAE', 'N/A')\n",
    "        r2 = filtered_performance[model].get('R²', 'N/A')\n",
    "        if isinstance(wmape, (int, float)):\n",
    "            wmape = f\"{wmape:.2f}%\"\n",
    "        if isinstance(mae, (int, float)):\n",
    "            mae = f\"{mae:.0f}\"\n",
    "        if isinstance(r2, (int, float)):\n",
    "            r2 = f\"{r2:.4f}\"\n",
    "        print(f\"  {i}. {model}: {score:.2f}分 (WMAPE: {wmape}, MAE: {mae}, R²: {r2})\")\n",
    "    \n",
    "    return final_ranking\n",
    "\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']\n",
    "\n",
    "def create_improved_models():\n",
    "    \"\"\"创建改进的默认模型配置（用于非搜索模式）\"\"\"\n",
    "    \n",
    "    models_config = {\n",
    "        \"RandomForest\": {\n",
    "            'model': RandomForestRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=12,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                max_features='sqrt',\n",
    "                bootstrap=True,\n",
    "                n_jobs=-1,\n",
    "                random_state=0\n",
    "            ),\n",
    "            'type': 'tree'\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            'model': XGBRegressor(\n",
    "                objective='reg:squarederror',\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.02,\n",
    "                max_depth=6,\n",
    "                min_child_weight=1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=0,\n",
    "                n_jobs=-1,\n",
    "                verbosity=0\n",
    "            ),\n",
    "            'type': 'tree'\n",
    "        },\n",
    "        \"LightGBM\": {\n",
    "            'model': lgb.LGBMRegressor(\n",
    "                objective='regression',\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.02,\n",
    "                max_depth=6,\n",
    "                num_leaves=31,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbose=-1\n",
    "            ),\n",
    "            'type': 'tree'\n",
    "        },\n",
    "        \"QRF\": {\n",
    "            'model': RandomForestQuantileRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=12,\n",
    "                min_samples_leaf=2,\n",
    "                max_features='sqrt',\n",
    "                n_jobs=-1,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'type': 'tree'\n",
    "        },\n",
    "        \"SVR\": {\n",
    "            'model': SVR(kernel='rbf', C=1000, gamma='auto', epsilon=0.01),\n",
    "            'type': 'svm'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return models_config\n",
    "\n",
    "def create_unified_deep_models(input_shape):\n",
    "    \"\"\"创建统一的深度学习模型（仅LSTM）\"\"\"\n",
    "    \n",
    "    def create_lstm_model(input_shape):\n",
    "        model = Sequential([\n",
    "            LSTM(32, return_sequences=True, input_shape=input_shape),\n",
    "            Dropout(0.2),\n",
    "            LSTM(16, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(8, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    \n",
    "    deep_models = {\n",
    "        \"LSTM\": create_lstm_model(input_shape)\n",
    "    }\n",
    "    \n",
    "    return deep_models\n",
    "\n",
    "def get_hyperparameter_search_config():\n",
    "    \"\"\"获取超参数搜索配置\"\"\"\n",
    "    \n",
    "    models_config = {\n",
    "        \"RandomForest\": {\n",
    "            'base_model': RandomForestRegressor(n_jobs=-1, random_state=0),\n",
    "            'param_grid': {\n",
    "                'n_estimators': [200, 300, 500],\n",
    "                'max_depth': [10, 12, 15],\n",
    "                'min_samples_split': [3, 5, 8],\n",
    "                'min_samples_leaf': [2, 4]\n",
    "            },\n",
    "            'param_distributions': {\n",
    "                'n_estimators': randint(200, 600),\n",
    "                'max_depth': randint(8, 16),\n",
    "                'min_samples_split': randint(2, 10),\n",
    "                'min_samples_leaf': randint(1, 5)\n",
    "            },\n",
    "            'type': 'tree',\n",
    "            'n_iter': 20\n",
    "        },\n",
    "        \n",
    "        \"XGBoost\": {\n",
    "            'base_model': XGBRegressor(\n",
    "                objective='reg:squarederror',\n",
    "                random_state=0,\n",
    "                n_jobs=-1,\n",
    "                verbosity=0\n",
    "            ),\n",
    "            'param_grid': {\n",
    "                'n_estimators': [400, 600, 800],\n",
    "                'learning_rate': [0.01, 0.02, 0.05, 0.08],\n",
    "                'max_depth': [5, 6, 7, 8],\n",
    "                'min_child_weight': [1, 3, 5],\n",
    "                'subsample': [0.7, 0.8, 0.9],\n",
    "                'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "            },\n",
    "            'param_distributions': {\n",
    "                'n_estimators': randint(300, 1000),\n",
    "                'learning_rate': uniform(0.005, 0.095),\n",
    "                'max_depth': randint(4, 12),\n",
    "                'min_child_weight': randint(1, 8),\n",
    "                'subsample': uniform(0.65, 0.3),\n",
    "                'colsample_bytree': uniform(0.65, 0.3),\n",
    "                'gamma': uniform(0, 0.5),\n",
    "                'reg_alpha': uniform(0, 0.2),\n",
    "                'reg_lambda': uniform(0.5, 2.5)\n",
    "            },\n",
    "            'type': 'tree',\n",
    "            'n_iter': 30\n",
    "        },\n",
    "        \n",
    "        \"LightGBM\": {\n",
    "            'base_model': lgb.LGBMRegressor(\n",
    "                objective='regression',\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbose=-1\n",
    "            ),\n",
    "            'param_grid': {\n",
    "                'n_estimators': [400, 600, 800],\n",
    "                'learning_rate': [0.01, 0.02, 0.05],\n",
    "                'max_depth': [5, 6, 8],\n",
    "                'num_leaves': [31, 50, 70],\n",
    "                'subsample': [0.7, 0.8, 0.9],\n",
    "                'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "            },\n",
    "            'param_distributions': {\n",
    "                'n_estimators': randint(300, 1000),\n",
    "                'learning_rate': uniform(0.005, 0.095),\n",
    "                'max_depth': randint(4, 12),\n",
    "                'num_leaves': randint(20, 80),\n",
    "                'subsample': uniform(0.65, 0.3),\n",
    "                'colsample_bytree': uniform(0.65, 0.3),\n",
    "                'reg_alpha': uniform(0, 0.2),\n",
    "                'reg_lambda': uniform(0.5, 2.5)\n",
    "            },\n",
    "            'type': 'tree',\n",
    "            'n_iter': 30\n",
    "        },\n",
    "        \n",
    "        \"QRF\": {\n",
    "            'base_model': RandomForestQuantileRegressor(n_jobs=-1, random_state=42),\n",
    "            'param_grid': {\n",
    "                'n_estimators': [200, 300],\n",
    "                'max_depth': [10, 12],\n",
    "                'min_samples_leaf': [2, 4]\n",
    "            },\n",
    "            'param_distributions': {\n",
    "                'n_estimators': randint(200, 500),\n",
    "                'max_depth': randint(8, 15),\n",
    "                'min_samples_leaf': randint(1, 5)\n",
    "            },\n",
    "            'type': 'tree',\n",
    "            'n_iter': 15\n",
    "        },\n",
    "        \n",
    "        \"SVR\": {\n",
    "            'base_model': SVR(),\n",
    "            'param_grid': {\n",
    "                'kernel': ['rbf'],\n",
    "                'C': [100, 2000],  \n",
    "                'gamma': ['scale', 0.04], \n",
    "                'epsilon': [0.05, 0.1]  \n",
    "            },\n",
    "            'type': 'svm',\n",
    "            'use_grid_only': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return models_config\n",
    "\n",
    "def unified_deep_learning_training_with_search(train_cleaned, test_cleaned, working_features, y_train, y_test, \n",
    "                                  model_performance, daily_predictors, sequence_predictors, product_name,\n",
    "                                  enable_search=True):\n",
    "    \"\"\"统一的深度学习模型训练（仅LSTM，支持超参数搜索）\"\"\"\n",
    "    \n",
    "    if not KERAS_AVAILABLE:\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n训练深度学习模型{'（启用搜索）' if enable_search else ''}...\")\n",
    "    \n",
    "    # 为深度学习模型选择特征\n",
    "    deep_features = select_model_specific_features(\n",
    "        working_features,\n",
    "        train_cleaned[working_features],\n",
    "        y_train,\n",
    "        'deep',\n",
    "        'DeepLearning'\n",
    "    )\n",
    "    \n",
    "    print(f\"深度学习模型使用 {len(deep_features)} 个特征\")\n",
    "    \n",
    "    X_train_deep_raw = train_cleaned[deep_features]\n",
    "    X_test_deep_raw = test_cleaned[deep_features]\n",
    "    \n",
    "    # 使用统一的预处理\n",
    "    X_train_deep, X_test_deep, y_train_deep, scaler_X_deep, scaler_y_deep = get_preprocessed_data(\n",
    "        X_train_deep_raw, X_test_deep_raw, y_train, 'deep'\n",
    "    )\n",
    "    \n",
    "    # 使用统一的序列长度\n",
    "    sequence_length = 7\n",
    "    max_possible_length = len(X_train_deep) // 8\n",
    "    sequence_length = min(sequence_length, max(3, max_possible_length))\n",
    "    \n",
    "    print(f\"使用序列长度: {sequence_length}\")\n",
    "    \n",
    "    if len(X_train_deep) <= sequence_length * 3:\n",
    "        print(\"训练数据不足，跳过深度学习模型\")\n",
    "        return\n",
    "    \n",
    "    # 准备序列训练数据\n",
    "    def prepare_sequence_data(X, y, seq_len):\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(seq_len, len(X)):\n",
    "            X_seq.append(X[i-seq_len:i])\n",
    "            y_seq.append(y[i])\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "    \n",
    "    X_train_seq, y_train_seq = prepare_sequence_data(X_train_deep, y_train_deep, sequence_length)\n",
    "    \n",
    "    if len(X_train_seq) <= 15:\n",
    "        print(\"序列数据不足，跳过深度学习模型\")\n",
    "        return\n",
    "    \n",
    "    # 训练验证集分割\n",
    "    val_size = max(5, len(X_train_seq) // 5)\n",
    "    X_train_model = X_train_seq[:-val_size]\n",
    "    y_train_model = y_train_seq[:-val_size]\n",
    "    X_val_model = X_train_seq[-val_size:]\n",
    "    y_val_model = y_train_seq[-val_size:]\n",
    "    \n",
    "    # 定义超参数搜索空间\n",
    "    if enable_search:\n",
    "        lstm_configs = [\n",
    "            {'units1': 32, 'units2': 16, 'dropout': 0.2, 'lr': 0.01, 'dense': 8},\n",
    "            {'units1': 24, 'units2': 12, 'dropout': 0.25, 'lr': 0.005, 'dense': 6},\n",
    "            {'units1': 48, 'units2': 24, 'dropout': 0.2, 'lr': 0.01, 'dense': 12},\n",
    "            {'units1': 32, 'units2': 16, 'dropout': 0.3, 'lr': 0.005, 'dense': 8},\n",
    "            {'units1': 40, 'units2': 20, 'dropout': 0.2, 'lr': 0.008, 'dense': 10},\n",
    "        ]\n",
    "    else:\n",
    "        # 默认配置\n",
    "        lstm_configs = [\n",
    "            {'units1': 32, 'units2': 16, 'dropout': 0.2, 'lr': 0.01, 'dense': 8}\n",
    "        ]\n",
    "    \n",
    "    input_shape = (sequence_length, X_train_deep.shape[1])\n",
    "    \n",
    "    # 训练LSTM\n",
    "    print(f\"\\n训练 LSTM 模型{'（搜索中）' if enable_search else ''}...\")\n",
    "    best_lstm_model = None\n",
    "    best_lstm_score = float('inf')\n",
    "    best_lstm_config = None\n",
    "    \n",
    "    for i, config in enumerate(lstm_configs):\n",
    "        if enable_search:\n",
    "            print(f\"  尝试配置 {i+1}/{len(lstm_configs)}: {config}\")\n",
    "        \n",
    "        try:\n",
    "            model = Sequential([\n",
    "                LSTM(config['units1'], return_sequences=True, input_shape=input_shape),\n",
    "                Dropout(config['dropout']),\n",
    "                LSTM(config['units2'], return_sequences=False),\n",
    "                Dropout(config['dropout']),\n",
    "                Dense(config['dense'], activation='relu'),\n",
    "                Dense(1)\n",
    "            ])\n",
    "            model.compile(optimizer=Adam(learning_rate=config['lr']), loss='mse', metrics=['mae'])\n",
    "            \n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "            ]\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train_model, y_train_model,\n",
    "                validation_data=(X_val_model, y_val_model),\n",
    "                epochs=50,\n",
    "                batch_size=min(16, max(4, len(X_train_seq) // 8)),\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            val_loss = min(history.history['val_loss'])\n",
    "            \n",
    "            if enable_search:\n",
    "                print(f\"    验证损失: {val_loss:.4f}\")\n",
    "            \n",
    "            if val_loss < best_lstm_score:\n",
    "                best_lstm_score = val_loss\n",
    "                best_lstm_model = model\n",
    "                best_lstm_config = config\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  配置 {i+1} 训练失败: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if best_lstm_model is not None:\n",
    "        if enable_search:\n",
    "            print(f\"  LSTM最佳配置: {best_lstm_config}, 验证损失: {best_lstm_score:.4f}\")\n",
    "        \n",
    "        predictor = SequencePredictor(\n",
    "            best_lstm_model, scaler_X_deep, scaler_y_deep, \n",
    "            deep_features, sequence_length, 'LSTM'\n",
    "        )\n",
    "        sequence_predictors['LSTM'] = predictor\n",
    "        \n",
    "        print(f\"  执行 LSTM 逐日序列预测...\")\n",
    "        y_pred = predictor.predict_test_period(test_cleaned, train_cleaned)\n",
    "        \n",
    "        if len(y_pred) > 0:\n",
    "            min_len = min(len(y_test), len(y_pred))\n",
    "            y_test_adj = y_test.iloc[:min_len].values\n",
    "            y_pred_adj = y_pred[:min_len]\n",
    "            \n",
    "            if min_len > 0:\n",
    "                performance = calculate_streamlined_metrics(y_test_adj, y_pred_adj)\n",
    "                model_performance['LSTM'] = performance\n",
    "                print(f\"  LSTM 逐日预测性能: MAE={performance['MAE']:.2f}, WMAPE={performance['WMAPE (%)']:.2f}%, R²={performance['R²']:.4f}\")\n",
    "                daily_predictors['LSTM'] = predictor\n",
    "\n",
    "def train_models_with_hyperparameter_search(train_cleaned, working_features, y_train, product_name,\n",
    "                                           max_time_per_model=180, search_strategy=\"smart\"):\n",
    "    \"\"\"使用超参数搜索训练模型\"\"\"\n",
    "    \n",
    "    print(f\"开始超参数搜索训练（策略: {search_strategy}）...\")\n",
    "    \n",
    "    # 获取搜索配置\n",
    "    models_config = get_hyperparameter_search_config()\n",
    "    \n",
    "    trained_models = {}\n",
    "    best_params_dict = {}\n",
    "    \n",
    "    for name, config in models_config.items():\n",
    "        print(f\"\\n训练 {name} 模型...\")\n",
    "        \n",
    "        try:\n",
    "            model_type = config['type']\n",
    "            \n",
    "            # 特征选择\n",
    "            model_features = select_model_specific_features(\n",
    "                working_features, \n",
    "                train_cleaned[working_features], \n",
    "                y_train, \n",
    "                model_type, \n",
    "                name\n",
    "            )\n",
    "            \n",
    "            print(f\"  {name} 使用 {len(model_features)} 个特征\")\n",
    "            \n",
    "            X_train = train_cleaned[model_features]\n",
    "            X_train_processed, _, y_train_processed, scaler_X, scaler_y = get_preprocessed_data(\n",
    "                X_train, X_train, y_train, model_type\n",
    "            )\n",
    "            \n",
    "            # 执行搜索\n",
    "            best_model, best_params, best_score, search_time = smart_hyperparameter_search(\n",
    "                config, X_train_processed, y_train_processed, name\n",
    "            )\n",
    "            \n",
    "            # 存储结果\n",
    "            trained_models[name] = {\n",
    "                'model': best_model,\n",
    "                'features': model_features,\n",
    "                'scaler_X': scaler_X,\n",
    "                'scaler_y': scaler_y,\n",
    "                'type': model_type,\n",
    "                'best_score': best_score,\n",
    "                'search_time': search_time\n",
    "            }\n",
    "            \n",
    "            best_params_dict[name] = best_params\n",
    "            \n",
    "            print(f\"  {name} 搜索完成，用时: {search_time:.1f}秒\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  训练 {name} 模型时出错: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return trained_models, best_params_dict\n",
    "\n",
    "def train_models_with_improved_defaults(train_cleaned, working_features, y_train, product_name):\n",
    "    \"\"\"使用改进默认参数的快速训练\"\"\"\n",
    "    \n",
    "    # 使用原来优化过的create_improved_models函数\n",
    "    models_config = create_improved_models()\n",
    "    \n",
    "    trained_models = {}\n",
    "    best_params_dict = {}\n",
    "    \n",
    "    for name, config in models_config.items():\n",
    "        print(f\"\\n训练 {name} 模型（改进默认参数）...\")\n",
    "        \n",
    "        try:\n",
    "            model = config['model']\n",
    "            model_type = config['type']\n",
    "            \n",
    "            model_features = select_model_specific_features(\n",
    "                working_features, \n",
    "                train_cleaned[working_features], \n",
    "                y_train, \n",
    "                model_type, \n",
    "                name\n",
    "            )\n",
    "            \n",
    "            X_train = train_cleaned[model_features]\n",
    "            X_train_processed, _, y_train_processed, scaler_X, scaler_y = get_preprocessed_data(\n",
    "                X_train, X_train, y_train, model_type\n",
    "            )\n",
    "            \n",
    "            # 对SVR做网格搜索\n",
    "            if name == 'SVR':\n",
    "                print(f\"  执行 {name} 网格搜索...\")\n",
    "                \n",
    "                from sklearn.model_selection import GridSearchCV\n",
    "                \n",
    "                param_grid = {\n",
    "                    'C': [100, 1000, 5000],\n",
    "                    'gamma': ['scale', 'auto', 0.01],\n",
    "                    'epsilon': [0.01, 0.05, 0.1]\n",
    "                }\n",
    "                \n",
    "                tscv = TimeSeriesSplit(n_splits=3)\n",
    "                grid_search = GridSearchCV(\n",
    "                    model, param_grid, \n",
    "                    scoring='neg_mean_absolute_error',\n",
    "                    cv=tscv, n_jobs=1, verbose=0\n",
    "                )\n",
    "                grid_search.fit(X_train_processed, y_train_processed)\n",
    "                model = grid_search.best_estimator_\n",
    "                \n",
    "                print(f\"  最佳参数: {grid_search.best_params_}\")\n",
    "                print(f\"  最佳得分: {grid_search.best_score_:.2f}\")\n",
    "            else:\n",
    "                # 其他模型正常训练\n",
    "                if name in ['XGBoost', 'LightGBM']:\n",
    "                    eval_set = [(X_train_processed, y_train_processed)]\n",
    "                    if name == 'XGBoost':\n",
    "                        model.fit(X_train_processed, y_train_processed, \n",
    "                                eval_set=eval_set, verbose=False)\n",
    "                    else:  # LightGBM\n",
    "                        model.fit(X_train_processed, y_train_processed, \n",
    "                                eval_set=eval_set, \n",
    "                                callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)])\n",
    "                else:\n",
    "                    model.fit(X_train_processed, y_train_processed)\n",
    "            \n",
    "            trained_models[name] = {\n",
    "                'model': model,\n",
    "                'features': model_features,\n",
    "                'scaler_X': scaler_X,\n",
    "                'scaler_y': scaler_y,\n",
    "                'type': model_type,\n",
    "                'best_score': None,\n",
    "                'search_time': 0\n",
    "            }\n",
    "            \n",
    "            # 获取当前参数作为\"最佳参数\"\n",
    "            best_params_dict[name] = model.get_params()\n",
    "            \n",
    "            print(f\"  {name} 训练完成\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  训练 {name} 模型时出错: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return trained_models, best_params_dict\n",
    "\n",
    "def smart_hyperparameter_search(config, X_train, y_train, model_name, data_threshold=200):\n",
    "    \"\"\"智能超参数搜索：根据数据量和模型类型选择搜索策略\"\"\"\n",
    "    \n",
    "    data_size = len(X_train)\n",
    "    print(f\"    数据量: {data_size}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # 动态调整CV折数\n",
    "        cv_folds = min(3, max(2, data_size // 20))\n",
    "        tscv = TimeSeriesSplit(n_splits=cv_folds)\n",
    "        \n",
    "        # SVR强制使用网格搜索\n",
    "        if config.get('use_grid_only', False):\n",
    "            print(f\"    使用网格搜索（强制模式）\")\n",
    "            \n",
    "            search = GridSearchCV(\n",
    "                estimator=config['base_model'],\n",
    "                param_grid=config['param_grid'],\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                cv=tscv,\n",
    "                n_jobs=1,\n",
    "                verbose=0\n",
    "            )\n",
    "        elif data_size < data_threshold:\n",
    "            # 小数据集：使用网格搜索\n",
    "            print(f\"    使用网格搜索（数据量 < {data_threshold}）\")\n",
    "            \n",
    "            search = GridSearchCV(\n",
    "                estimator=config['base_model'],\n",
    "                param_grid=config['param_grid'],\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                cv=tscv,\n",
    "                n_jobs=1,\n",
    "                verbose=0,\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            # 大数据集：使用随机搜索\n",
    "            print(f\"    使用随机搜索（数据量 >= {data_threshold}）\")\n",
    "            \n",
    "            search = RandomizedSearchCV(\n",
    "                estimator=config['base_model'],\n",
    "                param_distributions=config['param_distributions'],\n",
    "                n_iter=config['n_iter'],\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                cv=tscv,\n",
    "                n_jobs=1,\n",
    "                verbose=0,\n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"    搜索完成，用时: {search_time:.1f}秒\")\n",
    "        print(f\"    最佳参数: {search.best_params_}\")\n",
    "        print(f\"    最佳得分: {search.best_score_:.4f}\")\n",
    "        \n",
    "        return search.best_estimator_, search.best_params_, search.best_score_, search_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    智能搜索失败: {str(e)}\")\n",
    "        return fallback_to_defaults(config, X_train, y_train, model_name)\n",
    "\n",
    "def grid_search_with_timeout(config, X_train, y_train, model_name, max_time=180):\n",
    "    \"\"\"带超时的网格搜索\"\"\"\n",
    "    \n",
    "    print(f\"    执行网格搜索（最大时间: {max_time}秒）\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        \n",
    "        search = GridSearchCV(\n",
    "            estimator=config['base_model'],\n",
    "            param_grid=config['param_grid'],\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            cv=tscv,\n",
    "            n_jobs=1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        return search.best_estimator_, search.best_params_, search.best_score_, search_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    网格搜索失败: {str(e)}\")\n",
    "        return fallback_to_defaults(config, X_train, y_train, model_name)\n",
    "\n",
    "def random_search_with_timeout(config, X_train, y_train, model_name, max_time=180):\n",
    "    \"\"\"带超时的随机搜索\"\"\"\n",
    "    \n",
    "    print(f\"    执行随机搜索（最大时间: {max_time}秒）\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        \n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=config['base_model'],\n",
    "            param_distributions=config['param_distributions'],\n",
    "            n_iter=config['n_iter'],\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            cv=tscv,\n",
    "            n_jobs=1,\n",
    "            verbose=0,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        return search.best_estimator_, search.best_params_, search.best_score_, search_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    随机搜索失败: {str(e)}\")\n",
    "        return fallback_to_defaults(config, X_train, y_train, model_name)\n",
    "\n",
    "def fallback_to_defaults(config, X_train, y_train, model_name):\n",
    "    \"\"\"fallback到默认参数\"\"\"\n",
    "    \n",
    "    print(f\"    使用默认参数...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = config['base_model']\n",
    "    \n",
    "    # 使用改进的默认参数\n",
    "    if model_name == 'SVR':\n",
    "        model.set_params(kernel='rbf', C=1000, gamma='auto', epsilon=0.01)\n",
    "    elif model_name == 'XGBoost':\n",
    "        model.set_params(n_estimators=500, learning_rate=0.02, max_depth=6)\n",
    "    elif model_name == 'ANN':\n",
    "        model.set_params(hidden_layer_sizes=(128, 64, 32), alpha=0.001, learning_rate_init=0.001)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    return model, model.get_params(), None, search_time\n",
    "\n",
    "def save_best_parameters(best_params_dict, product_name, output_dir):\n",
    "    \"\"\"保存最佳参数到文件\"\"\"\n",
    "    \n",
    "    if not best_params_dict:\n",
    "        return\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    params_data = []\n",
    "    for model_name, params in best_params_dict.items():\n",
    "        for param_name, param_value in params.items():\n",
    "            params_data.append({\n",
    "                'model': model_name,\n",
    "                'parameter': param_name,\n",
    "                'best_value': str(param_value)\n",
    "            })\n",
    "    \n",
    "    if params_data:\n",
    "        params_df = pd.DataFrame(params_data)\n",
    "        params_path = os.path.join(output_dir, f\"{product_name}_最佳参数.xlsx\")\n",
    "        params_df.to_excel(params_path, index=False)\n",
    "        print(f\"最佳参数已保存至: {params_path}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 主要预测函数 - 逐日预测版本\n",
    "# =============================================================================\n",
    "def daily_prediction_and_comparison_with_search(merged_df, selected_features, product_name, output_dir,\n",
    "                                               use_search=True, max_time_per_model=180, search_strategy=\"smart\"):\n",
    "    \"\"\"执行逐日预测并与13周移动平均法比较（集成超参数搜索）\"\"\"\n",
    "    print(f\"\\n开始为{product_name}执行逐日预测与比较...\")\n",
    "    \n",
    "    # 确保日期列是datetime类型并按日期排序\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "    merged_df = merged_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # 划分训练集和测试集（2024年作为测试集）\n",
    "    train = merged_df[merged_df['date'] < '2024-01-01']\n",
    "    test = merged_df[merged_df['date'] >= '2024-01-01']\n",
    "    \n",
    "    if test.empty:\n",
    "        print(f\"警告: {product_name}没有2024年数据，无法进行预测比较\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"训练集大小: {train.shape[0]}, 测试集大小: {test.shape[0]}\")\n",
    "    \n",
    "    # 分别处理训练集和测试集，避免数据泄漏\n",
    "    train_cleaned, test_cleaned = leak_free_fix_data_issues(train, test)\n",
    "    print(f\"清理后训练集大小: {train_cleaned.shape[0]}, 测试集大小: {test_cleaned.shape[0]}\")\n",
    "    \n",
    "    # 加载已选择的特征（已经过滤了不可用特征）\n",
    "    working_features = [f for f in selected_features if f in train_cleaned.columns]\n",
    "    print(f\"使用特征数量: {len(working_features)}\")\n",
    "    \n",
    "    if len(working_features) == 0:\n",
    "        print(f\"警告: {product_name}没有可用特征，无法训练模型\")\n",
    "        return None\n",
    "    \n",
    "    # === 修改：使用新的训练函数 ===\n",
    "    y_train = train_cleaned['demand']\n",
    "    y_test = test_cleaned['demand']\n",
    "    \n",
    "    # 根据配置选择训练方式\n",
    "    if use_search:\n",
    "        print(f\"\\n使用超参数搜索训练模型（策略: {search_strategy}）...\")\n",
    "        trained_models, best_params_dict = train_models_with_hyperparameter_search(\n",
    "            train_cleaned, working_features, y_train, product_name,\n",
    "            max_time_per_model=max_time_per_model,\n",
    "            search_strategy=search_strategy\n",
    "        )\n",
    "        \n",
    "        # 保存最佳参数\n",
    "        save_best_parameters(best_params_dict, product_name, output_dir)\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n使用改进的默认参数训练模型...\")\n",
    "        trained_models, best_params_dict = train_models_with_improved_defaults(\n",
    "            train_cleaned, working_features, y_train, product_name\n",
    "        )\n",
    "    \n",
    "    # 创建预测器并执行预测（原逻辑保持不变）\n",
    "    daily_predictors = {}\n",
    "    model_performance = {}\n",
    "    sequence_predictors = {}\n",
    "    \n",
    "    for name, model_info in trained_models.items():\n",
    "        print(f\"\\n创建 {name} 预测器...\")\n",
    "        \n",
    "        try:\n",
    "            # 创建逐日预测器\n",
    "            predictor = DailyPredictor(\n",
    "                model_info['model'], \n",
    "                model_info['scaler_X'], \n",
    "                model_info['scaler_y'], \n",
    "                model_info['features'], \n",
    "                model_info['type'], \n",
    "                name\n",
    "            )\n",
    "            daily_predictors[name] = predictor\n",
    "            \n",
    "            # 执行逐日预测\n",
    "            print(f\"  执行 {name} 逐日预测...\")\n",
    "            y_pred = predictor.predict_test_period(test_cleaned, train_cleaned)\n",
    "            \n",
    "            # 计算性能指标\n",
    "            performance = calculate_streamlined_metrics(y_test, y_pred)\n",
    "            model_performance[name] = performance\n",
    "            \n",
    "            # 添加搜索信息\n",
    "            if 'best_score' in model_info:\n",
    "                performance['best_cv_score'] = model_info['best_score']\n",
    "            if 'search_time' in model_info:\n",
    "                performance['search_time'] = model_info['search_time']\n",
    "            \n",
    "            print(f\"  {name} 逐日预测性能: MAE={performance['MAE']:.2f}, WMAPE={performance['WMAPE (%)']:.2f}%, R²={performance['R²']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  创建 {name} 预测器时出错: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # 使用统一的深度学习训练（启用搜索）\n",
    "    unified_deep_learning_training_with_search(train_cleaned, test_cleaned, working_features, y_train, y_test, \n",
    "                              model_performance, daily_predictors, sequence_predictors, product_name,\n",
    "                              enable_search=use_search)  # 根据主配置决定是否搜索\n",
    "    \n",
    "    # 添加基线模型：LinearRegression（保持原逻辑）\n",
    "    print(f\"\\n训练基线模型...\")\n",
    "    try:\n",
    "        lr_features = select_model_specific_features(\n",
    "            working_features, \n",
    "            train_cleaned[working_features], \n",
    "            y_train, \n",
    "            'linear', \n",
    "            'LinearRegression'\n",
    "        )\n",
    "        \n",
    "        X_train_lr = train_cleaned[lr_features]\n",
    "        X_test_lr = test_cleaned[lr_features]\n",
    "        \n",
    "        X_train_lr_processed, X_test_lr_processed, y_train_lr_processed, scaler_X_lr, scaler_y_lr = get_preprocessed_data(\n",
    "            X_train_lr, X_test_lr, y_train, 'linear'\n",
    "        )\n",
    "        \n",
    "        lr_model = LinearRegression()\n",
    "        lr_model.fit(X_train_lr_processed, y_train_lr_processed)\n",
    "        \n",
    "        lr_predictor = DailyPredictor(lr_model, scaler_X_lr, scaler_y_lr, lr_features, 'linear', 'LinearRegression')\n",
    "        daily_predictors['LinearRegression'] = lr_predictor\n",
    "        \n",
    "        y_pred_lr = lr_predictor.predict_test_period(test_cleaned, train_cleaned)\n",
    "        performance_lr = calculate_streamlined_metrics(y_test, y_pred_lr)\n",
    "        model_performance['LinearRegression'] = performance_lr\n",
    "        \n",
    "        print(f\"  LinearRegression 基线性能: MAE={performance_lr['MAE']:.2f}, WMAPE={performance_lr['WMAPE (%)']:.2f}%, R²={performance_lr['R²']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  训练LinearRegression基线时出错: {str(e)}\")\n",
    "    \n",
    "    # 收集所有预测结果\n",
    "    print(f\"\\n收集所有模型的逐日预测结果...\")\n",
    "    model_predictions = {}\n",
    "    \n",
    "    for name, predictor in daily_predictors.items():\n",
    "        try:\n",
    "            if hasattr(predictor, 'predict_test_period'):\n",
    "                if name in sequence_predictors:\n",
    "                    # 序列预测器\n",
    "                    y_pred = predictor.predict_test_period(test_cleaned, train_cleaned)\n",
    "                else:\n",
    "                    # 普通预测器（使用A/B填充）\n",
    "                    y_pred = predictor.predict_test_period(test_cleaned, train_cleaned)\n",
    "                \n",
    "                # 确保预测长度与测试集一致\n",
    "                if len(y_pred) != len(y_test):\n",
    "                    min_len = min(len(y_pred), len(y_test))\n",
    "                    y_pred = y_pred[:min_len]\n",
    "                \n",
    "                model_predictions[name] = y_pred\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  获取 {name} 预测结果时出错: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # 创建改进的集成预测（排除基线模型）\n",
    "    if len(model_predictions) > 2:  # 确保有足够的模型进行集成\n",
    "        print(f\"\\n创建改进的动态加权集成预测...\")\n",
    "        try:\n",
    "            ensemble_result = create_performance_based_ensemble(\n",
    "                model_predictions, model_performance, exclude_baselines=True\n",
    "            )\n",
    "            \n",
    "            if ensemble_result[0] is not None:\n",
    "                ensemble_pred, ensemble_weights = ensemble_result\n",
    "                model_predictions[\"Improved_Ensemble\"] = ensemble_pred\n",
    "                \n",
    "                # 计算集成模型性能\n",
    "                y_test_adj = y_test.iloc[:len(ensemble_pred)]\n",
    "                if len(y_test_adj) == len(ensemble_pred):\n",
    "                    performance = calculate_streamlined_metrics(y_test_adj, ensemble_pred)\n",
    "                    model_performance[\"Improved_Ensemble\"] = performance\n",
    "                    print(f\"Improved_Ensemble 预测性能: MAE={performance['MAE']:.2f}, WMAPE={performance['WMAPE (%)']:.2f}%, R²={performance['R²']:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  创建改进集成模型时出错: {str(e)}\")\n",
    "    \n",
    "    # 13周移动平均法预测（基准方法）\n",
    "    print(f\"\\n执行13周移动平均法预测...\")\n",
    "    weekly_demand = merged_df.resample('W-SUN', on='date')['demand'].sum().reset_index()\n",
    "    weekly_demand.columns = ['week_start', 'actual_weekly_demand']\n",
    "    weekly_demand['ma_13'] = weekly_demand['actual_weekly_demand'].rolling(window=13, min_periods=1).mean()\n",
    "    weekly_demand['ma_forecast'] = weekly_demand['ma_13'].shift(1)\n",
    "    \n",
    "    weekly_demand_2024 = weekly_demand[weekly_demand['week_start'] >= '2024-01-01']\n",
    "    \n",
    "    # 将逐日预测结果按周汇总\n",
    "    test_results = test_cleaned[['date', 'demand']].copy()\n",
    "    \n",
    "    # 处理不同长度的预测结果\n",
    "    for name, pred in model_predictions.items():\n",
    "        pred_len = len(pred)\n",
    "        test_len = len(test_results)\n",
    "        \n",
    "        if pred_len == test_len:\n",
    "            test_results[f'{name}_prediction'] = pred\n",
    "        elif pred_len < test_len:\n",
    "            # 预测结果较短，用最后一个值填充\n",
    "            pred_extended = np.concatenate([pred, np.full(test_len - pred_len, pred[-1] if pred_len > 0 else 0)])\n",
    "            test_results[f'{name}_prediction'] = pred_extended\n",
    "        else:\n",
    "            # 预测结果较长，截断\n",
    "            test_results[f'{name}_prediction'] = pred[:test_len]\n",
    "    \n",
    "    # 按周汇总\n",
    "    agg_dict = {'demand': 'sum'}\n",
    "    for name in model_predictions.keys():\n",
    "        pred_col = f'{name}_prediction'\n",
    "        if pred_col in test_results.columns:\n",
    "            agg_dict[pred_col] = 'sum'\n",
    "    \n",
    "    weekly_model_results = test_results.resample('W-SUN', on='date').agg(agg_dict).reset_index()\n",
    "    \n",
    "    # 重命名列\n",
    "    new_columns = ['week_start', 'actual_weekly_demand']\n",
    "    for name in model_predictions.keys():\n",
    "        pred_col = f'{name}_prediction'\n",
    "        if pred_col in weekly_model_results.columns:\n",
    "            new_columns.append(f'{name}_forecast')\n",
    "    \n",
    "    weekly_model_results.columns = new_columns\n",
    "    \n",
    "    # 合并结果\n",
    "    comparison_df = pd.merge(\n",
    "        weekly_model_results, \n",
    "        weekly_demand_2024[['week_start', 'ma_forecast']], \n",
    "        on='week_start', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 计算每周性能指标\n",
    "    weekly_metrics = {}\n",
    "    \n",
    "    for col in comparison_df.columns:\n",
    "        if col.endswith('_forecast'):\n",
    "            if col == 'ma_forecast':\n",
    "                model_name = '13周移动平均'\n",
    "            else:\n",
    "                model_name = col.replace('_forecast', '')\n",
    "            \n",
    "            if not comparison_df[col].isna().all():\n",
    "                try:\n",
    "                    metrics = calculate_streamlined_metrics(\n",
    "                        comparison_df['actual_weekly_demand'], \n",
    "                        comparison_df[col]\n",
    "                    )\n",
    "                    weekly_metrics[model_name] = metrics\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  计算 {model_name} 周度指标时出错: {str(e)}\")\n",
    "    \n",
    "    # 计算模型综合排名\n",
    "    ranking_result = calculate_model_ranking(weekly_metrics, exclude_baselines=True)\n",
    "    \n",
    "    # 输出结果\n",
    "    print(f\"\\n{product_name}逐日预测转周度汇总性能比较:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 定义要显示的指标\n",
    "    metrics_to_show = ['MAE', 'WMAPE (%)', 'MAPE (%)', 'RMSE', 'R²']\n",
    "    \n",
    "    for metric_name in metrics_to_show:\n",
    "        print(f\"\\n{metric_name}排名:\")\n",
    "        metric_scores = {}\n",
    "        for model, metrics in weekly_metrics.items():\n",
    "            if metric_name in metrics:\n",
    "                metric_scores[model] = metrics[metric_name]\n",
    "        \n",
    "        # 排序（R²按降序，其他按升序）\n",
    "        reverse_sort = (metric_name == 'R²')\n",
    "        sorted_scores = sorted(metric_scores.items(), key=lambda x: x[1], reverse=reverse_sort)\n",
    "        \n",
    "        for i, (model, score) in enumerate(sorted_scores, 1):\n",
    "            if metric_name == 'R²':\n",
    "                print(f\"  {i}. {model}: {score:.4f}\")\n",
    "            elif '%' in metric_name:\n",
    "                print(f\"  {i}. {model}: {score:.2f}%\")\n",
    "            else:\n",
    "                print(f\"  {i}. {model}: {score:.0f}\")\n",
    "    \n",
    "    # 可视化比较\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    # 实际需求 - 使用粗实线和特殊标记\n",
    "    plt.plot(comparison_df['week_start'], comparison_df['actual_weekly_demand'], \n",
    "            'o-', label='实际需求', linewidth=3, markersize=8, color='black')\n",
    "\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "    color_idx = 0\n",
    "\n",
    "    # 优先显示非基线模型 - 使用虚线\n",
    "    baseline_models = ['LinearRegression', '13周移动平均']\n",
    "\n",
    "    for model in model_predictions.keys():\n",
    "        if model not in baseline_models:\n",
    "            forecast_col = f'{model}_forecast'\n",
    "            if forecast_col in comparison_df.columns:\n",
    "                plt.plot(comparison_df['week_start'], comparison_df[forecast_col], \n",
    "                        '--', label=f'{model}预测', linewidth=2, \n",
    "                        color=colors[color_idx % len(colors)])\n",
    "                color_idx += 1\n",
    "\n",
    "    # 最后显示基线模型 - 使用点划线和较浅的颜色\n",
    "    baseline_linestyles = ['-.', '-.']  # 点划线\n",
    "    baseline_alphas = [0.6, 0.6]  # 半透明\n",
    "    baseline_linewidths = [2.5, 2.5]  # 稍粗一点\n",
    "\n",
    "    for idx, model in enumerate(baseline_models):\n",
    "        if model == '13周移动平均':\n",
    "            forecast_col = 'ma_forecast'\n",
    "        else:\n",
    "            forecast_col = f'{model}_forecast'\n",
    "        \n",
    "        if forecast_col in comparison_df.columns and not comparison_df[forecast_col].isna().all():\n",
    "            plt.plot(comparison_df['week_start'], comparison_df[forecast_col], \n",
    "                    baseline_linestyles[idx], \n",
    "                    label=f'{model}(基线)', \n",
    "                    linewidth=baseline_linewidths[idx], \n",
    "                    alpha=baseline_alphas[idx],\n",
    "                    color=colors[color_idx % len(colors)])\n",
    "            color_idx += 1\n",
    "\n",
    "    plt.title(f'{product_name}逐日预测汇总每周需求比较 (2024年)', fontsize=16)\n",
    "    plt.xlabel('周起始日期', fontsize=14)\n",
    "    plt.ylabel('每周需求', fontsize=14)\n",
    "    plt.legend(fontsize=10, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_path = os.path.join(output_dir, f\"{product_name}_2024年逐日预测汇总比较.png\")\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"\\n预测比较图表保存至: {plot_path}\")\n",
    "    \n",
    "    # 保存详细结果\n",
    "    result_path = os.path.join(output_dir, f\"{product_name}_2024年逐日预测汇总结果.xlsx\")\n",
    "    comparison_df.to_excel(result_path, index=False)\n",
    "    print(f\"详细结果保存至: {result_path}\")\n",
    "    \n",
    "    # 保存逐日预测结果\n",
    "    daily_result_path = os.path.join(output_dir, f\"{product_name}_2024年逐日预测详细结果.xlsx\")\n",
    "    test_results.to_excel(daily_result_path, index=False)\n",
    "    print(f\"逐日预测详细结果保存至: {daily_result_path}\")\n",
    "    \n",
    "    # ===== 15日聚合评估 =====\n",
    "    bi15d_model_results = test_results.resample('15D', on='date').agg(agg_dict).reset_index()\n",
    "    bi15d_model_results = bi15d_model_results.rename(columns={'date': 'period_start', 'demand': 'actual_period_demand'})\n",
    "    \n",
    "    # 计算15日聚合指标（无基线）\n",
    "    bi15d_metrics = {}\n",
    "    for col in bi15d_model_results.columns:\n",
    "        if col.endswith('_prediction'):\n",
    "            model_name = col.replace('_prediction', '')\n",
    "            try:\n",
    "                metrics = calculate_streamlined_metrics(bi15d_model_results['actual_period_demand'], bi15d_model_results[col])\n",
    "                bi15d_metrics[model_name] = metrics\n",
    "            except Exception as e:\n",
    "                print(f\"  计算 {model_name} 15日指标时出错: {str(e)}\")\n",
    "    \n",
    "    # 保存15日汇总\n",
    "    result_15d_path = os.path.join(output_dir, f\"{product_name}_2024年逐日预测汇总结果_15D.xlsx\")\n",
    "    bi15d_model_results.to_excel(result_15d_path, index=False)\n",
    "    print(f\"详细结果保存至(15D): {result_15d_path}\")\n",
    "    \n",
    "    # ===== 14日聚合评估 =====\n",
    "    bi14d_model_results = test_results.resample('14D', on='date').agg(agg_dict).reset_index()\n",
    "    bi14d_model_results = bi14d_model_results.rename(columns={'date': 'period_start', 'demand': 'actual_period_demand'})\n",
    "    \n",
    "    bi14d_metrics = {}\n",
    "    for col in bi14d_model_results.columns:\n",
    "        if col.endswith('_prediction'):\n",
    "            model_name = col.replace('_prediction', '')\n",
    "            try:\n",
    "                metrics = calculate_streamlined_metrics(bi14d_model_results['actual_period_demand'], bi14d_model_results[col])\n",
    "                bi14d_metrics[model_name] = metrics\n",
    "            except Exception as e:\n",
    "                print(f\"  计算 {model_name} 14日指标时出错: {str(e)}\")\n",
    "    \n",
    "    result_14d_path = os.path.join(output_dir, f\"{product_name}_2024年逐日预测汇总结果_14D.xlsx\")\n",
    "    bi14d_model_results.to_excel(result_14d_path, index=False)\n",
    "    print(f\"详细结果保存至(14D): {result_14d_path}\")\n",
    "    \n",
    "    # ===== 30日聚合评估 =====\n",
    "    bi30d_model_results = test_results.resample('30D', on='date').agg(agg_dict).reset_index()\n",
    "    bi30d_model_results = bi30d_model_results.rename(columns={'date': 'period_start', 'demand': 'actual_period_demand'})\n",
    "    \n",
    "    bi30d_metrics = {}\n",
    "    for col in bi30d_model_results.columns:\n",
    "        if col.endswith('_prediction'):\n",
    "            model_name = col.replace('_prediction', '')\n",
    "            try:\n",
    "                metrics = calculate_streamlined_metrics(bi30d_model_results['actual_period_demand'], bi30d_model_results[col])\n",
    "                bi30d_metrics[model_name] = metrics\n",
    "            except Exception as e:\n",
    "                print(f\"  计算 {model_name} 30日指标时出错: {str(e)}\")\n",
    "    \n",
    "    result_30d_path = os.path.join(output_dir, f\"{product_name}_2024年逐日预测汇总结果_30D.xlsx\")\n",
    "    bi30d_model_results.to_excel(result_30d_path, index=False)\n",
    "    print(f\"详细结果保存至(30D): {result_30d_path}\")\n",
    "    \n",
    "    # 保存模型性能\n",
    "    if model_performance:\n",
    "        performance_df = pd.DataFrame(model_performance).T.reset_index().rename(columns={'index': 'model'})\n",
    "        performance_path = os.path.join(output_dir, f\"{product_name}_逐日预测模型性能对比.xlsx\")\n",
    "        performance_df.to_excel(performance_path, index=False)\n",
    "        print(f\"模型性能对比保存至: {performance_path}\")\n",
    "    \n",
    "    return {\n",
    "        'weekly_metrics': weekly_metrics,\n",
    "        'comparison_df': comparison_df,\n",
    "        'daily_results': test_results,\n",
    "        'bi15d_results': bi15d_model_results,\n",
    "        'bi15d_metrics': bi15d_metrics,\n",
    "        'model_performance': model_performance,\n",
    "        'daily_predictors': daily_predictors,\n",
    "        'ranking': ranking_result,\n",
    "        'best_params': best_params_dict,  # 新增：最佳参数\n",
    "        'bi14d_results': bi14d_model_results,\n",
    "        'bi14d_metrics': bi14d_metrics,\n",
    "        'bi30d_results': bi30d_model_results,\n",
    "        'bi30d_metrics': bi30d_metrics\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# 主程序\n",
    "# =============================================================================\n",
    "def main():\n",
    "    \"\"\"逐日预测版主程序\"\"\"\n",
    "    # === 新增：超参数搜索配置 ===\n",
    "    USE_HYPERPARAMETER_SEARCH = True  # 改为True启用搜索，False使用默认参数\n",
    "    MAX_TIME_PER_MODEL = 180  # 每个模型最大搜索时间（秒）\n",
    "    SEARCH_STRATEGY = \"smart\"  # \"smart\", \"grid\", \"random\"\n",
    "    \n",
    "    print(f\"超参数搜索: {'启用' if USE_HYPERPARAMETER_SEARCH else '禁用'}\")\n",
    "    if USE_HYPERPARAMETER_SEARCH:\n",
    "        print(f\"搜索策略: {SEARCH_STRATEGY}\")\n",
    "        print(f\"每模型最大时间: {MAX_TIME_PER_MODEL}秒\")\n",
    "    \n",
    "    base_dir = r\"/Users/bytedance/Desktop/特征工程补充/结果\"\n",
    "    products = ['电解镍', '高碳铬铁', '铝锭']\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for product in products:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"开始处理 {product} 的2024年逐日预测比较\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        product_output_dir = os.path.join(base_dir, f\"{product}预测结果\")\n",
    "        os.makedirs(product_output_dir, exist_ok=True)\n",
    "        print(f\"输出目录: {product_output_dir}\")\n",
    "        \n",
    "        # 1. 加载选定的特征\n",
    "        selected_features = load_selected_features(product, base_dir)\n",
    "        print(f\"已加载{product}的选定特征: {len(selected_features)}个\")\n",
    "        \n",
    "        # 2. 读取特征工程数据\n",
    "        feature_file = os.path.join(product_output_dir, f\"{product}_特征工程数据.xlsx\")\n",
    "        if not os.path.exists(feature_file):\n",
    "            print(f\"警告: 找不到 {product} 的特征工程数据文件: {feature_file}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            merged_df = pd.read_excel(feature_file)\n",
    "            \n",
    "            if merged_df.empty:\n",
    "                print(f\"警告: {product} 的特征工程数据为空，跳过处理\")\n",
    "                continue\n",
    "            \n",
    "            merged_df['date'] = pd.to_datetime(merged_df['date'], errors='coerce')\n",
    "            print(f\"已加载 {product} 的特征工程数据，形状: {merged_df.shape}\")\n",
    "            \n",
    "            # 3. 逐日预测比较 - 传入搜索配置\n",
    "            comparison_results = daily_prediction_and_comparison_with_search(\n",
    "                merged_df, selected_features, product, product_output_dir,\n",
    "                use_search=USE_HYPERPARAMETER_SEARCH,\n",
    "                max_time_per_model=MAX_TIME_PER_MODEL,\n",
    "                search_strategy=SEARCH_STRATEGY\n",
    "            )\n",
    "            \n",
    "            if comparison_results:\n",
    "                all_results[product] = comparison_results\n",
    "            \n",
    "            print(f\"\\n{product} 的逐日预测比较完成!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"处理 {product} 时出错: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # 4. 生成汇总报告\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"生成逐日预测汇总报告\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    generate_summary_report(all_results, base_dir)\n",
    "    \n",
    "    print(\"\\n所有逐日预测处理完成!\")\n",
    "\n",
    "def generate_summary_report(all_results, base_dir):\n",
    "    \"\"\"生成逐日预测汇总报告\"\"\"\n",
    "    print(\"\\n生成逐日预测汇总报告...\")\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"没有结果可以汇总\")\n",
    "        return\n",
    "    \n",
    "    # 收集所有指标数据\n",
    "    summary_data = []\n",
    "    \n",
    "    for product, results in all_results.items():\n",
    "        weekly_metrics = results.get('weekly_metrics', {})\n",
    "        \n",
    "        for model_name, metrics in weekly_metrics.items():\n",
    "            summary_data.append({\n",
    "                '产品': product,\n",
    "                '模型': model_name,\n",
    "                'MAE': metrics.get('MAE', np.nan),\n",
    "                'RMSE': metrics.get('RMSE', np.nan),\n",
    "                'R²': metrics.get('R²', np.nan),\n",
    "                'MAPE (%)': metrics.get('MAPE (%)', np.nan),\n",
    "                'WMAPE (%)': metrics.get('WMAPE (%)', np.nan)\n",
    "            })\n",
    "    \n",
    "    if summary_data:\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # 保存汇总数据\n",
    "        summary_path = os.path.join(base_dir, \"逐日预测汇总结果.xlsx\")\n",
    "        summary_df.to_excel(summary_path, index=False)\n",
    "        print(f\"  逐日预测汇总结果已保存至: {summary_path}\")\n",
    "        \n",
    "        # 生成各指标的最佳模型汇总\n",
    "        metrics_to_analyze = ['MAE', 'RMSE', 'MAPE (%)', 'WMAPE (%)', 'R²']\n",
    "        best_models_summary = []\n",
    "        \n",
    "        for metric in metrics_to_analyze:\n",
    "            print(f\"\\n基于{metric}的各产品最佳模型:\")\n",
    "            \n",
    "            for product in summary_df['产品'].unique():\n",
    "                product_data = summary_df[summary_df['产品'] == product]\n",
    "                \n",
    "                if metric == 'R²':\n",
    "                    # R²越大越好\n",
    "                    best_idx = product_data[metric].idxmax()\n",
    "                    best_value = product_data[metric].max()\n",
    "                else:\n",
    "                    # 其他指标越小越好\n",
    "                    best_idx = product_data[metric].idxmin()\n",
    "                    best_value = product_data[metric].min()\n",
    "                \n",
    "                if not pd.isna(best_value):\n",
    "                    best_model = product_data.loc[best_idx, '模型']\n",
    "                    \n",
    "                    if metric == 'R²':\n",
    "                        print(f\"- {product}: {best_model} ({metric}: {best_value:.4f})\")\n",
    "                        best_models_summary.append({\n",
    "                            '产品': product,\n",
    "                            '指标': metric,\n",
    "                            '最佳模型': best_model,\n",
    "                            '最佳值': f\"{best_value:.4f}\"\n",
    "                        })\n",
    "                    elif '%' in metric:\n",
    "                        print(f\"- {product}: {best_model} ({metric}: {best_value:.2f}%)\")\n",
    "                        best_models_summary.append({\n",
    "                            '产品': product,\n",
    "                            '指标': metric,\n",
    "                            '最佳模型': best_model,\n",
    "                            '最佳值': f\"{best_value:.2f}%\"\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"- {product}: {best_model} ({metric}: {best_value:.0f})\")\n",
    "                        best_models_summary.append({\n",
    "                            '产品': product,\n",
    "                            '指标': metric,\n",
    "                            '最佳模型': best_model,\n",
    "                            '最佳值': f\"{best_value:.0f}\"\n",
    "                        })\n",
    "        \n",
    "        if best_models_summary:\n",
    "            best_models_df = pd.DataFrame(best_models_summary)\n",
    "            best_models_path = os.path.join(base_dir, \"逐日预测最佳模型汇总.xlsx\")\n",
    "            best_models_df.to_excel(best_models_path, index=False)\n",
    "            print(f\"  逐日预测最佳模型汇总已保存至: {best_models_path}\")\n",
    "        \n",
    "        # 生成热力图 - 每个品种一个图，包含所有指标，每个指标独立着色\n",
    "        try:\n",
    "            # 检查实际可用的指标\n",
    "            available_metrics = [col for col in summary_df.columns if col not in ['产品', '模型']]\n",
    "            print(f\"  可用指标: {available_metrics}\")\n",
    "            \n",
    "            # 选择要显示的指标（确保都存在于数据中）\n",
    "            desired_metrics = ['MAE', 'RMSE', 'MAPE (%)', 'WMAPE (%)', 'R²']\n",
    "            metrics_for_heatmap = [m for m in desired_metrics if m in available_metrics]\n",
    "            \n",
    "            print(f\"  将显示的指标: {metrics_for_heatmap}\")\n",
    "            \n",
    "            if not metrics_for_heatmap:\n",
    "                print(\"  警告: 没有可用的指标数据生成热力图\")\n",
    "                return\n",
    "            \n",
    "            # 获取所有产品\n",
    "            products = summary_df['产品'].unique()\n",
    "            n_products = len(products)\n",
    "            \n",
    "            # 动态调整子图布局\n",
    "            if n_products == 1:\n",
    "                nrows, ncols = 1, 1\n",
    "            elif n_products == 2:\n",
    "                nrows, ncols = 1, 2\n",
    "            else:\n",
    "                nrows, ncols = 2, 2\n",
    "            \n",
    "            fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*12, nrows*10))\n",
    "            if n_products == 1:\n",
    "                axes = [axes]\n",
    "            elif nrows == 1:\n",
    "                axes = axes if hasattr(axes, '__len__') else [axes]\n",
    "            else:\n",
    "                axes = axes.flatten()\n",
    "            \n",
    "            for i, product in enumerate(products):\n",
    "                if i < len(axes):\n",
    "                    # 获取该产品的数据\n",
    "                    product_data = summary_df[summary_df['产品'] == product]\n",
    "                    \n",
    "                    if product_data.empty:\n",
    "                        axes[i].text(0.5, 0.5, f'{product}\\n无数据', \n",
    "                                   ha='center', va='center', transform=axes[i].transAxes)\n",
    "                        axes[i].set_title(f'{product}模型性能对比', fontsize=14)\n",
    "                        continue\n",
    "                    \n",
    "                    # 准备热力图数据 - 模型为行，指标为列\n",
    "                    heatmap_data = product_data.set_index('模型')[metrics_for_heatmap]\n",
    "\n",
    "                    # 检查数据是否为空\n",
    "                    if heatmap_data.empty or heatmap_data.isna().all().all():\n",
    "                        axes[i].text(0.5, 0.5, f'{product}\\n无有效数据', \n",
    "                                ha='center', va='center', transform=axes[i].transAxes)\n",
    "                        axes[i].set_title(f'{product}模型性能对比', fontsize=14)\n",
    "                        continue\n",
    "\n",
    "                    # ===== 新增：自定义排序 =====\n",
    "                    # 定义排序优先级：13周移动平均 > LinearRegression > 其他模型 > Improved_Ensemble\n",
    "                    def custom_sort_key(model_name):\n",
    "                        if model_name == '13周移动平均':\n",
    "                            return (0, model_name)\n",
    "                        elif model_name == 'LinearRegression':\n",
    "                            return (1, model_name)\n",
    "                        elif model_name == 'Improved_Ensemble':\n",
    "                            return (3, model_name)\n",
    "                        else:\n",
    "                            return (2, model_name)\n",
    "\n",
    "                    # 应用排序\n",
    "                    sorted_index = sorted(heatmap_data.index, key=custom_sort_key)\n",
    "                    heatmap_data = heatmap_data.loc[sorted_index]\n",
    "                    # ===== 排序结束 =====\n",
    "\n",
    "                    # 创建标准化后的数据用于着色，但显示原始值\n",
    "                    normalized_data = heatmap_data.copy()\n",
    "                    annotations = heatmap_data.copy()\n",
    "\n",
    "                    \n",
    "                    # 对每个指标进行独立标准化和格式化\n",
    "                    for metric in metrics_for_heatmap:\n",
    "                        if metric in heatmap_data.columns:\n",
    "                            metric_values = heatmap_data[metric].dropna()\n",
    "                            if len(metric_values) > 0:\n",
    "                                # 计算该指标的最小值和最大值\n",
    "                                min_val = metric_values.min()\n",
    "                                max_val = metric_values.max()\n",
    "                                \n",
    "                                if max_val != min_val:\n",
    "                                    if metric == 'R²':\n",
    "                                        # R²: 越大越好，直接标准化到0-1\n",
    "                                        normalized_data[metric] = (heatmap_data[metric] - min_val) / (max_val - min_val)\n",
    "                                    else:\n",
    "                                        # 其他指标: 越小越好，反向标准化\n",
    "                                        normalized_data[metric] = 1 - (heatmap_data[metric] - min_val) / (max_val - min_val)\n",
    "                                else:\n",
    "                                    # 如果最大值等于最小值，设为中间值\n",
    "                                    normalized_data[metric] = 0.5\n",
    "                                \n",
    "                                # 格式化注释文本\n",
    "                                if metric == 'R²':\n",
    "                                    annotations[metric] = heatmap_data[metric].apply(lambda x: f'{x:.3f}' if pd.notna(x) else 'N/A')\n",
    "                                elif '%' in metric:\n",
    "                                    annotations[metric] = heatmap_data[metric].apply(lambda x: f'{x:.1f}%' if pd.notna(x) else 'N/A')\n",
    "                                else:\n",
    "                                    annotations[metric] = heatmap_data[metric].apply(lambda x: f'{x:.0f}' if pd.notna(x) else 'N/A')\n",
    "                    \n",
    "                    # 绘制热力图\n",
    "                    sns.heatmap(normalized_data, \n",
    "                               annot=annotations, \n",
    "                               fmt='', \n",
    "                               cmap='RdYlGn',  # 使用统一色彩，但每个指标独立标准化\n",
    "                               cbar_kws={'label': '相对性能（绿色=较好）'}, \n",
    "                               linewidths=0.5, \n",
    "                               ax=axes[i],\n",
    "                               vmin=0, vmax=1)  # 固定色彩范围\n",
    "                    \n",
    "                    axes[i].set_title(f'{product} - 各模型性能对比', fontsize=14, pad=15)\n",
    "                    axes[i].set_xlabel('评估指标', fontsize=12)\n",
    "                    axes[i].set_ylabel('模型', fontsize=12)\n",
    "                    axes[i].tick_params(axis='x', rotation=45)\n",
    "                    axes[i].tick_params(axis='y', rotation=0)\n",
    "            \n",
    "            # 隐藏多余的子图\n",
    "            for j in range(n_products, len(axes)):\n",
    "                axes[j].set_visible(False)\n",
    "            \n",
    "            plt.suptitle('各产品模型性能对比热力图', fontsize=16, y=0.95)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.92])\n",
    "            \n",
    "            heatmap_path = os.path.join(base_dir, \"逐日预测各产品独立指标热力图.png\")\n",
    "            plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"  逐日预测各产品独立指标热力图已保存至: {heatmap_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  生成热力图时出错: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # 改进的模型一致性分析\n",
    "        try:\n",
    "            print(f\"\\n逐日预测模型一致性分析 (基于WMAPE40%+MAE30%+R²30%):\")\n",
    "            consistency_analysis = {}\n",
    "            \n",
    "            for product in summary_df['产品'].unique():\n",
    "                product_data = summary_df[summary_df['产品'] == product]\n",
    "                \n",
    "                # 排除基线模型进行分析\n",
    "                baseline_models = ['LinearRegression', '13周移动平均']\n",
    "                non_baseline_data = product_data[~product_data['模型'].isin(baseline_models)]\n",
    "                \n",
    "                if len(non_baseline_data) == 0:\n",
    "                    continue\n",
    "                \n",
    "                best_models = {}\n",
    "                \n",
    "                # 核心指标\n",
    "                core_metrics = ['MAE', 'RMSE', 'MAPE (%)', 'WMAPE (%)']\n",
    "                \n",
    "                # 处理需要最小化的指标\n",
    "                for metric in core_metrics:\n",
    "                    if metric in non_baseline_data.columns:\n",
    "                        best_idx = non_baseline_data[metric].idxmin()\n",
    "                        if not pd.isna(non_baseline_data.loc[best_idx, metric]):\n",
    "                            best_models[metric] = non_baseline_data.loc[best_idx, '模型']\n",
    "                \n",
    "                # R²单独处理（需要最大化）\n",
    "                if 'R²' in non_baseline_data.columns:\n",
    "                    r2_best_idx = non_baseline_data['R²'].idxmax()\n",
    "                    if not pd.isna(non_baseline_data.loc[r2_best_idx, 'R²']):\n",
    "                        best_models['R²'] = non_baseline_data.loc[r2_best_idx, '模型']\n",
    "                \n",
    "                if best_models:\n",
    "                    models_list = list(best_models.values())\n",
    "                    most_common = max(set(models_list), key=models_list.count)\n",
    "                    consistency_count = models_list.count(most_common)\n",
    "                    \n",
    "                    # 计算加权一致性分数\n",
    "                    weights = {'WMAPE (%)': 0.4, 'MAE': 0.3, 'R²': 0.3}\n",
    "                    weighted_score = 0\n",
    "                    total_weight = 0\n",
    "                    \n",
    "                    for metric, model in best_models.items():\n",
    "                        if metric in weights:\n",
    "                            weight = weights[metric]\n",
    "                            if model == most_common:\n",
    "                                weighted_score += weight\n",
    "                            total_weight += weight\n",
    "                        else:\n",
    "                            # 其他指标权重较小\n",
    "                            weight = 0.05\n",
    "                            if model == most_common:\n",
    "                                weighted_score += weight\n",
    "                            total_weight += weight\n",
    "                    \n",
    "                    weighted_consistency = weighted_score / total_weight if total_weight > 0 else 0\n",
    "                    \n",
    "                    consistency_analysis[product] = {\n",
    "                        **{f'{metric}最佳': model for metric, model in best_models.items()},\n",
    "                        '推荐模型': most_common,\n",
    "                        '一致性得分': f\"{consistency_count}/{len(best_models)}\",\n",
    "                        '加权一致性': f\"{weighted_consistency:.2f}\"\n",
    "                    }\n",
    "                    \n",
    "                    # 构建显示字符串，突出核心指标\n",
    "                    core_metrics_str = []\n",
    "                    other_metrics_str = []\n",
    "                    \n",
    "                    for metric, model in best_models.items():\n",
    "                        if metric in ['WMAPE (%)', 'MAE', 'R²']:\n",
    "                            core_metrics_str.append(f\"{metric}→{model}\")\n",
    "                        else:\n",
    "                            other_metrics_str.append(f\"{metric}→{model}\")\n",
    "                    \n",
    "                    display_str = \" | \".join(core_metrics_str)\n",
    "                    if other_metrics_str:\n",
    "                        display_str += f\" | 其他: {', '.join(other_metrics_str)}\"\n",
    "                    \n",
    "                    print(f\"- {product}: {display_str}\")\n",
    "                    print(f\"  → 推荐: {most_common} (一致性: {consistency_count}/{len(best_models)}, 加权: {weighted_consistency:.2f})\")\n",
    "            \n",
    "            if consistency_analysis:\n",
    "                consistency_df = pd.DataFrame(consistency_analysis).T.reset_index().rename(columns={'index': '产品'})\n",
    "                consistency_path = os.path.join(base_dir, \"逐日预测模型一致性分析.xlsx\")\n",
    "                consistency_df.to_excel(consistency_path, index=False)\n",
    "                print(f\"  逐日预测模型一致性分析已保存至: {consistency_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  模型一致性分析时出错: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # 生成最终推荐汇总\n",
    "        try:\n",
    "            print(f\"\\n最终模型推荐汇总:\")\n",
    "            recommendations = []\n",
    "            \n",
    "            for product, results in all_results.items():\n",
    "                ranking = results.get('ranking', [])\n",
    "                if ranking:\n",
    "                    # 获取排名前3的模型\n",
    "                    top_3 = ranking[:3]\n",
    "                    best_model = top_3[0][0]\n",
    "                    best_score = top_3[0][1]\n",
    "                    \n",
    "                    print(f\"- {product}: 推荐 {best_model} (综合得分: {best_score:.2f})\")\n",
    "                    print(f\"  前三名: {', '.join([f'{model}({score:.1f}分)' for model, score in top_3])}\")\n",
    "                    \n",
    "                    recommendations.append({\n",
    "                        '产品': product,\n",
    "                        '推荐模型': best_model,\n",
    "                        '综合得分': f\"{best_score:.2f}\",\n",
    "                        '前三名': ', '.join([f'{model}({score:.1f})' for model, score in top_3])\n",
    "                    })\n",
    "            \n",
    "            if recommendations:\n",
    "                recommendations_df = pd.DataFrame(recommendations)\n",
    "                recommendations_path = os.path.join(base_dir, \"最终模型推荐汇总.xlsx\")\n",
    "                recommendations_df.to_excel(recommendations_path, index=False)\n",
    "                print(f\"  最终模型推荐汇总已保存至: {recommendations_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  生成最终推荐时出错: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n逐日预测汇总报告已保存到: {base_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56142989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
